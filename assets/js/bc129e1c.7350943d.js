"use strict";(self.webpackChunkseatunnel_website=self.webpackChunkseatunnel_website||[]).push([[1881],{15680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>g});var r=t(96540);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},u="mdxType",f={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,p=i(e,["components","mdxType","originalType","parentName"]),u=c(t),d=a,g=u["".concat(s,".").concat(d)]||u[d]||f[d]||o;return t?r.createElement(g,l(l({ref:n},p),{},{components:t})):r.createElement(g,l({ref:n},p))}));function g(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,l=new Array(o);l[0]=d;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[u]="string"==typeof e?e:a,l[1]=i;for(var c=2;c<o;c++)l[c]=t[c];return r.createElement.apply(null,l)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},97776:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>f,frontMatter:()=>o,metadata:()=>i,toc:()=>c});var r=t(58168),a=(t(96540),t(15680));const o={},l="Flink SQL Kafka Connector",i={unversionedId:"connector/flink-sql/Kafka",id:"version-2.1.3/connector/flink-sql/Kafka",title:"Flink SQL Kafka Connector",description:"Description",source:"@site/versioned_docs/version-2.1.3/connector/flink-sql/Kafka.md",sourceDirName:"connector/flink-sql",slug:"/connector/flink-sql/Kafka",permalink:"/docs/2.1.3/connector/flink-sql/Kafka",draft:!1,editUrl:"https://github.com/apache/seatunnel-website/edit/main/versioned_docs/version-2.1.3/connector/flink-sql/Kafka.md",tags:[],version:"2.1.3",frontMatter:{},sidebar:"docs",previous:{title:"Flink SQL JDBC Connector",permalink:"/docs/2.1.3/connector/flink-sql/Jdbc"},next:{title:"How to use flink sql module",permalink:"/docs/2.1.3/connector/flink-sql/usage"}},s={},c=[{value:"Description",id:"description",level:2},{value:"Usage",id:"usage",level:2},{value:"1. kafka prepare",id:"1-kafka-prepare",level:3},{value:"2. prepare seatunnel configuration",id:"2-prepare-seatunnel-configuration",level:3},{value:"3. start flink local cluster",id:"3-start-flink-local-cluster",level:3},{value:"4. start Flink SQL job",id:"4-start-flink-sql-job",level:3},{value:"5. verify result",id:"5-verify-result",level:3}],p={toc:c},u="wrapper";function f(e){let{components:n,...t}=e;return(0,a.yg)(u,(0,r.A)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"flink-sql-kafka-connector"},"Flink SQL Kafka Connector"),(0,a.yg)("h2",{id:"description"},"Description"),(0,a.yg)("p",null,"With kafka connector, we can read data from kafka and write data to kafka using Flink SQL. Refer to the ",(0,a.yg)("a",{parentName:"p",href:"https://nightlies.apache.org/flink/flink-docs-release-1.13/docs/connectors/table/kafka/"},"Kafka connector")," for more details."),(0,a.yg)("h2",{id:"usage"},"Usage"),(0,a.yg)("p",null,"Let us have a brief example to show how to use the connector from end to end."),(0,a.yg)("h3",{id:"1-kafka-prepare"},"1. kafka prepare"),(0,a.yg)("p",null,"Please refer to the ",(0,a.yg)("a",{parentName:"p",href:"https://kafka.apache.org/quickstart"},"Kafka QuickStart")," to prepare kafka environment and produce data like following:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"$ bin/kafka-console-producer.sh --topic <topic-name> --bootstrap-server localhost:9092\n")),(0,a.yg)("p",null,"After executing the command, we will come to the interactive mode. Print the following message to send data to kafka."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},'{"id":1,"name":"abc"}\n>{"id":2,"name":"def"}\n>{"id":3,"name":"dfs"}\n>{"id":4,"name":"eret"}\n>{"id":5,"name":"yui"}\n')),(0,a.yg)("h3",{id:"2-prepare-seatunnel-configuration"},"2. prepare seatunnel configuration"),(0,a.yg)("p",null,"Here is a simple example of seatunnel configuration."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-sql"},"SET table.dml-sync = true;\n\nCREATE TABLE events (\n    id INT,\n    name STRING\n) WITH (\n    'connector' = 'kafka',\n    'topic'='<topic-name>',\n    'properties.bootstrap.servers' = 'localhost:9092',\n    'properties.group.id' = 'testGroup',\n    'scan.startup.mode' = 'earliest-offset',\n    'format' = 'json'\n);\n\nCREATE TABLE print_table (\n    id INT,\n    name STRING\n) WITH (\n    'connector' = 'print',\n    'sink.parallelism' = '1'\n);\n\nINSERT INTO print_table SELECT * FROM events;\n")),(0,a.yg)("h3",{id:"3-start-flink-local-cluster"},"3. start flink local cluster"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"$ ${FLINK_HOME}/bin/start-cluster.sh\n")),(0,a.yg)("h3",{id:"4-start-flink-sql-job"},"4. start Flink SQL job"),(0,a.yg)("p",null,"Execute the following command in seatunnel home path to start the Flink SQL job."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-bash"},"$ bin/start-seatunnel-sql.sh -c config/kafka.sql.conf\n")),(0,a.yg)("h3",{id:"5-verify-result"},"5. verify result"),(0,a.yg)("p",null,"After the job submitted, we can see the data printing by connector 'print' in taskmanager's log ."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-text"},"+I[1, abc]\n+I[2, def]\n+I[3, dfs]\n+I[4, eret]\n+I[5, yui]\n")))}f.isMDXComponent=!0}}]);