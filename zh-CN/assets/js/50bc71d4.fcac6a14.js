"use strict";(self.webpackChunkseatunnel_website=self.webpackChunkseatunnel_website||[]).push([[95809],{88513:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get","metadata":{"permalink":"/zh-CN/blog/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2023-3-31-SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get.md","source":"@site/blog/2023-3-31-SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get.md","title":"SeaTunnel 2.3.1 is released! The refactored AI Compatible feature allows ChatGPT to automatically generate Connector code","description":"SeaTunnel version 2.3.1 was released recently. This is a high-profile release with many important function updates and optimizations.","date":"2023-03-31T00:00:00.000Z","formattedDate":"2023\u5e743\u670831\u65e5","tags":[],"readingTime":4.965,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"nextItem":{"title":"Performance Test Report: SeaTunnel Synchronizes data in batches 420% Faster than GLUE!","permalink":"/zh-CN/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md"}},"content":">*SeaTunnel version 2.3.1 was released recently. This is a high-profile release with many important function updates and optimizations.*\\n>*At the level of programming user experience, the new version improves the stability of SeaTunnel Zeta and CI/CD; at the level of connectors, the new version implements 7+ new connectors and fixes existing commonly used connectors bugs, and improved security. The community refactored multiple underlying base classes and added an important feature, AI Compatible. With the optimized API, users can use ChatGPT 4.0 to quickly build the SaaS Connector they need.*\\n# Major Feature update\\n\\n## 01 SeaTunnel Zeta\\n\\nThe first version of the data integration engine-SeaTunnel Zeta is introduced in the SeaTunnel 2.3.0 release and has received feedback from numerous community users. In SeaTunnel version 2.3.1, we have fixed all the bugs reported by users, optimized the use of memory and threads, and greatly improved the stability of Zeta.\\n\\nIn version 2.3.1, the community also added several new Zeta features, including a dedicated JVM parameter configuration file, client output of job monitoring information, Rest API for Zeta cluster information and job information, etc.\\n\\nAt the checkpoint level, version 2.3.1 Zeta supports using OSS as checkpoint storage. It also supports savepoint running jobs and resuming jobs from savepoints.\\n\\nIn addition, version 2.3.1 also adds a set of Zeta\u2019s Rest API, which can be used to obtain the list of jobs running on Zeta, the status information of jobs, and the monitoring indicators of Zeta cluster nodes. For specific usage methods, please refer to\xa0**https:/ /seatunnel.apache.org/docs/seatunnel-engine/rest-api/**\\n\\n## 02 AI Compatible\\n\\nIn SeaTunnel 2.3.1, the HTTP interface and related APIs are reconstructed, and the SaaS Connector-related API and Connector construction process are simplified according to the existing xGPT level capabilities so that ChatGPT 4.0 can directly generate SaaS Connectors and quickly generate various SaaS Connector interfaces. Under normal circumstances, the results obtained by this method are 95% similar to the code written by open-source contributors (see appendix).\\n\\nOf course, because ChatGPT4.0 will be updated in October 2021, it is necessary to provide some latest vectorized documents for the latest SaaS interface adaptation to have the latest interface adaptation. However, this refactored API and code framework allows users to generate Connectors more quickly and contribute to the open-source community, making the SeaTunnel interface more powerful.\\n\\n# Connector\\n\\n## 01 7+ new connectors\\n\\nWhile fixing the bugs of known connectors and optimizing the connectors, the community has added 7 new connectors including SAP HANA, Persistiq, TDEngine, SelectDB Cloud, Hbase, FieldMapper Transform, and SimpleSQL Transform.\\n\\n## 02 Reimplement SQL Transform\\n\\nSince the previous SQL Transform connector was defined based on Flink SQL and Spark SQL, SQL Transform cannot adapt to the execution of multiple engines, so we removed the SQL Transform function in version 2.3.0. In version 2.3.1, we reimplemented SQL Transform. SQL Transform is an API that does not depend on a task-specific execution engine and can perfectly run on three different engines: Flink/Spark/Zeta. Special thanks to contributor Ma Chengyuan (GitHub ID: rewerma) for leading and contributing this important Feature.\\n\\nFor the functions already supported by SQL Transform, please refer to\xa0[https://seatunnel.apache.org/docs/2.3.1/transform-v2/sql-functions](https://seatunnel.apache.org/docs/2.3.1/transform-v2/sql-functions)\\n\\n## 03 New SQL Server CDC\\n\\nAt the CDC connector level, the community has newly added a SQL Server CDC connector, and made a lot of optimizations to MySQL CDC, improving the stability of MySQL CDC.\\n\\n## 04 Added CDC connector to output debezium-json format function\\n\\nIn addition, version 2.3.1 also added the function of the CDC connector to output debezium-json format. Users can use MySQL CDC to read binlog and output data in debezium-json format to Kafka, so that users can create new synchronization tasks to read The data in debezium-json format in Kafka is synchronized to the target data source, or you can directly write other programs to read the data in debezium-json format in Kafka to perform some indicator calculations.\\n\\n# Safety\\n\\nBefore version 2.3.1, users need to configure the database username, password, and other information in plain text in the config file, which may cause some security problems. In version 2.3.1, we added the configuration file encryption function, and users can fill in the encrypted database username, password, and other information in the config file. When the job is running, SeaTunnel will decrypt the content in the config file based on the default encryption and decryption algorithm. At the same time, the encryption function provides SPI, by which users can customize the parameter list of encryption and decryption and the algorithm of encryption and decryption based on their own needs.\\n\\nFor how to use this function, please refer to\xa0[https://seatunnel.apache.org/docs/2.3.1/connector-v2/Config-Encryption-Decryption](https://seatunnel.apache.org/docs/2.3.1/connector-v2/Config-Encryption-Decryption)\\n\\n# Third-party engine support\\n\\nSeaTunnel version 2.3.1 supports Spark version 3.3, as well as Flink 1.14.6, Flink 1.15, Flink 1.16, and other versions, basically covering the mainstream versions of Spark and Flink.\\n\\n# Client\\n\\nThe new version introduces an SPI for job configuration. Previously, only hocon json configuration files were supported. Now SPI is opened to the users to customize the format of job configuration files to meet different business system integration requirements.\\n\\n# Optimization\\n\\nSeaTunnel 2.1.3 version has made many important optimizations, including changes in core components, connector components, CI/CD, Zeta(ST-Engine), and E2E components, involving updating new functions, improving existing functions, and optimizing tests and deployment processes. Some notable changes include adding parallelism and column projection interfaces in Core API, introducing MySQL-CDC source factory in Connector-V2 and supporting only-once semantics for JDBC source connectors, improving CI/CD process and stability for E2E In Zeta (ST-Engine), the logic of restarting the job when all nodes are down is added, and the timeout period for writing data is configurable.\\n\\nFor a detailed list, see the Release Note [Improve] section.\\n\\n# Document\\n\\nIn addition, the new version also has a series of updates to the documentation, including adding transform v2 documentation and some hints, as well as improving the documentation of various connectors.\\n\\nSee the Release Note [Docs] section for details.\\n\\nDocument address:\xa0[https://seatunnel.apache.org/versions/](https://seatunnel.apache.org/versions/)\\n\\n# Release Note\\n\\n[https://github.com/apache/incubator-seatunnel/blob/2.3.1/release-note.md](https://github.com/apache/incubator-seatunnel/blob/2.3.1/release-note.md)\\n\\n* Project address:\xa0[https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n* Download address:\xa0[https://seatunnel.apache.org/download](https://seatunnel.apache.org/download)\\n# Acknowledgement to the contributors\\n\\n![contributors](/image/202303310331/contributors.png)"},{"id":"/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md","metadata":{"permalink":"/zh-CN/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2023-3-29-Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md.md","source":"@site/blog/2023-3-29-Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md.md","title":"Performance Test Report: SeaTunnel Synchronizes data in batches 420% Faster than GLUE!","description":"cover","date":"2023-03-29T00:00:00.000Z","formattedDate":"2023\u5e743\u670829\u65e5","tags":[],"readingTime":2.535,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"SeaTunnel 2.3.1 is released! The refactored AI Compatible feature allows ChatGPT to automatically generate Connector code","permalink":"/zh-CN/blog/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get"},"nextItem":{"title":"SeaTunnel now supports CDC (Capture Change Data) writing by ClickHouse Connector!","permalink":"/zh-CN/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md"}},"content":"cover\\n\\nSeaTunnel Zeta has been officially released with the joint efforts of the community. After comparing the performance of SeaTunnel with DataX and Airbyte, we also compared the performance of SeaTunnel with the popular data synchronization tool AWS GLUE.\\n\\nThe results showed that SeaTunnel batch syncs MySQL data to MySQL 420% faster than GLUE.\\n\\nTo ensure the accuracy of the test, we took on the test under the same test environment: under the same resource conditions, we tested SeaTunnel and AWS GLUE to synchronize data from MySQL to MySQL in batches and compared the time required for the two tools.\\n\\n![1](/image/202303311452/1.png)\\n\\n\\nWe created a table in MySQL containing 31 fields, with the primary key selected as an incrementing ID, and all other fields generated randomly, without setting any indexes. The table creation statement is as follows:\\n\\n```plain\\ncreate table test.type_source_table\\n(\\n    id                   int auto_increment\\n        primary key,\\n    f_binary             binary(64)          null,\\n    f_blob               blob                null,\\n    f_long_varbinary     mediumblob          null,\\n    f_longblob           longblob            null,\\n    f_tinyblob           tinyblob            null,\\n    f_varbinary          varbinary(100)      null,\\n    f_smallint           smallint            null,\\n    f_smallint_unsigned  smallint unsigned   null,\\n    f_mediumint          mediumint           null,\\n    f_mediumint_unsigned mediumint unsigned  null,\\n    f_int                int                 null,\\n    f_int_unsigned       int unsigned        null,\\n    f_integer            int                 null,\\n    f_integer_unsigned   int unsigned        null,\\n    f_bigint             bigint              null,\\n    f_bigint_unsigned    bigint unsigned     null,\\n    f_numeric            decimal             null,\\n    f_decimal            decimal             null,\\n    f_float              float               null,\\n    f_double             double              null,\\n    f_double_precision   double              null,\\n    f_longtext           longtext            null,\\n    f_mediumtext         mediumtext          null,\\n    f_text               text                null,\\n    f_tinytext           tinytext            null,\\n    f_varchar            varchar(100)        null,\\n    f_date               date                null,\\n    f_datetime           datetime            null,\\n    f_time               time                null,\\n    f_timestamp          timestamp           null\\n);\\n```\\n# SeaTunnel Task Configuration\\n\\nIn SeaTunnel, we split the data according to the ID field and process it in multiple sub-tasks. Here is the configuration file for SeaTunnel:\\n\\n```plain\\nenv {\\n    job.mode = \\"BATCH\\"\\n    checkpoint.interval = 300000\\n}\\nsource {\\n    Jdbc {\\n        url = \\"jdbc:mysql://XXX:3306/test\\"\\n        driver = \\"com.mysql.cj.jdbc.Driver\\"\\n        user = \\"root\\"\\n        password = \\"password\\"\\n        connection_check_timeout_sec = 100\\n        query = \\"select id, f_binary, f_blob, f_long_varbinary, f_longblob, f_tinyblob, f_varbinary, f_smallint, f_smallint_unsigned, f_mediumint, f_mediumint_unsigned, f_int, f_int_unsigned, f_integer, f_integer_unsigned, f_bigint, f_bigint_unsigned, f_numeric, f_decimal, f_float, f_double, f_double_precision, f_longtext, f_mediumtext, f_text, f_tinytext, f_varchar, f_date, f_datetime, f_time, f_timestamp from test\\"\\n        partition_column = \\"id\\"\\n        partition_num = 40\\n        parallelism = 2\\n    }\\n}\\nsink {\\nJdbc {\\n          url = \\"jdbc:mysql://XXX:3306/test\\"\\n         driver = \\"com.mysql.cj.jdbc.Driver\\" \\n        user = \\"root\\"\\n        password = \\"password\\"\\n         query = \\"insert into test_1 values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\\"\\n    }\\n}\\n```\\nUnder fixed JVM memory of 4G and parallelism of 2, SeaTunnel completed the synchronization in 1965 seconds. Based on this conclusion, we tested the speed of GLUE under the same memory and concurrency settings.\\n# GLUE Task Configuration\\n\\nWe created a MySQL-to-MySQL job as follows:\\n\\n![2](/image/202303311452/2.png)\\n\\n\\nConfiguration source connect with the target:\\n\\n![3](/image/202303311452/3.png)\\n\\nJob configuration:\\n\\n![4](/image/202303311452/4.png)\\n\\n\\n![5](/image/202303311452/5.png)\\n\\n\\nAdjust the memory: job parameters configuration\\n\\n![6](/image/202303311452/6-1.png)\\n\\n\\n\u2014 conf spark.yarn.executor.memory=4g\\n\\nUnder this configuration, GLUE took 8191 seconds to complete the synchronization.\\n\\n# Conclusion\\n\\nAfter comparing the best configurations, we conducted a more in-depth comparison for different memory sizes. The following chart shows the comparison results obtained through repeated testing under the same environment.\\n\\n![7](/image/202303311452/7.png)\\n\\n\\nThe unit is seconds.\\n\\n![8](/image/202303311452/8.png)\\n\\nNote: This comparison is based on SeaTunnel: commit ID f57b897, and we welcome to download and test it!"},{"id":"/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md","metadata":{"permalink":"/zh-CN/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2023-02-09-SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md.md","source":"@site/blog/2023-02-09-SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md.md","title":"SeaTunnel now supports CDC (Capture Change Data) writing by ClickHouse Connector!","description":"Written by Wang Hailin, Apache SeaTunnel PPMC","date":"2023-02-09T00:00:00.000Z","formattedDate":"2023\u5e742\u67089\u65e5","tags":[],"readingTime":5.75,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Performance Test Report: SeaTunnel Synchronizes data in batches 420% Faster than GLUE!","permalink":"/zh-CN/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md"},"nextItem":{"title":"In the recently released SeaTunnel 2.3.0 official version","permalink":"/zh-CN/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!"}},"content":"Written by Wang Hailin, Apache SeaTunnel PPMC\\n\\n## Preface \\nCurrently, SeaTunnel supports database change data capture (CDC https://github.com/apache/incubator-seatunnel/issues/3175), to transfer data changes to downstream systems in real time. SeaTunnel categorizes the captured data changes into the following 4 types: \\n- INSERT: Data insertion \\n- UPDATE_BEFORE: Historical value before data change \\n- UPDATE_AFTER: New value after data change \\n- DELETE: Data deletion \\n\\nTo handle the above data change operations, the Sink Connector needs to support writing behavior. This article will introduce how the ClickHouse Sink Connector supports writing these CDC types of data changes. \\n\\nFor CDC scenarios, the primary key is a necessary condition, so first, it needs to support the general requirements of INSERT, UPDATE, DELETE, etc. based on the primary key and ensure that the writing order is consistent with the CDC event order. In addition, considering the complexity of the data source in practice, it also needs to support UPSERT writing. Finally, according to the characteristics of ClickHouse itself, corresponding optimizations need to be made, such as UPDATE and DELETE being heavyweight operations in ClickHouse, which should be optimized based on the corresponding table engine\'s characteristics.\\n\\n## Overall design \\n\\nThe current ClickHouse Sink Connector is based on the JDBC Driver implementation, and a group of JDBC executors can be designed to encapsulate the processing of different types of data, making it convenient to switch or combine implementations based on actual scenarios and encapsulate implementation details. \\n\\nJdbcBatchStatementExecutor is the top-level interface of the executor.\\n\\n```\\npublic interface JdbcBatchStatementExecutor extends AutoCloseable {\\n\\n    void prepareStatements(Connection connection) throws SQLException;\\n\\n    void addToBatch(SeaTunnelRow record) throws SQLException;\\n\\n    void executeBatch() throws SQLException;\\n\\n    void closeStatements() throws SQLException;\\n\\n    @Override\\n    default void close() throws SQLException {\\n        closeStatements();\\n    }\\n}\\n```\\n\\n\\n`JdbcBatchStatementExecutor` has the following implementation classes: \\n\\n\\n\\n```\\nSimpleBatchStatementExecutor // implements simple SQL Batch execution logic \\nInsertOrUpdateBatchStatementExecutor // implements INSERT, UPDATE update, also supports UPSERT mode \\nReduceBufferedBatchStatementExecutor // memory accumulation, when refreshing to the database, the data change type (INSERT, UPDATE, DELETE) is distributed to the specific execution executor \\n```\\n\\n### Handling of cases where the primary key is not specified\\nCurrently, in CDC processing, the primary key is a necessary condition. If the Sink Connector is not specified in the primary key column configuration, it uses the append-only mode to write, calling `SimpleBatchStatementExecutor` directly.\\n### CDC data process\\nWe divide the execution logic of data processing as follows: different data types enter the corresponding Executor and are finally transformed into their respective SQL statements for execution, and Jdbc Batch batching is used during this process.\\n\\n\\n```\\nCDC Event\\n               /         \\\\\\n              /           \\\\\\n             /             \\\\\\n            /               \\\\\\n    DELETE Executor   INSERT OR UPDATE Executor\\n                            /          \\\\\\n                           /            \\\\\\n                          /              \\\\\\n                         /                \\\\\\n                     INSERT Executor    UPDATE Executor\\n```\\n\\n\\n### Maintaining the Order of CDC Data\\nCDC events are ordered, and writing must be processed in the order in which the events occur, otherwise data inconsistencies may occur. \\n\\nIn the previous logic, data of different types were distributed to their respective Executors and Jdbc Batch was used for batch submission to improve write performance, but categorizing batching can result in the order of submissions not being consistent with the CDC event order.\\n\\nWe can add an execution barrier marker, when the processed data row is of the same type as the previous data row, it can be batched, if not, the previous batch is first flushed to the database, ensuring that the data write order is strictly consistent with the CDC event order.\\n\\nExample for `InsertOrUpdateBatchStatementExecutor`\\n\\n\\n```\\npublic class InsertOrUpdateBatchStatementExecutor implements JdbcBatchStatementExecutor {\\n    @Override\\n    public void addToBatch(SeaTunnelRow record) throws SQLException {\\n        boolean currentChangeFlag = hasInsert(record);\\n        if (currentChangeFlag) {\\n            if (preChangeFlag != null && !preChangeFlag) {\\n                updateStatement.executeBatch();\\n                updateStatement.clearBatch();\\n            }\\n            valueRowConverter.toExternal(record, insertStatement);\\n            insertStatement.addBatch();\\n        } else {\\n            if (preChangeFlag != null && preChangeFlag) {\\n                insertStatement.executeBatch();\\n                insertStatement.clearBatch();\\n            }\\n            valueRowConverter.toExternal(record, updateStatement);\\n            updateStatement.addBatch();\\n        }\\n        preChangeFlag = currentChangeFlag;\\n        submitted = false;\\n    }\\n    \\n    @Override\\n    public void executeBatch() throws SQLException {\\n        if (preChangeFlag != null) {\\n            if (preChangeFlag) {\\n                insertStatement.executeBatch();\\n                insertStatement.clearBatch();\\n            } else {\\n                updateStatement.executeBatch();\\n                updateStatement.clearBatch();\\n            }\\n        }\\n        submitted = true;\\n    }\\n}\\n```\\n\\n\\nOf course, this will significantly slow down the batch processing, so we use `ReduceBufferedBatchStatementExecutor`to add a memory buffer layer, and when executing batch submissions, we distribute submissions to the database.\\n\\nExample for `ReduceBufferedBatchStatementExecutor`\\n\\n```\\npublic class ReduceBufferedBatchStatementExecutor implements JdbcBatchStatementExecutor {\\n    private final LinkedHashMap<SeaTunnelRow, Pair<Boolean, SeaTunnelRow>> buffer = new LinkedHashMap<>();\\n    \\n    @Override\\n    public void addToBatch(SeaTunnelRow record) throws SQLException {\\n        buffer.put(record, ...);\\n    }\\n    \\n    @Override\\n    public void executeBatch() throws SQLException {\\n        Boolean preChangeFlag = null;\\n        Set<Map.Entry<SeaTunnelRow, Pair<Boolean, SeaTunnelRow>>> entrySet = buffer.entrySet();\\n        for (Map.Entry<SeaTunnelRow, Pair<Boolean, SeaTunnelRow>> entry : entrySet) {\\n            Boolean currentChangeFlag = entry.getValue().getKey();\\n            if (currentChangeFlag) {\\n                if (preChangeFlag != null && !preChangeFlag) {\\n                    deleteExecutor.executeBatch();\\n                }\\n                insertOrUpdateExecutor.addToBatch(entry.getValue().getValue());\\n            } else {\\n                if (preChangeFlag != null && preChangeFlag) {\\n                    insertOrUpdateExecutor.executeBatch();\\n                }\\n                deleteExecutor.addToBatch(entry.getKey());\\n            }\\n            preChangeFlag = currentChangeFlag;\\n        }\\n    \\n        if (preChangeFlag != null) {\\n            if (preChangeFlag) {\\n                insertOrUpdateExecutor.executeBatch();\\n            } else {\\n                deleteExecutor.executeBatch();\\n            }\\n        }\\n        buffer.clear();\\n    }\\n}\\n```\\n\\n\\n### Implementing a General UPSERT Write\\nIn `InsertOrUpdateBatchStatementExecutor`, you can configure to turn on UPSERT, when processing INSERT or UPDATE data types, it will first use the primary key to query the data row to see if it already exists and then decide to use INSERT or UPDATE SQL for writing. \\n\\n*Note: This configuration is optional and will slow down the write speed, only opens when certain special scenarios are required.*\\n\\nExample for `InsertOrUpdateBatchStatementExecutor`\\n\\n```\\npublic class InsertOrUpdateBatchStatementExecutor implements JdbcBatchStatementExecutor {\\n    @Override\\n    public void addToBatch(SeaTunnelRow record) throws SQLException {\\n        boolean currentChangeFlag = hasInsert(record);\\n      ...\\n    }\\n\\n    private boolean hasInsert(SeaTunnelRow record) throws SQLException {\\n        if (upsertMode()) {\\n            return !exist(keyExtractor.apply(record));\\n        }\\n        switch (record.getRowKind()) {\\n            case INSERT:\\n                return true;\\n            case UPDATE_AFTER:\\n                return false;\\n            default:\\n                throw new UnsupportedOperationException();\\n        }\\n    }\\n    \\n    private boolean exist(SeaTunnelRow pk) throws SQLException {\\n        keyRowConverter.toExternal(pk, existStatement);\\n        try (ResultSet resultSet = existStatement.executeQuery()) {\\n            return resultSet.next();\\n        }\\n    }\\n}\\n```\\n\\n\\n### Optimizing UPSERT for ReplacingMergeTree Engine\\n\\nThe `ReplacingMergeTree` table engine can configure an `ORDER BY` field, and when executing the INSERT INTO statement, it covers the records with the same ORDER BY field. We can also utilize this feature to implement UPSERT.\\n\\nWhen the user writes to the `ReplacingMergeTree` table engine and the table\'s `ORDER BY` field is the same as the primary key field configured in the Sink Connector, both INSERT/UPDATE_AFTER data types are processed as INSERT to implement UPSERT.\\n### Optimizing Updates for the MergeTree Engine\\n\\nDELETE and UPDATE are heavyweight operations in ClickHouse, but there is an experimental lightweight deletion (https://clickhouse.com/docs/en/sql-reference/statements/delete) for `MergeTree` engine, which performs better than the heavyweight deletion. We allow the user to configure the lightweight deletion.\\n\\nWhen the user writes to the `MergeTree` table engine and enables the lightweight deletion, we treat both DELETE/UPDATE_BEFORE data types as lightweight deletions, and treat both INSERT/UPDATE_AFTER data types as INSERTs, avoiding the UPDATE operation and using the lightweight deletion.\\n\\n## Related PR \\n- https://github.com/apache/incubator-seatunnel/pull/3653 \\n\\nContribution to improving the related functions is welcomed, if you have any questions, please raise an issue on SeaTunnel GitHub (https://www.github.com/apache/incubator-seatunnel), and we will reply as soon as possible.\\n\\n## Reference \\n- https://clickhouse.com/docs/en/sql-reference/statements/delete \\n- https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree"},{"id":"Reveal the core design of the SeaTunnel Zeta synchronization engine!","metadata":{"permalink":"/zh-CN/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2023-01-10-Reveal-the-core-design-of-the-SeaTunnel-Zeta-synchronization-engine.md","source":"@site/blog/2023-01-10-Reveal-the-core-design-of-the-SeaTunnel-Zeta-synchronization-engine.md","title":"In the recently released SeaTunnel 2.3.0 official version","description":"In the recently released SeaTunnel 2.3.0 official version, the community self-developed engine SeaTunnel Zeta which has been under preparation for more than a year\u2014\u2014is officially released, and it will be used as the default engine of SeaTunnel in the future, providing users with high throughput, low latency, reliable consistent synchronization job operation guarantee.","date":"2023-01-10T00:00:00.000Z","formattedDate":"2023\u5e741\u670810\u65e5","tags":[{"label":"Meetup","permalink":"/zh-CN/blog/tags/meetup"}],"readingTime":11.495,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"Reveal the core design of the SeaTunnel Zeta synchronization engine!","title":"In the recently released SeaTunnel 2.3.0 official version","tags":["Meetup"]},"prevItem":{"title":"SeaTunnel now supports CDC (Capture Change Data) writing by ClickHouse Connector!","permalink":"/zh-CN/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md"},"nextItem":{"title":"SeaTunnel supports IoTDB to implement IoT data synchronization","permalink":"/zh-CN/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection"}},"content":"![](/image/16733429185569/16733443077196.png)\\n\\n\\nIn the recently released SeaTunnel 2.3.0 official version, the community self-developed engine SeaTunnel Zeta which has been under preparation for more than a year\u2014\u2014is officially released, and it will be used as the default engine of SeaTunnel in the future, providing users with high throughput, low latency, reliable consistent synchronization job operation guarantee.\\n\\nWhy does SeaTunnel develop its synchronization engine? What is the positioning of the SeaTunnel Engine? How is it different from traditional computing engines? What is the design idea? What is unique about the architectural design? These questions will be answered in this article.\\n\\n* Why develop our engine\\n* SeaTunnel Engine Positioning\\n* Design ideas\\n* Architecture design\\n* Unique advantages and features\\n* Current basic functions and features\\n* Future optimization plan\\n## 01 Why develop our engine\\n\\nIt was a year ago that the SeaTunnel community publicly stated for the first time that it would develop its engine. The reason why the team decided to develop a self-developed engine was that SeaTunnel\'s connector can run only on Flink or Spark, and Flink and Spark, as computing engines, have many unsolvable problems when integrating and synchronizing data.\\n\\nRefer to:\\nWhy do we self-develop the big data synchronization engine SeaTunnel Zeta?\\nhttps://github.com/apache/incubator-seatunnel/issues/1954\\n## 02 Design ideas\\n\\nThe general idea of engine design is as follows:\\n\\n1. Simple and easy to use, the new engine minimizes the dependence on third-party services, and can realize cluster management, snapshot storage, and cluster HA functions without relying on big data components such as Zookeeper and HDFS. This is very useful for users who do not have a big data platform or are unwilling to rely on a big data platform for data synchronization.\\n2. More resource-saving, at the CPU level, Zeta Engine internally uses Dynamic Thread Sharing (dynamic thread sharing) technology. In the real-time synchronization scenario, if the number of tables is large but the amount of data in each table is small, Zeta Engine will Synchronous tasks run in shared threads, which can reduce unnecessary thread creation and save system resources. On the read and data write side, the Zeta Engine is designed to minimize the number of JDBC connections. In the CDC scenario, Zeta Engine will try to reuse log reading and parsing resources as much as possible.\\n3. More stable. In this version, Zeta Engine uses Pipeline as the minimum granularity of Checkpoint and fault tolerance for data synchronization tasks. The failure of a task will only affect the tasks that have upstream and downstream relationships with it. Try to avoid task failures that cause the entire Job to fail. or rollback. At the same time, for scenarios where the source data has a storage time limit, Zeta Engine supports enabling data cache to automatically cache the data read from the source, and then the downstream tasks read the cached data and write it to the target. In this scenario, even if the target end fails and data cannot be written, it will not affect the normal reading of the source end, preventing the source end data from being deleted due to expiration.\\n4. Faster, Zeta Engine\u2019s execution plan optimizer will optimize the execution plan to reduce the possible network transmission of data, thereby reducing the loss of overall synchronization performance caused by data serialization and deserialization, and completing faster Data synchronization operations. Of course, it also supports speed limiting, so that sync jobs can be performed at a reasonable speed.\\n5. Data synchronization support for all scenarios. SeaTunnel aims to support full synchronization and incremental synchronization under offline batch synchronization, and support real-time synchronization and CDC.\\n\\n## 03 Architecture design\\n\\nSeaTunnel Engine is mainly composed of a set of APIs for data synchronization processing and a core computing engine. Here we mainly introduce the architecture design of the SeaTunnel Engine core engine.\\n![](/image/16733429185569/16733443263288.png)\\npicture\\n\\nSeaTunnel Engine consists of three main services: **CoordinatorService, TaskExecutionService, and SlotService.**\\n\\n### Coordinator Service\\n\\nCoordinatorService is the Master service of the cluster, which provides the generation process of each job from LogicalDag to ExecutionDag, and then to PhysicalDag, and finally creates the JobMaster of the job for scheduling execution and status monitoring of the job. CoordinatorService is mainly composed of 4 large functional modules:\\n1. JobMaster is responsible for the generation process from LogicalDag to ExecutionDag to PhysicalDag of a single job, and is scheduled to run by PipelineBaseScheduler.\\n2. CheckpointCoordinator, responsible for the Checkpoint process control of the job.\\n3. ResourceManager is responsible for the application and management of job resources. It currently supports Standalone mode and will support On Yarn and On K8s in the future.\\n4. Metrics Service, responsible for the statistics and summary of job monitoring information.\\n### TaskExecutionService\\n\\nTaskExecutionService is the Worker service of the cluster, which provides the real runtime environment of each Task in the job. TaskExecutionService uses Dynamic Thread Sharing technology to reduce CPU usage.\\n### SlotService\\n\\nSlotService runs on each node of the cluster and is mainly responsible for the division, application, and recycling of resources on the node.\\n## 04 Unique advantages and features\\n\\n### Autonomous cluster\\nSeaTunnel Engine has realized autonomous clustering (no centralization). To achieve cluster autonomy and job fault tolerance without relying on third-party service components (such as Zookeeper), SeaTunnel Engine uses Hazelcast as the underlying dependency. Hazelcast provides a distributed memory network, allowing users to operate a distributed collection like a normal Java collection locally. SeaTunnel saves the status information of the job in the memory grid of Hazelcast. When the Master node switches, it can Job state recovery based on data in the Hazelcast in-memory grid. At the same time, we have also implemented the persistence of Hazelcast memory grid data, and persisted the job status information to the storage (database of JDBC protocol, HDFS, cloud storage) in the form of WAL. In this way, even if the entire cluster hangs and restarts, the runtime information of the job can be repaired.\\n### Data cache\\nSeaTunnel Engine is different from the traditional Spark/Flink computing engine, it is an engine specially used for data synchronization. The SeaTunnel engine naturally supports data cache. When multiple synchronous jobs in the cluster share a data source, the SeaTunnel engine will automatically enable the data cache. The source of a job will read the data and write it into the cache, and all other jobs will no longer read data from the data source but are automatically optimized to read data from the Cache. The advantage of this is that it can reduce the reading pressure of the data source and reduce the impact of data synchronization on the data source.\\n### Speed control\\nSeaTunnel Engine supports the speed limit during data synchronization, which is very useful when reading data sources with high concurrency. A reasonable speed limit can not only ensure that the data is synchronized on time, but also minimize the pressure on the data source.\\n\\n### Shared connection pool to reduce database pressure\\nAt present, the underlying operating tools and data synchronization tools provided by computing engines such as Spark/Flink cannot solve the problem that each table needs a JDBC connection when the entire database is synchronized. Database connections are resources for the database. Too many database connections will put great pressure on the database, resulting in a decrease in the stability of database read and write delays. This is a very serious accident for business databases. To solve this problem, SeaTunnel Engine uses a shared connection pool to ensure that multiple tables can share JDBC connections, thereby reducing the use of database connections.\\n### Breakpoint resume (incremental/full volume)\\n\\nSeaTunnel Engine supports resumed uploads under offline synchronization. When the amount of data is large, a data synchronization job often needs to run for tens of minutes or several hours. If the middle job hangs up and reruns, it means wasting time. SeaTunnel Engine will continue to save the state (checkpoint) during the offline synchronization process. When the job hangs up and reruns, it will continue to run from the last checkpoint, which effectively solves the data that may be caused by hardware problems such as node downtime. Delay.\\n### The Schema revolution route\\nSchema evolution is a feature that allows users to easily change a table\'s current schema to accommodate data that changes over time. Most commonly, it is used when performing an append or overwrite operation, to automatically adjust the schema to include one or more new columns.\\n\\nThis capability is required in real-time data warehouse scenarios. Currently, the Flink and Spark engines do not support this feature.\\n### Fine-grained fault-tolerant design\\nFlink\'s design is fault tolerance and rollback at the entire job level. If a task fails, the entire job will be rolled back and restarted. The design of SeaTunnel Engine takes into account that in the data synchronization scenario, in many q cases, the failure of a task should only need to focus on fault tolerance for tasks that have upstream and downstream relationships with it. Based on this design principle, SeaTunnel Engine will first generate a logical DAG according to the user-configured job configuration file, then optimize the logical DAG, and finally generate a pipeline (a connected subgraph in a job DAG) to call and execute jobs at the granularity. fault tolerance.\\n\\nA typical usage scenario is:\\n\\nUse the CDC connector to read data from MySQL\'s binlog and write it to another MySQL. If you use Flink or Spark engine, once the target MySQL cannot write, it will cause the task of CDC to read the binlog to be terminated. If MySQL is set If the expiration time of the log is set, the problem of the target MySQL is solved, but the log of the source MySQL is cleared, which leads to data loss and other problems.\\n\\nSeaTunnel Engine will automatically optimize this synchronization task, automatically add the source to the target Cache, and then further optimize this job into two Pipelines, pipeline#1 is responsible for reading data from the CDC and writing it to the SeaTunnel Cache, and pipeline#2 is responsible for reading data from the SeaTunnel Cache Cache reads data and writes to target MySQL. If there is a problem with the target MySQL and cannot be written, the pipeline#2 of this synchronization job will be terminated, and the pipeline#1 will still run normally. This design fundamentally solves the above problems and is more in line with the processing logic of the data synchronization engine.\\n### Dynamically share threads to reduce resource usage\\nSeaTunnel Engine\'s Task design uses shared thread technology. Different from Flink/Spark, SeaTunnel Engine does not simply allow a Task to occupy a thread, but through a dynamic perception method - Dynamic Thread Sharing (Dynamic Thread Sharing) To judge whether a Task should share a thread with other Tasks or should monopolize a thread.\\n\\nCompared with single-threaded serial computing, multi-threaded parallel computing has better performance advantages, but if each Task uses an independent thread to run, when there are many tables for data synchronization and the number of Tasks is large, it will be in the Worker node Start very many threads on it. When the number of CPU cores is fixed, the more threads, the better. When the number of threads is too large, the CPU needs to spend a lot of time on thread context switching, which will affect computing performance.\\n\\nFlink/Spark usually limits the maximum number of tasks running on each node. In this way, it can avoid starting too many threads. To run more tasks on one node, SeaTunnel Engine can share thread technology. Let those tasks with a small amount of data share threads, and tasks with a large amount of data exclusively use threads. This method makes it possible for SeaTunnel Engine to run hundreds or thousands of table synchronization tasks on one node, with less resource occupation. Complete the synchronization of more tables.\\n## 05 Basic functions and features\\n\\n2.3.0 is the first official version of SeaTunnel Engine, which implements some basic functions. For the detailed design, please refer to: https://github.com/apache/incubator-seatunnel/issues/2272\\n\\n**[ Cluster Management ]**\\n* Support stand-alone operation\\n* Support cluster operation\\n* Autonomous cluster (no centralization), no need to specify a Master node for the SeaTunnel Engine cluster, SeaTunnel Engine elects the Master node by itself during operation and automatically selects a new Master node after the Master node hangs up.\\n* Automatic discovery of cluster nodes, the nodes with the same cluster_name will automatically form a cluster.\\n\\n**[ Core function ]**\\n* Supports running jobs in Local mode. The cluster is automatically destroyed after the job runs.\\n* It supports running jobs in Cluster mode (single machine or cluster) and submitting jobs to the SeaTunnel Engine service through SeaTunnel Client. After the job is completed, the service continues to run and waits for the next job submission.\\n* Support offline batch synchronization.\\n* Support real-time synchronization.\\n* Batch and flow integration, all SeaTunnel V2 version connectors can run in SeaTunnel Engine.\\n* Supports distributed snapshot algorithm cooperates with SeaTunnel V2 connector to support two-phase commit, and ensures data exactly-once.\\n* Supports job invocation at the Pipeline level to ensure that it can be started even when resources are limited.\\n* Supports job fault tolerance at the Pipeline level. The failure of a Task only affects the Pipeline it is in, and only the Task under the Pipeline needs to be rolled back.\\n* Supports dynamic thread sharing to achieve real-time synchronization of a large number of small data sets.\\n## 06 Future optimization plan\\n\\n* Support Cache mode, and first support Kafka as Cache\\n* Support JobHistory, support the persistence of JobHistory.\\n* Support indicator (Reader Rows, QPS, Reader Bytes) monitoring and indicator query\\n* Support dynamic modification of the execution plan.\\n* Support CDC.\\n* Support whole database synchronization\\n* Support multi-table synchronization\\n* Support for Schema Revolution"},{"id":"Apache IoTDB (Internet of Things Database) is a software system that integrates the collection","metadata":{"permalink":"/zh-CN/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-12-10-SeaTunnel-supports-IoTDB-to-implement-IoT-data-synchronization.md","source":"@site/blog/2022-12-10-SeaTunnel-supports-IoTDB-to-implement-IoT-data-synchronization.md","title":"SeaTunnel supports IoTDB to implement IoT data synchronization","description":"Apache IoTDB (Internet of Things Database) is a software system that integrates the collection, storage, management, and analysis of time series data of the Internet of Things, which can meet the needs of massive data storage, high-speed data reading, and complex data analysis in the field of Industrial Internet of Things. Currently, SeaTunnel already supports IoTDB Connector, realizing the connection of data synchronization scenarios in the IoT field.","date":"2022-12-10T00:00:00.000Z","formattedDate":"2022\u5e7412\u670810\u65e5","tags":[{"label":"Meetup","permalink":"/zh-CN/blog/tags/meetup"}],"readingTime":11.6,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"Apache IoTDB (Internet of Things Database) is a software system that integrates the collection","title":"SeaTunnel supports IoTDB to implement IoT data synchronization","tags":["Meetup"]},"prevItem":{"title":"In the recently released SeaTunnel 2.3.0 official version","permalink":"/zh-CN/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!"},"nextItem":{"title":"SeaTunnel engine, designed for tens-of-billions data integration","permalink":"/zh-CN/blog/Apache SeaTunnel Committer | Zongwen Li"}},"content":"![](/image/16714316310459/16714316482580.jpg)\\n> Apache IoTDB (Internet of Things Database) is a software system that integrates the collection, storage, management, and analysis of time series data of the Internet of Things, which can meet the needs of massive data storage, high-speed data reading, and complex data analysis in the field of Industrial Internet of Things. Currently, SeaTunnel already supports IoTDB Connector, realizing the connection of data synchronization scenarios in the IoT field.\\n\\n> At the SeaTunnel community online meeting in October this year, SeaTunnel Committer Wang Hailin introduced the implementation process of SeaTunnel\u2019s access to IoTDB, allowing users to have a deeper understanding of the operation method and principle of IoTDB data synchronization.\\n\\nThe topic I\u2019m sharing today is using SeaTunnel to play around with data synchronization in IoTDB.\\n\\nThis session is divided into 6 subsections. Firstly, we will have an understanding of the basic concept of SeaTunnel, and on this basis, we will focus on the functional features of IoTDB Connector, then we will analyze the data read and write functions of IoTDB Connector and the parsing of the implementation, and finally, we will show some typical usage scenarios and cases to let you understand how to use Finally, we will show some typical usage scenarios and cases to understand how to use the IoTDB Connector to implement into production environments. The last point is the community\u2019s next steps for the IoTDB Connector and guidance on how to get involved in contributing.\\n\\n## Introduction to SeaTunnel basic concepts\\nThis is the basic architecture of SeaTunnel, an engine built for data synchronization, with a set of abstract APIs for reading data from and writing to a variety of data sources.\\n\\n![](/image/16714316310459/16714316839299.jpg)\\nThe left-hand side briefly lists the Source scenarios, for example, we abstract the Source\u2019s API, Type, and State, to read the data source, unifying the data types of the various data sources to the abstract type defined in it, and some state recovery and retention of the read location during the reading process.\\n\\nThis is an abstraction for Source, and we have done a similar abstraction for Sink, i.e. how data is written, and how the data type matches the real data source type, and how the state is restored and retained.\\n\\nBased on these APIs, we will have a translation layer to translate these APIs to the corresponding execution engine. SeaTunnel currently supports three execution engines, Spark, Flink, and our own execution engine, SeaTunnel Engine, which will be released soon.\\n\\nThis is roughly what SeaTunnel does, SeaTunnel relies on Source and Sink to read and write data for data synchronization, we call them Connectors. The Connector consists of a Source and a Sink.\\n\\n![](/image/16714316310459/16714316928812.jpg)\\nFrom the diagram above we see the different data sources, Source is responsible for reading data from the various data sources and transforming it into SeaTunnelRow abstraction layer and Type to form the abstraction layer, Sink is responsible for pulling data from the abstraction layer and writing it to the concrete data store to transform it into the store concrete format.\\n\\nThe combination of Source + Abstraction Layer + Sink enables the synchronization of data between multiple heterogeneous data sources.\\n\\nI\u2019ll use a simple example below to illustrate how SeaTunnel\u2019s Source and Sink work.\\n\\n![](/image/16714316310459/16714317022389.jpg)\\n\\n![](/image/16714316310459/16714317067444.jpg)\\nWe can specify the number of Sources, Sink configuration file combinations through the configuration file The commands in the toolkit provided by SeaTunnel take the configuration file with them and when executed enable data handling.\\n\\n![](/image/16714316310459/16714317166018.jpg)\\n![](/image/16714316310459/16714317203806.jpg)\\n![](/image/16714316310459/16714317262218.jpg)\\nThis is the Connector ecosystem that is currently supported by SeaTunnel, such as the data sources supported by JBDC, HDFS, Hive, Pulsar, message queues, etc. are currently supported.\\n\\nThe list in the picture is not exhaustive of the Connectors supported by SeaTunnel. Under the GitHub SeaTunnel project, you can see the Plugins directory, where supported Connector plugins are constantly being added and where you can see the latest access in real-time.\\n\\n## IoTDB Connector Features\\nBelow is information about access to the IoTDB Connector.\\n\\nFirstly, we would like to introduce the functional features of IoTDB, the IoTDB Connector integrated with SeaTunnel, and what exactly it supports for your reference.\\n\\n## Source Features\\n![](/image/16714316310459/16714317512435.jpg)\\nFirstly, there are the typical usage scenarios supported by Source, such as bulk reading of devices, field projection, data type mapping, parallel reading, etc.\\n\\nAs you can see above, IoTDB supports all features except once, exactly once and stream mode, such as batch reads, IoTDB has a SQL syntax similar to group by device, which allows you to read data from multiple devices in a single batch. For basic data type projection, the SQL in IoTDB will take time by default when looking up any metric, or group by the device will take the device column, and we also support projection onto SeaTunnel columns by default.\\n\\nThe only data type not supported is Victor, all others are supported.\\n\\nFor the parallel read piece, the IoTDB data is actually timestamped and we use timestamped ranges to achieve parallel reads.\\n\\nThe recovery of the state, since we have divided the time range read into different splits, can be done based on the Split location information.\\n\\n## Sink functional features\\n![](/image/16714316310459/16714317679569.jpg)\\n\\nThe diagram above shows the features already supported by SeaTunnel. Regarding metadata extraction, we support the extraction of metadata such as measurement, device, etc. from SeaTunnelRow and the extraction or use of current processing time from SeaTunnelRow. Batch commits and exception retries are also supported.\\n## IoTDB data reading analysis\\nNext, we analyze the implementation and support for data reading.\\n## Data type mapping\\nThe first is the data type mapping, which actually reads the IoTDB data type to SeaTunnel, so it has to be converted to the SeaTunnel data type.\\n![](/image/16714316310459/16714317930593.jpg)\\nThe BOOLEAN, INT32, INT64, etc. listed here all have corresponding SeaTunnel data types. INT32 can be mapped according to the read type on the SeaTunnel, or to TINYINT, SMALLINT, or INT when the range of values is small.\\n\\nThe Vector type is not currently supported.\\n\\n![](/image/16714316310459/16714318216373.jpg)\\nThis is the corresponding example code showing how the mapping is done where the type conversion is done.\\n\\n## Field projection\\n\\nThe other is the field projection when reading, we can automatically map Time fields when reading IoTDB data, or we can choose to map some of the data to SeaTunnel, such as TIMESTAMP, or BIGINT.\\n\\n![](/image/16714316310459/16714318381313.jpg)\\nThe SQL extraction of column codes allows you to extract only some of the columns you need, and when used on SeaTunnel, you can specify the name, type, etc. of the column after it is mapped to SeaTunnel via fields. The final result of the data read on SeaTunnel is shown in the figure above.\\n\\n![](/image/16714316310459/16714318550071.jpg)\\n\\nWe have just seen that we do not have the time column in the SQL, but the actual result is that there is this column, so we support the projection of the time column field, the time column can actually be projected into different data types, the user can convert according to their needs. The diagram above shows the implementation logic.\\n\\n## Batch read Device\\nThis is a common requirement, as we are likely to synchronize data in large batches with the same data structure.\\n\\n![](/image/16714316310459/16714318796196.jpg)\\n\\nSeaTunnel supports the align-by-device syntax so that device columns can also be projected onto the SeaTunnelRow\\n\\n![](/image/16714316310459/16714318873362.jpg)\\nAssuming there is a table in IoTDB, we project the device column onto SeaTunnel by making it data as well through syntax. After configuring the device name column and specifying the data type, we end up reading the data on SeaTunnel in the format shown above, containing the Time, device column, and the actual data value. This makes it possible to read data from the same device in bulk.\\n\\n## Parallel reading\\nThe other is a parallel read.\\n\\n* Split\\nWe have scoped the table by the Time column and if we are reading in parallel we may want to scope the table to allow parallel threads/processes to read a specific range of data. By configuring the three parameters, the end result will be a query SQL, where the original SQL is divided into different splits with query conditions to achieve the actual read SQL.\\n\\n* Allocate Split to the reader\\nOnce the split is done, there is an allocation logic to follow in order to distribute it to each parallel reader.\\n\\n![](/image/16714316310459/16714319281280.jpg)\\n\\nThis logic is based on the ID of the split to the reader, which may be more random, or more uniform if the ID of the split is more hashed, depending on the Connector.\\n\\n![](/image/16714316310459/16714319372730.jpg)\\n\\nThe result achieved is shown in the picture.\\n\\n# Status recovery\\n\\nThere is also state recovery involved when reading because if the task is large, the reading will take longer, and if there is an error or exception in the middle, you have to consider how to recover the state from the point where the error occurred, and then read it again afterward.\\n\\n![](/image/16714316310459/16714319281280.jpg)\\n![](/image/16714316310459/16714319569097.jpg)\\n![](/image/16714316310459/16714319599521.jpg)\\n\\nSeaTunnel\u2019s state recovery is mainly through the reader storing the unread Split information into the state, and then the engine will periodically take a snapshot of the state when reading so that we can restore the last snapshot when we recover and continue reading afterward.\\n\\n## IoTDB Connector Data Write Analysis\\nThe next step is the parsing of the data writes.\\n\\n## Data type mapping\\n![](/image/16714316310459/16714319862080.jpg)\\n\\nData writing also involves data type mapping, but here, in contrast to data reading, it maps the SeaTunnel data types to the IoTDB data types. As IoTDB only has INT32, the writing process involves lifting the data types TINYINT and SMALLINT. All other data types can be converted one-to-one; ARRAY and VECTOR data types are not yet supported.\\n\\n![](/image/16714316310459/16714319949478.jpg)\\n\\nThe above diagram shows the corresponding code, the implementation logic will need to be seen in our specific mapping.\\n\\n## Dynamic injection of metadata\\nSeaTunnel supports the dynamic injection of metadata.\\n\\nWhen heterogeneous data sources are written to the IoTDB, device, measurement, and time are extracted from each row of data, either by serializing the SeaTunnelRow with a fixed column value as configured. Alternatively, the system time can be used as the time, or the current system time can be populated if no time column is specified, and the storage group can be configured to be automatically appended to the device prefix.\\n\\n![](/image/16714316310459/16714320117277.jpg)\\n\\nFor example, suppose that the structure of a row in SeaTunnel reading the data format shown above can be configured to synchronize to the IoTDB and the result obtained is as follows.\\n\\n![](/image/16714316310459/16714320201848.jpg)\\n\\nThe temperature and humidity columns we need were extracted, and ts and device names were extracted as the original data for the IoTDB.\\n\\n## Batch commits and exception retries\\n\\nIn addition, Sink needs to handle batch and retry when writing. For batches, we can configure the appropriate batch configuration, including support for configuring the number and interval of batch commits; if the data is cached to memory, you can enable a separate thread for timed commits.\\n\\nFor retries, SeaTunnel supports the configuration of the number of retries, the waiting interval and the maximum number of retries, as well as the possibility to end a retry if it encounters a non-recoverable error when it has finished.\\n\\n![](/image/16714316310459/16714320394193.jpg)\\n\\n## IoTDB Connector Usage Examples\\n\\nAfter the previous analysis of reading and writing data, let\u2019s look at three typical examples of usage scenarios.\\n\\n## Exporting data from IoTDB\\nThe first scenario is exporting data from the IoTDB, the example I have given here is reading data from the IoTDB to the Console.\\n\\n* Read in parallel, output to Console\\n\\nParallelism: 2\\n\\nNumber of batches: 24\\n\\nTime frame: 2022\u201309\u201325 ~ 2022\u201309\u201326\\n![](/image/16714316310459/16714320856052.jpg)\\n\\nLet\u2019s assume that we have a data table in IoTDB and we want to export the data to the Console. The whole configuration is shown above and needs to map the columns of data we want to export and the time range to check.\\n\\nThis is the simplest example, but in practice, the Sink side may be more complex, so you will need to refer to the documentation of the corresponding data source for the appropriate configuration.\\n\\n## Importing data to IoTDB\\n\\n* Read database, batch write to IoTDB\\n    * Batch writing: one commit every 1024 entries or every 1000 ms\\n    \\n    * -Extracting metadata device, timestamp, measurement\\n    \\n    * -Specify the storage group: root.test_group\\n\\n![](/image/16714316310459/16714321290339.jpg)\\nAnother typical usage scenario is to import data from other data sources into IoTDB. suppose I have an external database table with columns like ts, temperature, humidity, etc. and we import it into IoTDB, requiring the columns of temperature and humidity, but the rest can be left out. The whole configuration is shown in the diagram above, you can refer to it.\\n\\nOn the Sink side, you mainly have to specify the Key of the device column, such as from which data the device is extracted, from which class the time is extracted, which columns to write to the IoTDB, etc.\\n\\nAs you can see, we can configure the storage group, which is the storage group of the IoTDB, which can be specified by the storage group.\\n\\n## Synchronizing data between IoTDB\\nThe third scenario is to synchronize data between IoTDB and IoTDB and write to IoTDB in bulk, suppose there is a table in IoTDB that needs to be synchronized to another IoTDB, after synchronization the storage group has changed and the name of the indicator of the data column has also changed, then you can use projection to rewrite the indicator name and use SQL to rewrite the storage group.\\n\\n![](/image/16714316310459/16714321480992.jpg)\\n\\n## How to get involved in contribution\\nFinally, a few words about the next steps for the IoTDB Connector and how you can get involved in improving the Connector and contributing new features that are needed.\\n\\n## Next steps for the IoTDB Connector\\n\\n* Support for reading and writing vector data types\\n* Support for tsfile reads and writes\\n* Support for writing tsfile and reloading to IoTDB"},{"id":"Apache SeaTunnel Committer | Zongwen Li","metadata":{"permalink":"/zh-CN/blog/Apache SeaTunnel Committer | Zongwen Li","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-12-9-SeaTunnel-engine-designed-for-tens-of-billions-data-integration.md","source":"@site/blog/2022-12-9-SeaTunnel-engine-designed-for-tens-of-billions-data-integration.md","title":"SeaTunnel engine, designed for tens-of-billions data integration","description":"Apache SeaTunnel Committer | Zongwen Li","date":"2022-12-09T00:00:00.000Z","formattedDate":"2022\u5e7412\u67089\u65e5","tags":[{"label":"Meetup","permalink":"/zh-CN/blog/tags/meetup"}],"readingTime":9.925,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"Apache SeaTunnel Committer | Zongwen Li","title":"SeaTunnel engine, designed for tens-of-billions data integration","tags":["Meetup"]},"prevItem":{"title":"SeaTunnel supports IoTDB to implement IoT data synchronization","permalink":"/zh-CN/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection"},"nextItem":{"title":"Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!","permalink":"/zh-CN/blog/During the joint Apache SeaTunnel & IoTDB Meetup on October 15,"}},"content":"![](/image/16714309762810/16714309876928.jpg)\\nApache SeaTunnel Committer | Zongwen Li\\n\\n## Introduction to Apache SeaTunnel\\nApache SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data.\\n\\nApache SeaTunnel will try its best to solve the problems that may be encountered in the process of mass data synchronization, such as data loss and duplication, task accumulation and delay, low throughput, etc.\\n\\n## Milestones of SeaTunnel\\nSeaTunnel, formerly known as Waterdrop, was open-sourced on GitHub in 2017.\\n\\nIn October 2021, the Waterdrop community joined the Apache incubator and changed its name to SeaTunnel.\\n\\n## SeaTunnel Growth\\n\\n![](/image/16714309762810/16714310892722.jpg)\\n![](/image/16714309762810/16714310916195.jpg)\\n![](/image/16714309762810/16714310939883.jpg)\\nWhen SeaTunnel entered the Apache incubator, the SeaTunnel community ushered in rapid growth.\\n\\nAs of now, the SeaTunnel community has a total of 151 contributors, 4314 Stars, and 804 forks.\\n\\n## Pain points of Existing engines\\nThere are many pain points faced by the existing computing engines in the field of data integration, and we will talk about this first. The pain points usually lie in three directions:\\n\\n* The fault tolerance ability of the engine;\\n* Difficulty in configuration, operation, and maintenance of engine jobs;\\n* The resource usage of the engine.\\n\\n## fault tolerance\\nGlobal Failover\\n![Global-failover](/image/16714309762810/16714311670656.jpg)\\nFor distributed streaming processing systems, high throughput and low latency are often the most important requirements. At the same time, fault tolerance is also very important in distributed systems. For scenarios that require high correctness, the implementation of exactly once is often very important.\\n\\nIn a distributed streaming processing system, since the computing power, network, load, etc. of each node are different, the state of each node cannot be directly merged to obtain a true global state. To obtain consistent results, the distributed processing system needs to be resilient to node failure, that is, it can recover to consistent results when it fails.\\n\\nAlthough it is claimed in their official blog that Spark\u2019s Structured Streaming uses the Chandy-Lamport algorithm for Failover processing, it does not disclose more details.\\n\\nFlink implemented Checkpoint as a fault-tolerant mechanism based on the above algorithm and published related papers: Lightweight Asynchronous Snapshots for Distributed Dataflows\\n\\nIn the current industrial implementation, when a job fails, all nodes of the job DAG need to failover, and the whole process will last for a long time, which will cause a lot of upstream data to accumulate.\\n\\n## Loss of Data\\n![](/image/16714309762810/16714312426416.jpg)\\nThe previous problem will cause a long-time recovery, and the business service may accept a certain degree of data delay.\\n\\nIn a worse case, a single sink node cannot be recovered for a long time, and the source data has a limited storage time, such as MySQL and Oracle log data, which will lead to data loss.\\n\\n## Configuration is cumbersome\\nSingle table Configuration\\n\\n![](/image/16714309762810/16714312637015.jpg)\\nThe previous examples are cases regarding a small number of tables, but in real business service development, we usually need to synchronize thousands of tables, which may be divided into databases and tables at the same time;\\n\\nThe status quo is that we need to configure each table, a large number of table synchronization takes a lot of time for users, and it is prone to problems such as field mapping errors, which are difficult to maintain.\\n\\n## Not supporting Schema Evolution\\n\\n![Not-supports-DDL](/image/16714309762810/16714312769761.jpg)\\nBesides, according to the research report of Fivetran, 60% of the company\u2019s schema will change every month, and 30% will change every week.\\n\\nHowever, none of the existing engines supports Schema Evolution. After changing the Schema each time, the user needs to reconfigure the entire link, which makes the maintenance of the job very cumbersome.\\n\\n## The high volume of resource usage\\n\\nThe database link takes up too much\\n\\n![](/image/16714309762810/16714313100541.jpg)\\nIf our Source or Sink is of JDBC type, since the existing engine only supports one or more links per table, when there are many tables to be synchronized, more link resources will be occupied, which will bring a great burden to the database server.\\n## Operator pressure is uncontrollable\\n\\n![](/image/16714309762810/16714313301435.jpg)\\nIn the existing engine, a buffer and other control operators are used to control the pressure, that is, the back pressure mechanism; since the back pressure is transmitted level by level, there will be pressure delay, and at the same time, the processing of data will not be smooth enough, increasing the GC time, fault-tolerant completion time, etc.\\n\\nAnother case is that neither the source nor the sink has reached the maximum pressure, but the user still needs to control the synchronization rate to prevent too much impact on the source database or the target database, which cannot be controlled through the back pressure mechanism.\\n\\n## Architecture goals of Apache SeaTunnel Engine\\nTo solve these severe issues faced by computing engines, we self-developed our engine expertise in big data integration.\\n\\nFirstly, let\u2019s get through what goals this engine wants to achieve.\\n\\n## Pipeline Failover\\n\\n![](/image/16714309762810/16714313559400.jpg)\\nIn the data integration case, there is a possibility that a job can synchronize hundreds of sheets, and the failure of one node or one table will lead to the failure of all tables, which is too costly.\\n\\nWe expect that unrelated Job Tasks will not affect each other during fault tolerance, so we call a vertex collection with upstream and downstream relationships a Pipeline, and a Job can consist of one or more pipelines.\\n\\n## Regional Failover\\nNow if there is an exception in the pipeline, we still need to failover all the vertex in the pipeline; but can we restore only part of the vertex?\\n![](/image/16714309762810/16714313919617.jpg)\\nFor example, if the Source fails, the Sink does not need to restart. In the case of a single Source and multiple Sinks, if a single Sink fails, only the Sink and Source that failed will be restored; that is, only the node that failed and its upstream nodes will be restored.\\n\\nObviously, the stateless vertex does not need to be restarted, and since SeaTunnel is a data integration framework, we do not have aggregation state vertexes such as Agg and Count, so we only need to consider Sink;\\n\\n* Sink does not support idempotence & 2PC; no restart and restart will result in the same data duplication, which can only be solved by Sink without restarting;\\n* Sink supports idempotence, but does not support 2PC: because it is idempotent writing, it does not matter whether the source reads data inconsistently every time, and it does not need to be restarted;\\n* Sink supports 2PC:\\n* If the Source supports data consistency, if an abort is not executed, the processed old data will be automatically ignored through the channel data ID, and at the same time, it will face the problem that the transaction session time may time out;\\n* If the Source does not support data consistency, perform abort on the Sink to discard the last data, which has the same effect as restarting but does not require initialization operations such as re-establishing links;\\n* That is, the simplest implementation is to execute abort.\\nWe use the pipeline as the minimum granularity for fault-tolerant management, and use the Chandy-Lamport algorithm to realize fault-tolerant distributed jobs.\\n\\n## Data Cache\\n![](/image/16714309762810/16714314318184.jpg)\\nFor sink failure, when data cannot be written, a possible solution is to work two jobs at the same time.\\n\\nOne job reads the database logs using the CDC source connector and then writes the data to Kafka using the Kafka Sink connector. Another job reads data from Kafka using the Kafka source connector and writes data to the destination using the destination sink connector.\\n\\nThis solution requires users to have a deep understanding of the underlying technology, and both tasks will increase the difficulty of operation and maintenance. Because every job needs JobMaster, it requires more resources.\\n\\nIdeally, the user only knows that they will be reading data from the source and writing data to the sink, and at the same time, during this process, the data can be cached in case the sink fails. The sync engine needs to automatically add caching operations to the execution plan and ensure that the source still works in the event of a sink failure. In this process, the engine needs to ensure that the data written to the cache and read from the cache are transactional, to ensure data consistency.\\n\\n## Sharding & Multi-table Sync\\n![](/image/16714309762810/16714314489916.jpg)\\n\\nFor a large number of table synchronization, we expect that a single Source can support reading multiple structural tables, and then use the side stream output to keep consistent with a single table stream.\\n\\nThe advantage of this is that it can reduce the link occupation of the data source and improve the utilization rate of thread resources.\\n\\nAt the same time, in SeaTunnel Engine, these multiple tables will be regarded as a pipeline, which will increase the granularity of fault tolerance; there are trade-offs, and the user can choose how many tables a pipeline can pass through.\\n\\n## Schema Evolution\\n![](/image/16714309762810/16714314658701.jpg)\\nSchema Evolution is a feature that allows users to easily change the current schema of a table to accommodate changing data over time. Most commonly, it is used when performing an append or overwrite operation, to automatically adjust the schema to include one or more new columns.\\n\\nThis feature is required for real-time data warehouse scenarios. Currently, the Flink and Spark engines do not support this feature.\\n\\nIn SeaTunnel Engine, we will use the Chandy-Lamport algorithm to send DDL events, make them flow in the DAG graph and change the structure of each operator, and then synchronize them to the Sink.\\n\\n## Shared Resource\\n![Shared-resource](/image/16714309762810/16714314806989.jpg)\\nThe Multi-table feature can reduce the use of some Source and Sink link resources. At the same time, we have implemented Dynamic Thread Resource Sharing in SeaTunnel Engine, reducing the resource usage of the engine on the server.\\n\\n## Speed Control\\n![](/image/16714309762810/16714315001348.jpg)\\nAs for the problems that cannot be solved by the back pressure mechanism, we will optimize the Buffer and Checkpoint mechanism:\\n\\n* Firstly, We try to allow Buffer to control the amount of data in a period;\\n* Secondly, by the Checkpoint mechanism, the engine can lock the buffer after the Checkpoint reaches the maximum number of parallelism and executes an interval time, prohibiting the writing of Source data, achieving the result of taking the pressure proactively, avoiding issues like back pressure delay or failure to be delivered to Source.\\nThe above is the design goal of SeaTunnel Engine, hoping to help you better solve the problems that bother you in data integration. In the future, we will continue to optimize the experience of using SeaTunnel so that more people are willing to use it.\\n\\n## The future of Apache SeaTunnel\\nAs an Apache incubator project, the Apache SeaTunnel community is developing rapidly. In the following community planning, we will focus on four directions:\\n\\nSupport more data integration scenarios (Apache SeaTunnel Engine)\\nIt is used to solve the pain points that existing engines cannot solve, such as the synchronization of the entire database, the synchronization of table structure changes, and the large granularity of task failure;\\n> Guys who are interested in the engine can pay attention to this Umbrella: https://github.com/apache/incubator-seatunnel/issues/2272\\n\\nExpand and improve Connector & Catalog ecology\\nSupport more Connector & Catalog, such as TiDB, Doris, Stripe, etc., and improve existing connectors, improve their usability and performance, etc.;\\nSupport CDC connector for real-time incremental synchronization scenarios.\\n> Guys who are interested in connectors can pay attention to this Umbrella: https://github.com/apache/incubator-seatunnel/issues/1946\\n\\nSupport for more versions of the engines\\nSuch as Spark 3.x, Flink 1.14.x, etc.\\n> Guys who are interested in supporting Spark 3.3 can pay attention to this PR: https://github.com/apache/incubator-seatunnel/pull/2574\\n\\nEasier to use (Apache SeaTunnel Web)\\nProvides a web interface to make operations more efficient in the form of DAG/SQL Simple and more intuitive display of Catalog, Connector, Job, etc.;\\nAccess to the scheduling platform to make task management easier\\n> Guys who are interested in Web can pay attention to our Web sub-project: https://github.com/apache/incubator-seatunnel-web"},{"id":"During the joint Apache SeaTunnel & IoTDB Meetup on October 15,","metadata":{"permalink":"/zh-CN/blog/During the joint Apache SeaTunnel & IoTDB Meetup on October 15,","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-11-17-Mafengwo-finally-chose-Apache-SeaTunnel-after-analyzing-these-9-points-of-how-it-works.md","source":"@site/blog/2022-11-17-Mafengwo-finally-chose-Apache-SeaTunnel-after-analyzing-these-9-points-of-how-it-works.md","title":"Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!","description":"Bo Bi, data engineer at Mafengwo","date":"2022-11-17T00:00:00.000Z","formattedDate":"2022\u5e7411\u670817\u65e5","tags":[{"label":"Meetup","permalink":"/zh-CN/blog/tags/meetup"}],"readingTime":18.22,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"During the joint Apache SeaTunnel & IoTDB Meetup on October 15,","title":"Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!","tags":["Meetup"]},"prevItem":{"title":"SeaTunnel engine, designed for tens-of-billions data integration","permalink":"/zh-CN/blog/Apache SeaTunnel Committer | Zongwen Li"},"nextItem":{"title":"A tutorial to help you develop a SeaTunnel Connector hand-by-hand while avoiding pitfalls","permalink":"/zh-CN/blog/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls"}},"content":"![](/image/16714322747890/16714322908857.jpg)\\n\\n![](/image/16714322747890/16714322944041.jpg)\\nBo Bi, data engineer at Mafengwo\\n\\n> During the joint Apache SeaTunnel & IoTDB Meetup on October 15, Bo Bi, the data engineer at a leading Chinese travel-social e-commerce platform Mafengwo, introduced the basic principles of SeaTunnel and related enterprise practice thinking, the pain points and optimization thinking in typical scenarios of Mafengwo\u2019s big data development and scheduling platform, and shared his experience of participating in community contributions. We hope to help you understand SeaTunnel and the paths and skills of community building at the same time.\\n\\n\\n## Introduction to the technical principle of SeaTunnel\\nSeaTunnel is a distributed, high-performance data integration platform for the synchronization and transformation of large volumes of data (offline and real-time)\\n\\nThe diagram above shows the workflow of SeaTunnel, which in simple terms consists of 3 parts: input, transformation, and output; more complex data processing is just a combination of several actions.\\n\\nIn a synchronization scenario, such as importing Kafka to Elasticsearch, Kafka is the Source of the process and Elasticsearch is the Sink of the process.\\n\\nIf, during the import process, the field columns do not match the external data columns to be written and some column or type conversion is required, or if you need to join multiple data sources and then do some data widening, field expansion, etc., then you need to add some Transform in the process, corresponding to the middle part of the picture.\\n\\n![](/image/16714322747890/16714323322988.jpg)\\nThis shows that the core of SeaTunnel is the Source, Transform and Sink process definitions.\\n\\nIn Source we can define the data sources we need to read, in Sink, we can define the data pipeline and eventually write the external storage, and we can transform the data in between, either using SQL or custom functions.\\n\\n## SeaTunnel Connector API Version V1 Architecture Breakdown\\nFor a mature component framework, there must be something unique about the design pattern of the API design implementation that makes the framework scalable.\\n\\nThe SeaTunnel architecture consists of three main parts.\\n\\n1\u3001SeaTunnel Basic API.\\n\\n1. the implementation of the SeaTunnel base API.\\n\\n2. SeaTunnel\u2019s plug-in system.\\n\\n## SeaTunnel Basic API\\n![](/image/16714322747890/16714323668557.jpg)\\nThe above diagram shows the definition of the interface, the Plugin interface in SeaTunnel abstracts the various actions of data processing into a Plugin.\\n\\nThe five parts of the diagram below, Basesource, Basetransfform, Basesink, Runtimeenv, and Execution, all inherit from the Plugin interface.\\n![](/image/16714322747890/16714323741126.jpg)\\n\\nAs a process definition plug-in, Source is responsible for reading data, Transform is responsible for transforming, Sink is responsible for writing and Runtimeenv is setting the base environment variables.\\n\\nThe overall SeaTunnel base API is shown below\\n\\n![](/image/16714322747890/16714323846302.jpg)\\nExecution, the data flow builder used to build the entire data flow based on the first three, is also part of the base API\\n\\n![](/image/16714322747890/16714323920717.jpg)\\n\\n## SeaTunnel Base API Implementation\\n\\nBased on the previous basic APIs, SeaTunnel has been implemented in separate packages for different computing engines, currently the Spark API abstraction and the Flink API abstraction, which logically completes the process of building the data pipeline.\\n\\n![](/image/16714322747890/16714323741126.jpg)\\n\\nDue to space constraints, we will focus on Spark batch processing. Based on the wrapped implementation of the previous base Api, the first is that Base spark source implements Base source, base Spark transform implements Base transform and Base Spark sink implements Base sink.\\n\\nThe method definition uses Spark\u2019s Dataset as the carrier of the data, and all data processing is based on the Dataset, including reading, processing and exporting.\\n\\nThe SparkEnvironment, which internally encapsulates Spark\u2019s Sparksession in an Env, makes it easy for individual plugins to use.\\n\\n![](/image/16714322747890/16714324136843.jpg)\\n\\nThe Spark batch process ends with SparkBatchExecution (the data stream builder), which is the core code snippet used to functionally build our data stream Pipeline, the most basic data stream on the left in the diagram below.\\n\\nThe user-based definition of each process component is also the configuration of Source Sink, Transform. More complex data flow logic can be implemented, such as multi-source Join, multi-pipeline processing, etc., all of which can be built through Execution.\\n\\n![](/image/16714322747890/16714324237449.jpg)\\n## SeaTunnel Connector V1 API Architecture Summary\\n![](/image/16714322747890/16714324336701.jpg)\\nSeaTunnel\u2019s API consists of three main parts.\\n\\nThe first part is the SeaTunnel base API, which provides the basic abstract interfaces such as Source, Sink, Transform, and Plugin.\\n\\nThe second part is based on a set of interfaces Transform, Sink, Source, Runtime, and Execution provided by the SeaTunnel base API, which is wrapped and implemented on the Flink and Spark engines respectively, i.e. Spark engine API layer abstraction and Flink engine API layer abstraction.\\n\\nBoth Flink and Spark engines support stream and batch processing, so there are different ways to use streams/batches under the Flink API abstraction and Spark abstraction APIs, such as Flinkstream and Flinkbatch under the Flink abstraction API, and Sparkbatch and Sparkstreaming under the Spark abstraction API.\\n\\nThe third part is the plug-in system, based on Spark abstraction and Flink API abstraction, SeaTunnel engine implements rich connectors and processing plug-ins, while developers can also be based on different engine API abstractions, and extensions to achieve their own Plugin.\\n\\nSeaTunnel Implementation Principle\\nCurrently, SeaTunnel offers a variety of ways to use Flink, Spark, and FlinkSQL. Due to space limitations, we will introduce the execution principles of the Spark method.\\n\\nFirst, the entry starts the command Start-seatunnel-spark.sh via the shell, which internally calls Sparkstarter\u2019s Class, which parses the parameters passed by the shell script, and also parses the Config file to determine which Connectors are defined in the Config file, such as Fake, Console, etc.\\n![](/image/16714322747890/16714324454477.jpg)\\nThen find the Connector path from the Connector plugin directory and stitch it into the Spark-submit launch command with \u2014 jar, so that the found Plugin jar package can be passed to the Spark cluster as a dependency.\\n\\nFor Connector plugins, all Spark Connectors are packaged in the plugin directory of the distribution (this directory is managed centrally).\\n\\nAfter Spark-submit is executed, the task is submitted to the Spark cluster, and the Main class of the Spark job\u2019s Driver builds the data flow Pipeline through the data flow builder Execution, combined with Souce, Sink, and Transform so that the whole chain is connected.\\n\\n## SeaTunnel Connector V2 API Architecture\\n\\nIn the latest community release of SeaTunnel 2.2.0-beta, the refactoring of the Connectorapi, now known as the SeaTurnelV2 API, has been completed!\\n\\nWhy do we need to reconfigure?\\n\\nAs the Container is currently a strongly coupled engine, i.e. Flink and Spark API, if the Flink or Spark engine is upgraded, the Connector will also have to be adjusted, possibly with changes to parameters or interfaces.\\n\\nThis can lead to multiple implementations for different engines and inconsistent parameters to develop a new Connector. Therefore, the community has designed and implemented the V2 version of the API based on these pain points.\\n\\n![](/image/16714322747890/16714324726276.jpg)\\n\\n## SeaTunnel V2 API Architecture\\n\\nSeaTunnel V2 API Architecture\\n\\n### 1.Table API\\n\\n\xb7DataType: defines SeaTunnel\u2019s data structure SeaTunnelRow, which is used to isolate the engine\\n\\n\xb7Catalog: used to obtain Table Scheme, Options, etc..\\n\\n\xb7Catalog Storage: used to store user-defined Table Schemes etc. for unstructured engines such as Kafka.\\n\\n\xb7Table SPI: mainly used to expose the Source and Sink interfaces as an SPI\\n\\n### 2. Source & Sink API\\n\\nDefine the Connector\u2019s core programming interface for implementing the Connector\\n\\n### 3.Engine API\\n\xb7Translation: The translation layer, which translates the Source and Sink APIs implemented by the Connector into a runnable API inside the engine.\\n\\n\xb7Execution: Execution logic, used to define the execution logic of Source, Transform, Sink and other operations within the engine.\\n\\nThe Source & Sink API is the basis for the implementation of the connector and is very important for developers.\\n\\nThe design of the v2 Source & Sink API is highlighted below\\n\\n## SeaTunnel Connector V2 Source API\\nThe current version of SeaTunnel\u2019s API design draws on some of Flink\u2019s design concepts, and the more core classes of the Source API are shown below.\\n\\n![](/image/16714322747890/16714325444078.jpg)\\n![](/image/16714322747890/16714325474972.jpg)\\nThe core Source API interaction flow is shown above. In the case of concurrent reads, the enumerator SourceSplitEnumerator is required to split the task and send the SourceSplit down to the SourceReader, which receives the split and uses it to read the external data source.\\n\\nIn order to support breakpoints and Eos semantics, it is necessary to preserve and restore the state, for example by preserving the current Reader\u2019s Split consumption state and restoring it after a failure in each Reader through the Checkpoint state and Checkpoint mechanism, so that the data can be read from the place where it failed.\\n\\n## SeaTunnel Connector V2 Sink API\\n![](/image/16714322747890/16714325600316.jpg)\\nThe overall Sink API interaction flow is shown in the diagram below. The SeaTunnel sink is currently designed to support distributed transactions, based on a two-stage transaction commit.\\n\\nFirst SinkWriter continuously writes data to an external data source, then when the engine does a checkpoint, it triggers a first-stage commit.\\n\\nSinkWriter needs to do a Prepare commit, which is the first stage of the commit.\\n\\nThe engine will determine if all the Writer\'s first stage succeeds, and if they all succeed, the engine will combine the Subtask\u2019s Commit info with the Commit method of the Committer to do the actual commit of the transaction and operate the database for the Commit, i.e. the second stage of the commit. This is the second stage of commit.\\n\\n![](/image/16714322747890/16714325681738.jpg)\\nFor the Kafka sink connector implementation, the first stage is to do a pre-commit by calling KafkaProducerSender.prepareCommit().\\n\\nThe second commit is performed via Producer.commitTransaction();.\\n\\nflush(); flushes the data from the Broker\u2019s system cache to disk.\\n\\nFinally, it is worth noting!\\n\\nBoth SinkCommitter and SinkAggregatedCommitter can perform a second stage commit to replace the Committer in the diagram. The difference is that SinkCommitter can only do a partial commit of a single Subtask\u2019s CommitInfo, which may be partially successful and partially unsuccessful, and cannot be handled globally. The difference is that the SinkCommitter can only do partial commits of a single Subtask\u2019s CommitInfo, which may be partially successful and partially unsuccessful.\\n\\nSinkAggregatedCommitter is a single parallel, aggregating the CommitInfo of all Subtask, and can do the second stage commit as a whole, either all succeed or all fail, avoiding the problem of inconsistent status due to partial failure of the second stage.\\n\\nIt is therefore recommended that the SinkAggregatedCommitter be used in preference.\\n\\n## Comparison of SeaTunnel V1 and V2 API processing flows\\nWe can look at the changes before and after the V1 V2 upgrade from a data processing perspective, which is more intuitive, Spark batch processing as an example: SeaTunnel V1: The entire data processing process is based on the Spark dataset API, and the Connector and the compute engine are strongly coupled.\\n\\n![](/image/16714322747890/16714325887598.jpg)\\nSeaTunnel V2: Thanks to the work of the engine translator, the Connector API, and the SeaTunnelRow, the data source of the SeaTunnel internal data structures accessed through the Connector, are translated by the translation layer into a runnable Spark API and spark dataset that is recognized inside the engine during data transformation.\\n\\nAs data is written out, the Spark API and Spark dataset are translated through the translation layer into an executable connector API inside the SeaTunnel connector and a data source of internal SeaTunnel structures that can be used.\\n\\n> Overall, the addition of a translation layer at the API and compute engine layers decouples the Connector API from the engine, and the Connector implementation no longer depends on the compute engine, making the extension and implementation more flexible.\\n\\n> In terms of community planning, the V2 API will be the main focus of development, and more features will be supported in V2, while V1 will be stabilized and no longer maintained.\\n\\n## Practice and reflections on our off-line development scheduling platform\\n\\n### Practice and reflections on our off-line development scheduling platform\\n\\n![](/image/16714322747890/16714326227360.jpg)\\nHornet\u2019s Nest Big Data Development Platform, which focuses on providing one-stop big data development and scheduling services, helps businesses solve complex problems such as data development management, task scheduling and task monitoring in offline scenarios.\\n\\nThe offline development and scheduling platform plays the role of the top and the bottom. The top is to provide open interface API and UI to connect with various data application platforms and businesses, and the bottom is to drive various computations and storage, and then run in an orderly manner according to the task dependency and scheduling time.\\n\\n## Platform Capabilities\\n**Data development**\\n\\nTask configuration, quality testing, release live\\n\\n**\xb7Data synchronisation**\\n\\nData access, data processing, data distribution\\n\\n**\xb7Scheduling capabilities**\\n\\nSupports timed scheduling, triggered scheduling\\n\\n**\xb7Operations and Maintenance Centre\\n**\\nJob Diagnosis, Task O&M, Instance O&M\\n\\n**\xb7Management**\\n\\nLibrary table management, permission management, API management, script management\\n\\nIn summary, the core capabilities of the offline development scheduling platform are openness, versatility, and one-stop shopping. Through standardized processes, the entire task development cycle is managed and a one-stop service experience is provided.\\n\\n## The architecture of the platform\\n![](/image/16714322747890/16714326749427.jpg)\\nThe Hornet\u2019s Nest Big Data Development and Scheduling Platform consists of four main modules: the task component layer, the scheduling layer, the service layer, and the monitoring layer.\\n\\nThe service layer is mainly responsible for job lifecycle management (e.g. job creation, testing, release, offline); Airflow dagphthon file building and generating, task bloodline dependency management, permission management, API (providing data readiness, querying of task execution status).\\n\\nThe scheduling layer is based on Airflow and is responsible for the scheduling of all offline tasks.\\n\\nA task component layer that enables users to develop data through supported components that include tools such as SparkSQL/, HiveSQ, LMR), StarRocks import, etc., directly interfacing with underlying HDFS, MySQL, and other storage systems.\\n\\nThe monitoring layer is responsible for all aspects of monitoring and alerting on scheduling resources, computing resources, task execution, etc.\\n\\n## Open Data Sync Capability Scenarios\\nChallenges with open capabilities: Need to support multiple business scenarios and meet flexible data pipeline requirements (i.e. extend to support more task components such as hive2clickhourse, clickhourse2mysql, etc.)\\n\\nExtending task components based on Airflow: higher maintenance costs for extensions, need to reduce costs and increase efficiency (based on the limited provider\'s Airflow offers, less applicable in terms of usage requirements, Airflow is a Python technology stack, while our team is mainly based on the Java technology stack, so the technology stack difference brings higher iteration costs)\\n\\nSelf-developed task components: the high cost of platform integration, long development cycle, high cost of the configuration of task components. (Research or implement task components by yourself, different ways of adapting the parameters of the components in the service layer, no uniform way of parameter configuration)\\n\\nWe wanted to investigate a data integration tool that, firstly, supported a rich set of components, provided out-of-the-box capabilities, was easy to extend, and offered a uniform configuration of parameters and a uniform way of using them to facilitate platform integration and maintenance.\\n\\n* Selection of data integration tools\\n![](/image/16714322747890/16714327002726.jpg)\\nTo address the pain points mentioned above, we actively explored solutions and conducted a selection analysis of several mainstream data integration products in the industry. As you can see from the comparison above, Datax and SeaTunnel both offer good scalability, and high stability, support rich connector plugins, provide scripted, uniformly configurable usage, and have active communities.\\n\\nHowever, Datax is limited by being distributed and is not well suited to massive data scenarios.\\n\\nIn contrast, SeaTunnel offers the ability to provide distributed execution, distributed transactions, scalable levels of data handling, and the ability to provide a unified technical solution in data synchronization scenarios.\\n\\nIn addition to the advantages and features described above and the applicable scenarios, more importantly, the current offline computing resources for big data are unified and managed by yarn, and for the subsequently extended tasks we also wish to execute on Yarn, we finally prefer SeaTunnel for our usage scenarios.\\n\\nFurther performance testing of SeaTunnel and the development of an open data scheduling platform to integrate SeaTunnel may be carried out at a later stage, and its use will be rolled out gradually.\\n\\n## Outbound scenario: Hive data sync to StarRocks\\n\\nTo briefly introduce the background, the Big Data platform has now completed the unification of the OLAP engine layer, using the StarRocks engine to replace the previous Kylin engine as the main query engine in OLAP scenarios.\\n\\nIn the data processing process, after the data is modelled in the data warehouse, the upper model needs to be imported into the OLAP engine for query acceleration, so there are a lot of tasks to push data from Hive to StarRocks every day. task (based on a wrapper for the StarRocks Broker Load import method) to a StarRocks-based table.\\n\\nThe current pain points are twofold.\\n\\n\xb7Long data synchronization links: Hive2StarRocks processing links, which require at least two tasks, are relatively redundant.\\n\\n\xb7Outbound efficiency: From the perspective of outbound efficiency, many Hive models themselves are processed by Spark SQL, and based on the processing the Spark Dataset in memory can be pushed directly to StarRocks without dropping the disk, improving the model\u2019s regional time.\\n\\n![](/image/16714322747890/16714327218590.jpg)\\nStarRocks currently also supports Spark Load, based on the Spark bulk data import method, but our ETL is more complex, needs to support data conversion multi-table Join, data aggregation operations, etc., so temporarily can not meet.\\n\\nWe know from the SeaTunnel community that there are plans to support the StarRocks Sink Connector, and we are working on that part as well, so we will continue to communicate with the community to build it together later.\\n\\n## How to get involved in community building\\n### SeaTunnel Community Contribution\\nAs mentioned earlier, the community has completed the refactoring of the V1 to V2 API and needs to implement more connector plug-ins based on the V2 version of the connector API, which I was lucky enough to contribute to.\\n\\nI am currently responsible for big data infrastructure work, which many mainstream big data components big data also use, so when the community proposed a connector issue, I was also very interested in it.\\n\\nAs the platform is also investigating SeaTunnel, learning and being able to contribute pr to the community is a great way to learn about SeaTunnel.\\n\\nI remember at first I proposed a less difficult pr to implement the WeChat sink connector, but in the process of contributing I encountered many problems, bad coding style, code style did not take into account the rich output format supported by the extension, etc. Although the process was not so smooth, I was really excited and accomplished when the pr was merged. Although the process was not so smooth, it was very exciting and rewarding when the pr was merged.\\n\\nAs I became more familiar with the process, I became much more efficient at submitting pr and was confident enough to attempt difficult issues.\\n### How to get involved in community contributions quickly\\n* Good first issue\\nGood first issue #3018 #2828\\n\\nIf you are a first-time community contributor, it is advisable to focus on the Good first issue first, as it is basically a relatively simple and newcomer-friendly issue.\\n\\nThrough Good first issue, you can get familiar with the whole process of participating in the GitHub open source community contribution, for example, first fork the project, then submit the changes, and finally submit the pull request, waiting for the community to review, the community will target to you to put forward some suggestions for improvement, directly will leave a comment below, until when your pr is merged in, this will have completed a comp\\n\\n* Subscribe to community mailings\\nOnce you\u2019re familiar with the pr contribution process, you can subscribe to community emails to keep up to date with what\u2019s happening in the community, such as what features are currently being worked on and what\u2019s planned for future iterations. If you\u2019re interested in a feature, you can contribute to it in your own situation!\\n* Familiarity with git use\\nThe main git commands used in development are git clone, git pull, git rebase and git merge. git rebase is recommended in the community development specification and does not generate additional commits compared to git merge.\\n* Familiarity with GitHub project collaboration process\\nOpen source projects are developed collaboratively by multiple people, and the collaboration method on GitHub is at its core outlined in fork For example, the apache st project, which is under the apache space, is first forked to our own space on GitHub\\n\\nThen modify the implementation, mention a pull request, and submit the pull request to be associated with the issue, in the commit, if we change a long time, in the upward commit, then the target branch has a lot of new commits exhausted this time we need to do a pull& merge or rebase.\\n\\n* Source code compilation project\\nIt is important to be familiar with source compilation, as local source compilation can prove that the code added to a project can be compiled, and can be used as a preliminary check before committing to pr. Source compilation is generally slow and can be speeded up by using mvn -T for multi-threaded parallel compilation.\\n* Compilation checks\\nPre-compilation checks, including Licence header, Code checkstyle, and Document checkstyle, will be checked during Maven compilation, and if they fail, the CI will not be passed. So it is recommended to use some plug-in tools in the idea to improve the efficiency, such as Code checkstyle has a plug-in to automatically check the code specification, Licence header can add code templates in the idea, these have been shared by the community before how to do!\\n* Add full E2E\\n\\nAdd full E2E testing and ensure that the E2E is passed before the Pull request.\\n\\nFinally, I hope more students will join the SeaTunnel community, where you can not only feel the open-source spirit and culture of Apache but also understand the management process of Apache projects and learn good code design ideas.\\n\\nWe hope that by working together and growing together, we can build SeaTunnel into a top-notch data integration platform."},{"id":"/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls","metadata":{"permalink":"/zh-CN/blog/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-09-20-A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls.md","source":"@site/blog/2022-09-20-A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls.md","title":"A tutorial to help you develop a SeaTunnel Connector hand-by-hand while avoiding pitfalls","description":"SeaTunnel Connector Acess Plan","date":"2022-09-20T00:00:00.000Z","formattedDate":"2022\u5e749\u670820\u65e5","tags":[],"readingTime":12.055,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!","permalink":"/zh-CN/blog/During the joint Apache SeaTunnel & IoTDB Meetup on October 15,"},"nextItem":{"title":"Code Demo for SeaTunnel Connector Development Process","permalink":"/zh-CN/blog/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process"}},"content":"![](https://miro.medium.com/max/1400/0*4fOZaPYhwL2pdUpK)\\n\\n## SeaTunnel Connector Acess Plan\\nDuring the recent live event of the SeaTunnel Connector Access Plan, Beluga open source engineer Wang Hailin shared the \u201cSeaTunnel Connector Access Plan and Development Guide to Avoiding Pit,\u201d and taught everyone how to develop a connector from scratch, including the whole process \u2014 from preparation to testing, and final PR.\\n\\n## Speaker\\n![](https://miro.medium.com/max/1100/0*LRtFiJkgV5DEWOAa)\\n**Wang Hailin**\\n\\nWailin Hailin is an open-source enthusiast, SkyWalking Committer, DolphinScheduler, and SeaTunnel contributor. His current work focuses on performance monitoring, data processing, and more. He likes to study related technical implementations and participate in community exchanges and contributions.\\n\\n## This presentation is divided into 5 parts:\\n1. About the connector access incentive program\\n2. Preparation before claiming/developing connector\\n3. Small things in development\\n4. Considerations for writing E2E Tests\\n5. Preparations to submit a PR\\n\\n## 1. About the Connector Access Incentive Plan\\nFirstly, let me introduce the SeaTunnel Connector Access Incentive Program, and the steps to develop a connector from start to finish (even for novices). This includes the whole process of preparation for development, testing, and final PR.\\n\\nThe SeaTunnel community released a new connector API not long ago, which supports running on various engines, including Flink, Spark, and more. This eliminates the need for repeated development of the old version.\\n\\nAfter the new API is released, the old connector needs to be migrated, or the new connector should be supported.\\n\\nIn order to motivate the community to actively participate in the SeaTunnel Connector Access work and help build SeaTunnel into a more efficient data integration platform, the SeaTunnel community-initiated activities, sponsored by Beluga Open Source.\\n\\nThe activities have three modes: simple, medium, and hard for the task of accessing the connector. The threshold is low.\\n\\nYou can see which tasks need to be claimed on the activity issue list, as well as segmentation based on difficulty and priority. You can choose the activity you are comfortable with. You can start contributing based on the difficulty level.\\n![](https://miro.medium.com/max/1400/0*laqub6yhNDOqPaGc)\\n\\nThe ecological construction of SeaTunnel can become more complete and advanced only with the help of your contributions. You are welcome to participate actively.\\n\\nIn order to express our gratitude, our event has set up a link where points can be exchanged for physical prizes. The more points you get, the more prized you can win!\\n\\nPresently, we\u2019ve seen many small partners participate in the event and submit their connectors. It\u2019s not too late to join as there is still a significant period of time before the event ends. Based on the difficulty of the activity, the deadline may be relaxed or extended.\\n\\n## 2. Preparations Before Claiming/Developing Connectors\\n\\nSo, how do you get involved with this amazing activity?\\n\\nBy first getting to know the basics of a connector.\\n\\n### 01. What is a connector?\\n![](https://miro.medium.com/max/750/0*IjdxVOKUu649s7vQ)\\nA connector is composed of Source and SInk (Source + Sink).\\n\\nIn the above figure, the connectors are connected to various data sources at the upper and lower layers. The source is responsible for reading data from external data sources, while the sink is responsible for writing data to external sources.\\n\\nThere is also an abstraction layer between the source and the sink.\\n\\nThrough this abstraction later, the data types of various data sources can be uniformly converted into the data format of SeaTunnelRow. This allows users to arbitrarily assemble various sources and sinks, so as to realize the integration of heterogeneous data sources, and data synchronization between multiple data sources.\\n\\n### 02. How to claim a connector\\n\\nAfter understanding the basic concepts, the next step is to claim the connector.\\n\\nGitHub link: [https://github.com/apache/incubator-seatunnel/issues/1946](https://github.com/apache/incubator-seatunnel/issues/1946)\\n\\nYou can use the above-mentioned GitHub link to see our plans for connecting to the connector. You can make any additions at any time.\\n\\nFirst, find a connector that has not been claimed. To avoid conflicts, search the entire issue to see if anyone has submitted a PR.\\n\\nAfter claiming the connector, we suggest that you create an issue of the corresponding feature, synchronize the problems you encountered in the development, and discuss the design of your solution.\\n\\nIf you encounter any problems and need help, you can describe them in the issue, and the community can take it up together. Participate in the discussions to help solve the problem. This is also added to the record of the function implementation process, which makes it easy to refer to when maintaining and modifying in the future.\\n\\n### 03. Compile the project\\n\\nAfter claiming the connector, it\u2019s time to prepare the development environment.\\n\\nFirst, fork the SeaTunnel project to the local development environment and compile it.\\n\\nHere\u2019s the compilation reference documentation: [https://github.com/apache/incubator-seatunnel/blob/dev/docs/en/contribution/setup.md](https://github.com/apache/incubator-seatunnel/blob/dev/docs/en/contribution/setup.md)\\n\\nRun the testcase in the documentation after the compilation is successful. You might encounter some issues/problems during the first contact compilation process, such as the following compilation errors:\\n\\n![](https://miro.medium.com/max/1400/0*rGkqQzdfwd6Dp-mR)\\n![](https://miro.medium.com/max/1400/0*r2X63dr2YBTxZGen)\\n\\n**The solution to the above exceptions:**\\n\\n```\\nrm {your_maven_dir}/repository/org/apache/seatunnel\\n./mvnw clean\\nRecompile it\\n```\\n\\n### 04. Understand Connector related code structure\\nThe success of project compilation means that the development environment is ready. Next, let\u2019s take a look at the project code structure and API interface structure of the connector.\\n\\n#### Engineering Code structure\\n\\nAfter the project is compiled, there are three parts related to the connector. The first part is the code implementation and dependency management of the new connector module.\\n\\n* seatunnel-connectors-v2 stores the connector submodule\\n* seatunnel-connectors-v2-dist manages connectors-v2 maven dependencies\\n\\nThe second part is the example. When testing locally, you can build a corresponding case on the example to test the connector.\\n\\n* seatunnel-flink-connector-v2-example example running on Flink\\n* seatunnel-spark-connector-v2-example example running on Spark\\n\\nThe third part is the E2E-testcase: adding targeted test cases on the respective running engines of Spark or Flink, and verifying the functional logic of the connector through automated testing.\\n\\n* seatunnel-flink-connector-v2-e2e testcase running on Flink\\n* seatunnel-spark-connector-v2-e2e testcase running on Spark\\n\\n**Code structure (interfaces, base classes)**\\n\\nThe public interfaces and base classes used in the development are fully described in our readme. For example, API function usage scenarios.\\n\\nHere\u2019s the link: [https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/README.en.md](https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/README.en.md)\\n\\n### 05. See how other people develop connectors\\n\\nAfter going through the above steps, don\u2019t rush to start the work. Instead, first, check out how others do it.\\n\\nWe strongly recommend you check out the connector novice development tutorial shared on the community\'s official account:\\n\\n* [SeaTunnel Connector Minimalist Development Process]\\n* [New API Connector Development Analysis]\\n* [The way of decoupling Apache SeaTunnel (Incubating) and the computing engine \u2014 what we\u2019ve done to reconstruct the API]\\n\\nIn addition, you can refer to the merged Connector code to see the scope of changes, the public interfaces and dependencies used, and the test cases.\\n\\n## 3. Small Issues/Tasks During Development\\n\\nNext, you have to officially enter the connector development process. What problems may be encountered during the development process?\\n\\nThe connector is divided into the source and sink ends \u2014 you can choose either one or both.\\n\\n![](https://miro.medium.com/max/640/0*QxOnCYLu4AcvKw58)\\n\\n### 01. Source-related development\\n\\nThe first thing to pay attention to when developing a source is to determine the reading mode of the source: is it streaming or batch? Is support still required?\\n\\nUse the Source#getBoundedness interface to mark the modes supported by the source.\\n\\nFor example, Kafka naturally supports streaming reading, but it can also support batch mode reading by obtaining lastOffset in the source.\\n\\nAnother question to be aware of: does the source require concurrent reads? If it is single concurrency, after the source is started, a reader will be created to read the data from the data source.\\n\\nIf you want to achieve multi-concurrency, you need to implement an enumerator interface through which data blocks are allocated to readers, and the readers each read their allocated data blocks.\\n\\nFor example, the Kafka source uses partition sharding, and the jdbc source uses fields for range query sharding. It should be noted here that if it is a concurrent reading method, the stability of the data block distribution rules must be ensured.\\n\\nThis is because currently, the connector has a corresponding enumerator on each shard in actual operation, and it is necessary to ensure that the enumerator has data in each shard.\\n\\nThirdly, does the source need to support resumable transfer/state restoration?\\n\\nIf you want to support this, you need to implement:\\n\\n* **Source#restoreEnumerator**: restore state\\n* **Enumerator#snapshotState**: storage shard allocation\\n* **Reader#snapshotState**: stores the read position\\n\\n### 02. Sink-related development\\n\\nIf the sink is a common sink implementation, use Sink#createWriter to write our data according to the concurrency of the source.\\n\\nIf you need to support failure recovery, you need to implement:\\n\\n* **Sink#restoreWriter**: restore state\\n* **Writer#snapshotState**: snapshot state\\n\\nIf you want to support a two-phase commit, you need to implement the following interfaces:\\n\\n* Sink#createCommitter\\n* Writer#prepareCommit: pre-commit\\n* Committer#commit: abort Phase 2 commit\\n\\n![](https://miro.medium.com/max/640/0*GpYdUR2mTlur8jHQ)\\n\\n### 03. Connector related\\n\\nNext, let\u2019s take a look at some of the general problems, especially when the first contribution is made with different styles for each environment, there are often various problems. Therefore, it is recommended that you import tools/checkstyle/checkStyle.xml from the project during development, and use a unified coding format.\\n\\nWhether it is a source or a sink, it will involve defining the data format. The community is pushing for a unified data format definition.\\n\\nTo define Schema, please refer to PR: [https://github.com/apache/incubator-seatunnel/pull/2392](https://github.com/apache/incubator-seatunnel/pull/2392)\\nTo define the Format, please refer to PR: [https://github.com/apache/incubator-seatunnel/pull/2435](https://github.com/apache/incubator-seatunnel/pull/2435)\\n\\nIf you feel that the compilation speed is slow, you can temporarily annotate the old version of the connector-related module in order to speed up both development and debugging.\\n\\n### 04. How to seek help\\n\\nWhen you encounter problems during development and need help, you can:\\n\\n* Describe the problem in your Issue and call active contributors\\n* Discuss on mailing lists and Slack\\n* Communicate through the WeChat group (if you have not joined, please follow the SeaTunnel official account to join the group, and add a small assistant WeChat seatunnel1)\\n* There may be a community docking group for docking third-party components (allowing you to do more with less)\\n\\n## 4. Notes on Writing E2E Tests\\nE2E testing is very important. It is often called the gatekeeper of connector quality.\\n\\nThis is because, if the connector you wrote is not tested, it could be difficult for the community to judge whether there are problems with the implementation of the static code.\\n\\nTherefore, E2E testing is not only functional verification but also a process of checking data logic, which can reduce the pressure on the community to review code and ensure basic functional correctness.\\n\\nIn E2E testing, these are some of the problems that may be encountered:\\n\\n### 01. E2E Failed \u2014 Test Case Network Address Conflict\\n**Because the E2E network deployment structure has the following characteristics:**\\n\\n* External components that Spark, Flink, and e2e-test case depend on (for example, MySQL), use the container networkAliases(host) as the access address\\n* e2e-test case on both sides of Spark and Flink may run in parallel under the same host\\n* External components that e2e-test case depends on, need to map ports to hosts for e2e-test case to access\\n\\n**Therefore, E2E has to pay attention to:**\\n\\n* The external components e2e-test case depends on the ports mapped to the external networkAliases, and so cannot be the same in the test cases on both sides of Spark and Flink\\n* e2e-test case uses localhost, the above-mapped port, to access external components\\n* e2e\u2019s configuration file uses networkAliases(host), the external components that depend on port access in the container\\n\\nHere\u2019s the E2E Testcase reference PR: [https://github.com/apache/incubator-seatunnel/pull/2429](https://github.com/apache/incubator-seatunnel/pull/2429)\\n\\n### 02. E2E failure \u2014 Spark jar package conflict\\nSpark uses the parent first-class loader by default, which may conflict with the package referenced by the connector. For this, the userClassPathFirst classloader can be configured in the Connector environment.\\n\\nHowever, the current packaging structure of SeaTunnel will cause userClassPathFirst to not work properly, so we created an issue, [https://github.com/apache/incubator-seatunnel/pull/2474](https://github.com/apache/incubator-seatunnel/pull/2474), to track this issue. Everyone is welcome to contribute solutions.\\n\\nCurrently, this can only be resolved by replacing conflicting packages in the spark jars directory with the documentation.\\n\\n### 03. E2E failure \u2014 Connector jar package conflict\\n\\nBoth the old and new versions of Connector are dependent on the E2E project and cause conflicts.\\n\\nPR [https://github.com/apache/incubator-seatunnel/pull/2414](https://github.com/apache/incubator-seatunnel/pull/2414) has resolved this issue.\\n\\n\\n**Version conflict between Connector-v2:**\\n* Mainly occurs during E2E, because the E2E project depends on all Connectors\\n* We may plan to provide a separate test project for each Connector (or version) in the future\\n\\n### 04. Insufficient E2E \u2014 Sink Logic Verification\\n\\nThe FakeSource of the Connector-v2 version can only generate random data of a few fixed columns at present, and the community partners are optimizing it to make it better. [https://github.com/apache/incubator-seatunnel/pull/2406\\n](https://github.com/apache/incubator-seatunnel/pull/2406)\\nThat said, we can temporarily solve this problem by simulating the data of the specified content through Transform sql:\\n![](https://miro.medium.com/max/1400/0*_uvD-JWrVbABolAq)\\n\\n### 05. Insufficient E2E \u2014 Source validation data\\nThe Assert Sink can configure column rules, but cannot do row-level value checking. For this problem, you can temporarily use other connector sinks with external storage for query verification data.\\n\\n### 06. E2E stability improvement\\nIn many cases, when E2E starts, you might use Thread.sleep to wait for resource initialization. Here, sleep will cause fewer initialization failures but more time-wasting issues.\\n\\nIn addition, due to the instability of resources, network, and other issues, you might be able to run it now but not later.\\n![](https://miro.medium.com/max/1400/0*iBxwGDaHfXROqtEt)\\n![](https://miro.medium.com/max/1400/0*c2yFYbeVWPvHV7SY)\\nTo avoid this problem, Thread.sleep can be replaced with Awaitility.\\n\\n### 07. A method to speed up E2E\\nAt present, I see that most people run E2E tests separately for both source and sink. If you want to speed up the PR process, it is recommended that you combine both the sink and source into one E2E testcase for verification, and run the testcase only once.\\n\\n## 5. Checks Before Submitting a PR\\nAfter completing the previous steps, please make sure you do some checks before submitting PR \u2014 including the following aspects:\\n\\nComplete recompile project:\\n\\n* Codestyle validation, dependency validation\\n* The successful compilation before does not mean that it can be compiled successfully now\\n\\nRunning E2E locally succeeds:\\n* Both Flink and Spark are verified\\n\\nSupplement or change the document and review it again before submitting:\\n* Review for places not covered by tests\\n* Places that hav been reviewed before and needs to be checked again\\n* Review for including all files, not just code\\n\\nThe above operations and steps can greatly save CI resources, speed up PR Merged, and reduce the costs of community reviews."},{"id":"/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process","metadata":{"permalink":"/zh-CN/blog/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-09-19-Code-Demo-for-SeaTunnel-Connector-Development-Process.md","source":"@site/blog/2022-09-19-Code-Demo-for-SeaTunnel-Connector-Development-Process.md","title":"Code Demo for SeaTunnel Connector Development Process","description":"At the Apache SeaTunnel&Apache Doris Joint Meetup held on July 24, Liu Li \u2014 senior engineer of WhaleOps and contributor to Apache SeaTunnel \u2014 mentioned an easy way to develop a connector in SeaTunnel quickly.","date":"2022-09-19T00:00:00.000Z","formattedDate":"2022\u5e749\u670819\u65e5","tags":[],"readingTime":9.61,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"A tutorial to help you develop a SeaTunnel Connector hand-by-hand while avoiding pitfalls","permalink":"/zh-CN/blog/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls"},"nextItem":{"title":"Talk With Overseas contributors | Why do I contribute to SeaTunnel?","permalink":"/zh-CN/blog/2022/09/14/Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel"}},"content":"At the Apache SeaTunnel&Apache Doris Joint Meetup held on July 24, Liu Li \u2014 senior engineer of WhaleOps and contributor to Apache SeaTunnel \u2014 mentioned an easy way to develop a connector in SeaTunnel quickly.\\n\\n![](https://miro.medium.com/max/700/1*Rbd5BrSuGiZUQA53DXZrBw.png)\\nWe\u2019ll divide it into four key parts:\\n\\n\u25cf The definition of a Connector\\n\\n\u25cf How to access data sources and targets\\n\\n\u25cf Code to demonstrate how to implement a Connector\\n\\n\u25cf Sources and targets that are currently supported\\n\\n## Definition of a Connector\\nThe Connector consists of Source and Sink and is a concrete implementation of accessing data sources.\\n\\nSource: The Source is responsible for reading data from sources such as MySQLSource, DorisSource, HDFSSource, TXTSource, and more.\\n\\nSink: The Sink is responsible for writing read data to the target, including MySQLSink, ClickHouseSink, HudiSink, and more. Data transfer, and more specifically, data synchronization is completed through the cooperation between the Source and Sink.\\n\\n![](https://miro.medium.com/max/298/1*hsfa9Xtzt7o028XjCpqoOg.png)\\n\\nOf course, different sources and sinks can cooperate with each other.\\n\\nFor example, you can use MySQL Source, and Doris Sink to synchronize data from MySQL to Doris, or even read data from MySQL Source and write to HDFS Sink.\\n\\n## How to access data sources and targets\\n\\n### How to access Source\\nFirstly, let\u2019s take a look at how we can access the Source. To elaborate, let\u2019s dive in and check out how we can implement a source and the core interfaces that need to be implemented to access the Source.\\n\\nThe simplest Source is a single concurrent Source. However, if a source does not support state storage and other advanced functions, what interfaces should we implement in these simple single concurrent sources?\\n\\nFirstly, we need to use getBoundedness in the Source to identify whether the Source supports real-time or offline, or both.\\n\\ncreateReader creates a Reader whose main function is to read the specific implementation of data. A single concurrent source is really simple as we only need to implement one method, pollNext, through which the read data is sent.\\n\\nIf concurrent reading is required, what additional interfaces should we implement?\\n![](https://miro.medium.com/max/393/1*bRxRjyMOGkVqseQkg0ONWg.png)\\n\\nFor concurrent reading, we\u2019ll introduce a new member, called the Enumerator.\\n\\nWe implement createEnumerator in Source, and the main function of this member is to create an Enumerator to split the task into segments and then send it to the Reader.\\n\\nFor example, a task can be divided into 4 splits.\\n\\nIf it is concurrent twice, it\u2019ll correspond to two Readers. Two of the four splits will be sent to Reader1, and the other two will be sent to Reader2.\\n\\nIf the number of concurrencies is more \u2014 for example, let\u2019s say there are four concurrences, then you have to create four Readers. You have to use the corresponding four splits for concurrent reading for improved efficiency.\\n\\nA corresponding interface in the Enumerator called the addSplitsBack sends the splits to the corresponding Reader. Through this method, the ID of the Reader can be specified.\\n\\nSimilarly, there is an interface called the addSplits in the Reader to receive the splits sent by the Enumerator for data reading.\\n\\nIn a nutshell, for concurrent reading, we need an Enumerator to implement task splitting and send the splits to the reader. Also, the reader receives the splits and uses them for reading.\\n\\nIn addition, if we need to support resuming and exactly-once semantics, what additional interfaces should we implement?\\n\\nIf the goal is to resume the transfer from a breakpoint, we must save the state and restore it. For this, we need to implement a restoreEnumerator in Source.\\n\\nThe restoreEnumerator method is used to restore an Enumerator through the state and restore the split.\\n\\nCorrespondingly, we need to implement a snapshotState in this enumerator, which is used to save the state of the current Enumerator and perform failure recovery during checkpoints.\\n\\nAt the same time, the Reader will also have a snapshotState method to save the split state of the Reader.\\n\\nIn the event of a failed restart, the Enumerator can be restored through the saved state. After the split is restored, reading can be continued from the place of failure, including fetching and incoming data.\\n\\nThe exact one-time semantics actually requires the source to support data replays, such as Kafka, Pulsar, and others. In addition, the sink must be submitted in two phases, i.e., the precise one-time semantics can be achieved with the cooperation of these two sources and sinks.\\n\\n### How to access Sink\\n\\nNow, let\u2019s take a look at how to connect to the Sink. What interfaces does the Sink need to implement?\\n\\nTruth be told, Sink is relatively simple. For concurrent sinks, when state storage and two-phase commit are not supported, the Sink is simple.\\n\\nTo elaborate, the Sink does not distinguish between stream synchronization and batch synchronization as the Sink \u2014 and the entire SeaTunnel API system \u2014 supports **Unified Stream and Batch Processing.**\\n\\nFirstly, we need to implement createWriter. A Writer is used for data writing.\\n\\nYou need to implement a writer method in Writer through which data is written to the target library.\\n\\n![](https://miro.medium.com/max/414/1*xQ7DRHdBGv-ofjSYdSHAoA.png)\\n\\nAs shown in the figure above, if two concurrencies are set, the engine will call the createWriter method twice in order to generate two Writers. The engine will feed data to these two writers, which will write the data to the target through the write method.\\n\\nFor a more advanced setup, for example, we need to support **two-phase commit and state storage**.\\n\\nHere, what additional interfaces should we implement?\\n\\nFirst, let\u2019s introduce a new member, the Committer, whose main role is for the second-stage commit.\\n\\n![](https://miro.medium.com/max/414/1*cvj1i2A-E-1c_bCZneshtg.png)\\n\\nSince Sink is stored in state, it is necessary to restore Writer through the state. Hence, restoreWriter should be implemented.\\n\\nAlso, since we have introduced a new member, the Committer, we should also implement a createCommitter in the sink. We can then use this method to create a Committer for the second-stage commit or rollback.\\n\\nIn this case, what additional interfaces does Writer need to implement?\\n\\nSince it is a two-phase commit, the first-phase commit is done in the Writer through the implementation of the prepareCommit method \u2014 which is mainly used for the first-phase commit.\\n\\nIn addition, state storage and failure recovery is also supported, meaning we need snapshotState to take snapshots at checkpoints. This saves the state for failure recovery scenarios.\\n\\nThe Committer is the core here. It is mainly used for rollback and commit operations in the second phase.\\n\\nFor the corresponding process, we need to write data to the database. Here, the engine will trigger the first stage commit during the checkpoint, and then the Writer needs to prepare a commit.\\n\\nAt the same time, it will return commitInfo to the engine, and the engine will judge whether the first stage commits of all writers are successful.\\n\\nIf they are indeed successful, the engine will use the commit method to actually commit.\\n\\nFor MySQL, the first-stage commit just saves a transaction ID and sends it to the commit. The engine determines whether the transaction ID is committed or rolled back.\\n\\n## How to implement the Connector\\nWe\u2019ve taken a look at Source and Sink; let\u2019s now look at how to access the data source and implement your own Connector.\\n\\nFirstly, we need to build a development environment for the Connector.\\n\\n### The necessary environment\\n1. Java 1.8\\\\11, Maven, IntelliJ IDEA\\n\\n2. Windows users need to additionally download gitbash (https://gitforwindows.org/)\\n\\n3. Once you have these, you can download the SeaTunnel source code by cloning the git.\\n\\n4. Download SeaTunnel source code 1, git clone https://github.com/apache/incubator-seatunnel.git2, cd incubator-seatunnel\\n\\n### SeaTunnel Engineering Structure\\nWe then open it again through the IDE, and see the directory structure as shown in the figure:\\n\\n![](https://miro.medium.com/max/700/1*utRhNAsYiqQqBFa4Tjewgw.png)\\n\\nThe directory is divided into several parts:\\n\\n1. Connector \u2014 v2\\n\\nSpecific implementation of the new Connector(Connector \u2014 v2) will be placed in this module.\\n\\n2. connector-v2-dist\\n\\nThe translation layer of the new connector translates into specific engine implementation \u2014 instead of implementing under corresponding engines such as Spark, Flink, and ST-Engine. ST-Engine is the \u201cimportant, big project\u201d the community is striving to implement. This project is worth the wait.\\n\\n3. examples\\n\\nThis package provides a single-machine local operation method, which is convenient for debugging while implementing the Connector.\\n\\n4. e2e\\n\\nThe e2e package is for e2e testing of the Connector.\\n\\nNext, let\u2019s check out how a Connector can be created (based on the new Connector). Here is the step-by-step process:\\n\\n1. Create a new module in the seatunnel-connectors-v2 directory and name it this way: connector-{connector name}.\\n\\n2. The pom file can refer to the pom file of the existing connector and add the current child model to the parent model\u2019s pom file.\\n\\n3. Create two new packages corresponding to the packages of Source and Sink, respectively:\\n\\na. org.apache.seatunnel.connectors.seatunnel.{connector name}.source\\n\\nb. org.apache.seatunnel.connectors.seatunnel.{connector name}.sink\\n\\nTake this mysocket example shown in the figure:\\n\\n![](https://miro.medium.com/max/700/1*K1btD2gNwYxj96OJnPfW2Q.png)\\n\\nTo do some implementation, develop the connector. During implementation, you can use the example module for local debugging if you need to debug. That said, this module mainly provides the local running environment of Flink and Spark.\\n\\n![](https://miro.medium.com/max/700/1*qOc3q7okzo7jObHxloc7WQ.png)\\n\\nAs you can see in the image, there are numerous examples under the \u201cExample\u201d module \u2014 including seatunnel-flink-connector-v2-example.\\n\\nSo how do you use them?\\n\\nLet\u2019s take an example. The debugging steps on Flink are as follows (these actions are under the seatunnel-flink-connector-v2-example module:\\n\\n1. Add connector dependencies in pom.xml\\n\\n2. Add the task configuration file under resources/examples\\n\\n3. Configure the file in the SeaTunnelApiExample main method\\n\\n4. Run the main method\\n\\n### Code Demo\\n\\nThis code demonstration is based on DingTalk.\\n\\nHere\u2019s a reference\uff08 19:35s\u201337:10s\uff09:\\n\\nhttps://weixin.qq.com/sph/A1ri7B\\n\\n![](https://miro.medium.com/max/700/1*ej9ronizPtC09ILWJDlbUg.png)\\n\\n### New Connectors supported at this stage\\n\\nAs of July 14, contributions and statistics for the completed connectors are welcome. You are more than welcome to try them out, and raise issues in our community if you find bugs.\\n\\n![](https://miro.medium.com/max/700/1*RHNJDcbvKmSt2UGGSz3Icg.png)\\n\\nThe Connector shared below have already been claimed and developed:\\n\\n![](https://miro.medium.com/max/700/1*RHNJDcbvKmSt2UGGSz3Icg.png)\\n\\nAlso, we have Connectors in the roadmap \u2014 the connectors we want to support in the near future. To foster the process, the SeaTunnel Community initiated SeaTunnel Connector Access Incentive Plan, you are more than welcome to contribute to the project.\\n\\nSeaTunnel Connector Access Incentive Plan: https://github.com/apache/incubator-seatunnel/issues/1946\\n\\nYou can claim tasks that haven\u2019t been marked in the comment area, and take a spree home! Here is part of the connectors that need to be accessed as soon as possible:\\n![](https://miro.medium.com/max/414/1*n-ixPtq066Acx4Ja5qNQqw.png)\\nIn fact, the implementations of Connectors like Feishu, DingTalk, and Facebook messenger are quite simple as the connectors do not need to carry a large amount of data (just a simple Source and Sink). This is in sharp contrast to Hive and other databases that need to consider transaction consistency or concurrency issues.\\n\\nWe welcome everyone to make contributions and join our Apache SeaTunnel family!\\n\\n## About SeaTunnel\\nSeaTunnel (formerly Waterdrop) is an easy-to-use, ultra-high-performance distributed data integration platform that supports the real-time synchronization of massive amounts of data and can synchronize hundreds of billions of data per day stably and efficiently.\\n\\n### Why do we need SeaTunnel?\\n\\nSeaTunnel does everything it can to solve the problems you may encounter in synchronizing massive amounts of data.\\n\\n* Data loss and duplication\\n* Task buildup and latency\\n* Low throughput\\n* Long application-to-production cycle time\\n* Lack of application status monitoring\\n\\n### SeaTunnel Usage Scenarios\\n* Massive data synchronization\\n* Massive data integration\\n* ETL of large volumes of data\\n* Massive data aggregation\\n* Multi-source data processing\\n\\n### Features of SeaTunnel\\n\\n* Rich components\\n* High scalability\\n* Easy to use\\n* Mature and stable"},{"id":"/2022/09/14/Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel","metadata":{"permalink":"/zh-CN/blog/2022/09/14/Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-09-14-Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel.md","source":"@site/blog/2022-09-14-Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel.md","title":"Talk With Overseas contributors | Why do I contribute to SeaTunnel?","description":"As SeaTunnel gets popular around the world, it is attracting more and more contributors from overseas to join the open-source career. Among them, a big data platform engineer at Kakao enterprise corp., Namgung Chan has recently contributed the Neo4j Sink Connector for the SeaTunnel. We have a talk with him to know why SeaTunnel is attractive to him, and how he thinks SeaTunnel should gain popularity in the South Korean market.","date":"2022-09-14T00:00:00.000Z","formattedDate":"2022\u5e749\u670814\u65e5","tags":[],"readingTime":1.98,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Code Demo for SeaTunnel Connector Development Process","permalink":"/zh-CN/blog/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process"},"nextItem":{"title":"SeaTunnel 2.1.3 released! Introducing in Assert Sink connector and NullRate, Nulltf Transform","permalink":"/zh-CN/blog/2022/09/12/SeaTunnel-2.1.3-released"}},"content":"As SeaTunnel gets popular around the world, it is attracting more and more contributors from overseas to join the open-source career. Among them, a big data platform engineer at Kakao enterprise corp., Namgung Chan has recently contributed the Neo4j Sink Connector for the SeaTunnel. We have a talk with him to know why SeaTunnel is attractive to him, and how he thinks SeaTunnel should gain popularity in the South Korean market.\\n\\n## Personal Profile\\n![](https://miro.medium.com/max/1400/1*sKzXjqu6M_VmoperNBYUGQ.jpeg)\\n\\nNamgung Chan, South Korea, Big Data Platform Engineer at Kakao enterprise corp.\\n\\nBlog (written in Korean): https://getchan.github.io/\\nGitHub ID: https://github.com/getChan\\nLinkedIn : https://www.linkedin.com/in/namgung-chan-6a06441b6/\\n\\n### Contributions to the community\\nHe writes the Neo4j Sink Connector code for the new SeaTunnel Connector API.\\n\\n### How to know SeaTunnel for the first time?\\nIt\u2019s the first time Namgung Chan to engage in open source. He wants to learn technical skills by contributing, at the same time experience the open-source culture.\\n\\nFor him, an open source project which is written by java lang, and made for data engineering, has many issues of \u2018help wanted\u2019 or \u2018good first issue\u2019 is quite suitable. Then he found SeaTunnel on the Apache Software Foundation project webpage.\\n\\n### The first impression of SeaTunnel Community\\nThough it was his first open source experience, he felt it was comfortable and interesting to go to the community. He also felt very welcome, because there are many \u2018good first issue, and \u2018volunteer wanted\u2019 tagged issues and will get a quick response of code review.\\n\\nWith gaining knowledge of Neo4j, he grows much more confident in open source contribution.\\n\\n### Research and comparison\\nBefore knowing about SeaTunnel, Namgung Chan used Spring Cloud Data Flow for data integration. While after experiencing SeaTunnel, he thinks the latter is more lightweight than SCDF, because in SCDF, every source, processor, and sink component are individual applications, but SeaTunnel is not.\\n\\nThough hasn\u2019t used SeaTunnel in his working environment yet, Namgung Chan said he would like to use it positively when he is in need, especially for data integration for various data storage.\\n\\n\\n### Expectations for SeaTunnel\\nThe most exciting new features or optimizations for Namgung Chan are:\\n\\nData Integration for various data storage.\\nStrict data validation. monitoring extension\\nLow computing resource\\nexactly-once data processing\\nIn the future, Namgung Chan plans to keep contributing from light issues to heavy ones, and we hope he will have a good time here!"},{"id":"/2022/09/12/SeaTunnel-2.1.3-released","metadata":{"permalink":"/zh-CN/blog/2022/09/12/SeaTunnel-2.1.3-released","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-09-12-SeaTunnel-2.1.3-released.md","source":"@site/blog/2022-09-12-SeaTunnel-2.1.3-released.md","title":"SeaTunnel 2.1.3 released! Introducing in Assert Sink connector and NullRate, Nulltf Transform","description":"More than a month after the release of Apache SeaTunnel(Incubating) 2.1.2, we have been collecting user and developer feedback to bring you version 2.1.3. The new version introduces the Assert Sink connector, which is an inurgent need in the community, and two Transforms, NullRate and Nulltf. Some usability problems in the previous version have also been fixed, improving stability and efficiency.","date":"2022-09-12T00:00:00.000Z","formattedDate":"2022\u5e749\u670812\u65e5","tags":[],"readingTime":2.885,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Talk With Overseas contributors | Why do I contribute to SeaTunnel?","permalink":"/zh-CN/blog/2022/09/14/Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel"},"nextItem":{"title":"Upcoming API Connector Development Analysis","permalink":"/zh-CN/blog/Upcoming API Connector Development Analysis"}},"content":"![](https://miro.medium.com/max/1400/1*7jtTFNpvwC6nquA-BLfqGg.png)\\n\\nMore than a month after the release of Apache SeaTunnel(Incubating) 2.1.2, we have been collecting user and developer feedback to bring you version 2.1.3. The new version introduces the Assert Sink connector, which is an inurgent need in the community, and two Transforms, NullRate and Nulltf. Some usability problems in the previous version have also been fixed, improving stability and efficiency.\\n\\nThis article will introduce the details of the update of Apache SeaTunnel(Incubating) **version 2.1.3**.\\n\\n* Release Note: [https://github.com/apache/incubator-seatunnel/blob/2.1.3/release-note.md](https://github.com/apache/incubator-seatunnel/blob/2.1.3/release-note.md)\\n* Download address: [https://seatunnel.apache.org/download](https://seatunnel.apache.org/download)\\n\\n## Major feature updates\\n### Introduces Assert Sink connector\\nAssert Sink connector is introduced in SeaTunnel version 2.1.3to verify data correctness. Special thanks to Lhyundeadsoul for his contribution.\\n\\n### Add two Transforms\\nIn addition, the 2.1.3 version also adds two Transforms, NullRate and Nulltf, which are used to detect data quality and convert null values \u200b\u200bin the data to generate default values. These two Transforms can effectively improve the availability of data and reduce the frequency of abnormal situations. Special thanks to wsyhj and Interest1-wyt for their contributions.\\n\\nAt present, SeaTunnel has supported 9 types of Transforms including Common Options, Json, NullRate, Nulltf, Replace, Split, SQL, UDF, and UUID, and the community is welcome to contribute more Transform types.\\n\\nFor details of Transform, please refer to the official documentation: [https://seatunnel.apache.org/docs/2.1.3/category/transform](https://seatunnel.apache.org/docs/2.1.3/category/transform)\\n\\n### ClickhouseFile connector supports Rsync data transfer method now\\nAt the same time, SeaTunnel 2.1.3 version brings Rsync data transfer mode support to ClickhouseFile connector, users can now choose SCP and Rsync data transfer modes. Thanks to Emor-nj for contributing to this feature.\\n\\n### Specific feature updates:\\n\\n* Flink Fake data supports BigInteger type https://github.com/apache/incubator-seatunnel/pull/2118\\n* Add Flink Assert Sink connector https://github.com/apache/incubator-seatunnel/pull/2022\\n* Spark ClickhouseFile connector supports Rsync data file transfer method https://github.com/apache/incubator-seatunnel/pull/2074\\n* Add Flink Assert Sink e2e module https://github.com/apache/incubator-seatunnel/pull/2036\\n* Add NullRate Transform for detecting data quality https://github.com/apache/incubator-seatunnel/pull/1978\\n* Add Nulltf Transform for setting defaults https://github.com/apache/incubator-seatunnel/pull/1958\\n### Optimization\\n* Refactored Spark TiDB-related parameter information\\n* Refactor the code to remove redundant code warning information\\n* Optimize connector jar package loading logic\\n* Add Plugin Discovery module\\n* Add documentation for some modules\\n* Upgrade common-collection from version 4 to 4.4\\n* Upgrade common-codec version to 1.13\\n### Bug Fix\\nIn addition, in response to the feedback from users of version 2.1.2, we also fixed some usability issues, such as the inability to use the same components of Source and Sink, and further improved the stability.\\n\\n* Fixed the problem of Hudi Source loading twice\\n* Fix the problem that the field TwoPhaseCommit is not recognized after Doris 0.15\\n* Fixed abnormal data output when accessing Hive using Spark JDBC\\n* Fix JDBC data loss when partition_column (partition mode) is set\\n* Fix KafkaTableStream schema JSON parsing error\\n* Fix Shell script getting APP_DIR path error\\n* Updated Flink RunMode enumeration to get correct help messages for run modes\\n* Fix the same source and sink registered connector cache error\\n* Fix command line parameter -t( \u2014 check) conflict with Flink deployment target parameter\\n* Fix Jackson type conversion error problem\\n* Fix the problem of failure to run scripts in paths other than SeaTunnel_Home\\n### Acknowledgment\\nThanks to all the contributors (GitHub ID, in no particular order,), it is your efforts that fuel the launch of this version, and we look forward to more contributions to the Apache SeaTunnel(Incubating) community!\\n\\n`leo65535, CalvinKirs, mans2singh, ashulin, wanghuan2054, lhyundeadsoul, tobezhou33, Hisoka-X, ic4y, wsyhj, Emor-nj, gleiyu, smallhibiscus, Bingz2, kezhenxu94, youyangkou, immustard, Interest1-wyt, superzhang0929, gaaraG, runwenjun`"},{"id":"Upcoming API Connector Development Analysis","metadata":{"permalink":"/zh-CN/blog/Upcoming API Connector Development Analysis","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-06-23-New_Connector.md","source":"@site/blog/2022-06-23-New_Connector.md","title":"Upcoming API Connector Development Analysis","description":"After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API.","date":"2022-06-23T00:00:00.000Z","formattedDate":"2022\u5e746\u670823\u65e5","tags":[{"label":"Meetup","permalink":"/zh-CN/blog/tags/meetup"}],"readingTime":3.28,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"Upcoming API Connector Development Analysis","title":"Upcoming API Connector Development Analysis","tags":["Meetup"]},"prevItem":{"title":"SeaTunnel 2.1.3 released! Introducing in Assert Sink connector and NullRate, Nulltf Transform","permalink":"/zh-CN/blog/2022/09/12/SeaTunnel-2.1.3-released"},"nextItem":{"title":"Apache SeaTunnel \u4e0e\u8ba1\u7b97\u5f15\u64ce\u7684\u89e3\u8026\u4e4b\u9053\uff0c\u91cd\u6784API\u6211\u4eec\u505a\u4e86\u4e9b\u4ec0\u4e48","permalink":"/zh-CN/blog/2022/05/31/engine"}},"content":"After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API.\\n\\n## Priliminary Setup\\n- Environment configuration: JDK8 and Scala2.11 are recommended.\\n- As before, we need to download the latest code locally through git and import it into the IDE, project address: https://github.com/apache/incubator-seatunnel . At the same time, switch the branch to api-draft, and currently use this branch to develop the new version of the API and the corresponding Connector. The project structure is as follows:\\n\\n  ![Project Structure](/image/20220623/en/0.png)\\n\\n## Prerequisites\\n- At present, in order to distinguish different Connectors, we put the connectors that support\\n    - Flink/Spark under the ``seatunnel-connectors/seatunnel-connectors-flink(spark)`` module.\\n    - New version of the Connector is placed under the ``seatunnel-connectors/seatunnel-connectors-seatunnel`` module.\\n\\n  As we can see from the above figure, we have implemented Fake, Console, Kafka Connector, and Clickhouse Connector is also being implemented.\\n- At present, the data type we support is SeaTunnelRow, so no matter the type of data generated by the Source or the type of data consumed by the Sink, it should be SeaTunnelRow.\\n\\n# Development of Connector\\nTaking Fake Connector as an example, let\'s introduce how to implement a new Connector:\\n\\n- Create a corresponding module with a path under ``seatunnel-connectors-seatunnel``, which is at the same level as other new connectors.\\n- Modify the ``seatunnel-connectors-seatunnel/pom.xml`` file, add a new module to modules, modify ``seatunnel-connectors-seatunnel/seatunnel-connector-seatunnel-fake/pom.xml``, add seatunnel-api dependencies, and correct parent Quote. The resulting style is as follows:\\n\\n  ![Style](/image/20220623/en/1.png)\\n\\n- The next step is to create the corresponding package and related classes, create FakeSource, and need to inherit SeaTunnel Source.\\n\\n    - Note : The Source of SeaTunnel adopts the design of stream and batch integration. The Source of SeaTunnel determines whether current Source is a stream or batch through attribute getBoundedness.\\n\\n  So you can specify a Source as a stream or batch by dynamic configuration (refer to the default method). The configuration defined by the user in the configuration file can be obtained through the prepare method to realize the customized configuration.\\n\\n  Then create FakeSourceReader, FakeSource SplitEnumerator, and FakeSourceSplit to inherit the corresponding abstract classes (which can be found in the corresponding classes). As long as we implement the corresponding methods of these classes, then our SeaTunnel Source Connector is basically completed.\\n- Next, just follow the existing example to write the corresponding code. The most important one is the FakeSource Reader, which defines how we obtain data from the outside, which is the most critical part of the Source Connector. Every time a piece of data is generated, we need to place it in the collector as shown:\\n\\n  ![Source](/image/20220623/en/2.png)\\n- After the code development is complete, we need to configure the configuration file ``plugin-mapping.properties`` located under ``seatunnel-connectors/modules``. Adding a seatunnel\\n  ``.source.FakeSource = seatunnel-connector-fake``\\n  means that SeaTunnel can find the jar package corresponding to the project by looking for a Source named FakeSource. This allows the Connector to be used in the normal configuration file.\\n- For a detailed description of writing Source and Sink and SeaTunnel API, please refer to the introduction at ``seatunnel-connectors/seatunnel-connectors-seatunnel/ README.zh.md``.\\n\\n## Connector Testing\\n- For testing, we can find the ``seatunnel-flink(spark)-new-connector-example`` module in seatunnel-examples, and test it against different engines to ensure that the performance of the Connector is as consistent as possible. If you find any discrepancies, you can mark them in the document, modify the configuration file under resource, add our Connector to the configuration, and introduce ``seatunnel-flink(spark)-new-connector-example/pom.xml`` dependency, you can execute ``SeaTunnelApiExample`` to test.\\n- The default is stream processing mode, and the execution mode is switched to batch mode by modifying ``job.mode=BATCH`` in the environment of the configuration file.\\n\\n## Submit PR\\nWhen our Connector is ready, we can submit PR to github. After reviewing by other partners, our contributed Connector will become part of SeaTunnel!"},{"id":"/2022/05/31/engine","metadata":{"permalink":"/zh-CN/blog/2022/05/31/engine","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-05-31-engine.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2022-05-31-engine.md","title":"Apache SeaTunnel \u4e0e\u8ba1\u7b97\u5f15\u64ce\u7684\u89e3\u8026\u4e4b\u9053\uff0c\u91cd\u6784API\u6211\u4eec\u505a\u4e86\u4e9b\u4ec0\u4e48","description":"Apache SeaTunnel (Incubating) \u4e0e Apache Inlong (Incubating) \u76845\u6708\u8054\u5408Meetup\u4e2d\uff0c\u7b2c\u4e8c\u4f4d\u5206\u4eab\u7684\u5609\u5bbe\u662f\u6765\u81ea\u767d\u9cb8\u5f00\u6e90\u7684\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u674e\u5b97\u6587\u3002\u5728\u4f7f\u7528Apache SeaTunnel (Incubating) \u7684\u8fc7\u7a0b\u4e2d\uff0c\u4ed6\u53d1\u73b0\u4e86 Apache SeaTunnel (Incubating) \u5b58\u5728\u7684\u56db\u5927\u95ee\u9898\uff1aConnector\u5b9e\u73b0\u6b21\u6570\u591a\u3001\u53c2\u6570\u4e0d\u7edf\u4e00\u3001\u96be\u4ee5\u652f\u6301\u591a\u4e2a\u7248\u672c\u7684\u5f15\u64ce\u4ee5\u53ca\u5f15\u64ce\u5347\u7ea7\u96be\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u4ee5\u4e0a\u7684\u96be\u9898\uff0c\u674e\u5b97\u6587\u5c06\u76ee\u6807\u653e\u5728\u5c06Apache SeaTunnel (Incubating)\u4e0e\u8ba1\u7b97\u5f15\u64ce\u8fdb\u884c\u89e3\u8026\uff0c\u91cd\u6784\u5176\u4e2dSource\u4e0eSink API\uff0c\u5b9e\u73b0\u6539\u826f\u4e86\u5f00\u53d1\u4f53\u9a8c\u3002","date":"2022-05-31T00:00:00.000Z","formattedDate":"2022\u5e745\u670831\u65e5","tags":[],"readingTime":15.505,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Upcoming API Connector Development Analysis","permalink":"/zh-CN/blog/Upcoming API Connector Development Analysis"},"nextItem":{"title":"\u767e\u4ebf\u7ea7\u6570\u636e\u540c\u6b65\uff0c\u5982\u4f55\u57fa\u4e8e SeaTunnel \u7684 ClickHouse \u5b9e\u73b0\uff1f","permalink":"/zh-CN/blog/2022/05/10/ClickHouse"}},"content":"![](/image/20220531/ch/0.jpg)\\n\\n\\n\\nApache SeaTunnel (Incubating) \u4e0e Apache Inlong (Incubating) \u76845\u6708\u8054\u5408Meetup\u4e2d\uff0c\u7b2c\u4e8c\u4f4d\u5206\u4eab\u7684\u5609\u5bbe\u662f\u6765\u81ea\u767d\u9cb8\u5f00\u6e90\u7684\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u674e\u5b97\u6587\u3002\u5728\u4f7f\u7528Apache SeaTunnel (Incubating) \u7684\u8fc7\u7a0b\u4e2d\uff0c\u4ed6\u53d1\u73b0\u4e86 Apache SeaTunnel (Incubating) \u5b58\u5728\u7684\u56db\u5927\u95ee\u9898\uff1aConnector\u5b9e\u73b0\u6b21\u6570\u591a\u3001\u53c2\u6570\u4e0d\u7edf\u4e00\u3001\u96be\u4ee5\u652f\u6301\u591a\u4e2a\u7248\u672c\u7684\u5f15\u64ce\u4ee5\u53ca\u5f15\u64ce\u5347\u7ea7\u96be\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u4ee5\u4e0a\u7684\u96be\u9898\uff0c\u674e\u5b97\u6587\u5c06\u76ee\u6807\u653e\u5728\u5c06Apache SeaTunnel (Incubating)\u4e0e\u8ba1\u7b97\u5f15\u64ce\u8fdb\u884c\u89e3\u8026\uff0c\u91cd\u6784\u5176\u4e2dSource\u4e0eSink API\uff0c\u5b9e\u73b0\u6539\u826f\u4e86\u5f00\u53d1\u4f53\u9a8c\u3002\\n\\n\u672c\u6b21\u6f14\u8bb2\u4e3b\u8981\u5305\u542b\u56db\u4e2a\u90e8\u5206\uff1a\\n\\n1.  Apache SeaTunnel (Incubating)**\u91cd\u6784\u7684\u80cc\u666f\u548c\u52a8\u673a**\\n    \\n2.  Apache SeaTunnel (Incubating)**\u91cd\u6784\u7684\u76ee\u6807**\\n    \\n3.  Apache SeaTunnel (Incubating)**\u91cd\u6784\u6574\u4f53\u7684\u8bbe\u8ba1**\\n    \\n4.  Apache SeaTunnel (Incubating) **Source API\u7684\u8bbe\u8ba1**\\n    \\n5.  Apache SeaTunnel (Incubating) **Sink API\u7684\u8bbe\u8ba1**\\n    \\n\\n\\n![](/image/20220531/ch/1.jpg)\\n\\n\\n\\n**\u674e\u5b97\u6587**\\n\\n\u767d\u9cb8\u5f00\u6e90 \u9ad8\u7ea7\u5de5\u7a0b\u5e08\\n\\nApache SeaTunnel(Incubating)\\n\\n& Flink Contributor, Flink CDC & Debezium Contributor\\n\\n## **01** \u91cd\u6784\u7684\u80cc\u666f\u4e0e\u52a8\u673a\\n\\n### 01 Apache SeaTunnel(Incubating)\u4e0e\u5f15\u64ce\u8026\u5408\\n\\n\u7528\u8fc7Apache SeaTunnel (Incubating) \u7684\u5c0f\u4f19\u4f34\u6216\u8005\u5f00\u53d1\u8005\u5e94\u8be5\u77e5\u9053\uff0c\u76ee\u524dApache SeaTunnel (Incubating) \u4e0e\u5f15\u64ce\u5b8c\u5168\u8026\u5408\uff0c\u5b8c\u5168\u57fa\u4e8eSpark\u3001Flink\u5f00\u53d1\uff0c\u5176\u4e2d\u7684\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u90fd\u57fa\u4e8eFlink\u3001Spark\u5f15\u64ce\u3002\u4ece\u8d21\u732e\u8005\u548c\u7528\u6237\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u6211\u4eec\u80fd\u53d1\u73b0\u4e00\u4e9b\u95ee\u9898\u3002\\n\\n**\u4ece\u8d21\u732e\u8005\u7684\u89d2\u5ea6**\uff1a\u53cd\u590d\u5b9e\u73b0Connector\uff0c\u6ca1\u6709\u6536\u83b7\u611f\uff1b\u6f5c\u5728\u8d21\u732e\u8005\u7531\u4e8e\u5f15\u64ce\u7248\u672c\u4e0d\u4e00\u81f4\u65e0\u6cd5\u8d21\u732e\u793e\u533a\uff1b\\n\\n**\u4ece\u7528\u6237\u7684\u89d2\u5ea6**\uff1a\u76ee\u524d\u5f88\u591a\u516c\u53f8\u91c7\u7528Lambda\u67b6\u6784\uff0c\u79bb\u7ebf\u4f5c\u4e1a\u4f7f\u7528Spark\uff0c\u5b9e\u65f6\u4f5c\u4e1a\u4f7f\u7528Flink\uff0c \u4f7f\u7528\u4e2d\u5c31\u4f1a\u53d1\u73b0SeaTunnel \u7684Connector\u53ef\u80fdSpark\u6709\uff0c\u4f46\u662fFlink\u6ca1\u6709\uff0c\u4ee5\u53ca\u4e24\u4e2a\u5f15\u64ce\u5bf9\u4e8e\u540c\u4e00\u5b58\u50a8\u5f15\u64ce\u7684Connector\u7684\u53c2\u6570\u4e5f\u4e0d\u7edf\u4e00\uff0c\u6709\u8f83\u9ad8\u7684\u4f7f\u7528\u6210\u672c\uff0c\u8131\u79bb\u4e86SeaTunnel\u7b80\u5355\u6613\u7528\u7684\u521d\u8877\uff1b\u8fd8\u6709\u7528\u6237\u63d0\u95ee\u8bf4\u76ee\u524d\u652f\u4e0d\u652f\u6301Flink\u76841.14\u7248\u672c\uff0c\u6309\u7167\u76ee\u524dSeaTunnel\u7684\u67b6\u6784\uff0c\u60f3\u8981\u652f\u6301Flink\u76841.14\u5c31\u5fc5\u987b\u629b\u5f03\u4e4b\u524d\u7684\u7248\u672c\uff0c\u56e0\u6b64\u8fd9\u4e5f\u4f1a\u5bf9\u4e4b\u524d\u7248\u672c\u7684\u7528\u6237\u9020\u6210\u5f88\u5927\u7684\u95ee\u9898\u3002\\n\\n\u56e0\u6b64\uff0c\u6211\u4eec\u4e0d\u7ba1\u662f\u505a\u5f15\u64ce\u5347\u7ea7\u6216\u8005\u652f\u6301\u66f4\u591a\u7684\u7248\u672c\u7684\u7528\u6237\u90fd\u5f88\u56f0\u96be\u3002\\n\\n\u53e6\u5916Spark\u548cFlink\u90fd\u91c7\u7528\u4e86Chandy-lamport\u7b97\u6cd5\u5b9e\u73b0\u7684Checkpoint\u5bb9\u9519\u673a\u5236\uff0c\u4e5f\u5728\u5185\u90e8\u8fdb\u884c\u4e86DataSet\u4e0eDataStream\u7684\u7edf\u4e00\uff0c\u4ee5\u6b64\u4e3a\u524d\u63d0\u6211\u4eec\u8ba4\u4e3a\u89e3\u8026\u662f\u53ef\u884c\u7684\u3002\\n\\n## **02** Apache SeaTunnel(Incubating)\u4e0e\u5f15\u64ce\u89e3\u8026\\n\\n\u56e0\u6b64\u4e3a\u4e86\u89e3\u51b3\u4ee5\u4e0a\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u6211\u4eec\u6709\u4e86\u4ee5\u4e0b\u7684\u76ee\u6807\uff1a\\n\\n1.  **Connector\u53ea\u5b9e\u73b0\u4e00\u6b21**\uff1a\u9488\u5bf9\u53c2\u6570\u4e0d\u7edf\u4e00\u3001Connector\u591a\u6b21\u5b9e\u73b0\u7684\u95ee\u9898\uff0c\u6211\u4eec\u5e0c\u671b\u5b9e\u73b0\u4e00\u4e2a\u7edf\u4e00\u7684Source \u4e0eSink API;\\n    \\n2.  **\u652f\u6301\u591a\u4e2a\u7248\u672c\u7684Spark\u4e0eFlink\u5f15\u64ce**\uff1a\u5728Source\u4e0eSink API\u4e0a\u518d\u52a0\u5165\u7ffb\u8bd1\u5c42\u53bb\u652f\u6301\u591a\u4e2a\u7248\u672c\u4e0eSpark\u548cFlink\u5f15\u64ce\uff0c\u89e3\u8026\u540e\u8fd9\u4e2a\u4ee3\u4ef7\u4f1a\u5c0f\u5f88\u591a\u3002\\n    \\n3.  **\u660e\u786eSource\u7684\u5206\u7247\u5e76\u884c\u903b\u8f91\u548cSink\u7684\u63d0\u4ea4\u903b\u8f91**\uff1a\u6211\u4eec\u5fc5\u987b\u63d0\u4f9b\u4e00\u4e2a\u826f\u597d\u7684API\u53bb\u652f\u6301Connector\u5f00\u53d1\uff1b\\n    \\n4.  **\u652f\u6301\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u6570\u636e\u5e93\u6574\u5e93\u540c\u6b65**\uff1a\u8fd9\u4e2a\u662f\u76ee\u524d\u5f88\u591a\u7528\u6237\u63d0\u5230**\u9700\u8981CDC**\u652f\u6301\u884d\u751f\u7684\u9700\u6c42\u3002\u6211\u4e4b\u524d\u53c2\u4e0e\u8fc7Flink CDC\u793e\u533a\uff0c\u5f53\u65f6\u6709\u8bb8\u591a\u7528\u6237\u63d0\u51fa\u5728CDC\u7684\u573a\u666f\u4e2d\uff0c\u5982\u679c\u76f4\u63a5\u4f7f\u7528Flink CDC\u7684\u8bdd\u4f1a\u5bfc\u81f4\u6bcf\u4e00\u4e2a\u8868\u90fd\u6301\u6709\u4e00\u4e2a\u94fe\u63a5\uff0c\u5f53\u9047\u5230\u9700\u8981\u6574\u5e93\u540c\u6b65\u9700\u6c42\u65f6\uff0c\u5343\u5f20\u8868\u5c31\u6709\u5343\u4e2a\u94fe\u63a5\uff0c\u8be5\u60c5\u51b5\u65e0\u8bba\u662f\u5bf9\u4e8e\u6570\u636e\u5e93\u8fd8\u662fDBA\u90fd\u662f\u4e0d\u80fd\u63a5\u53d7\u7684\uff0c\u5982\u679c\u8981\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5c31\u662f\u5f15\u5165Canal\u3001Debezium\u7b49\u7ec4\u4ef6\uff0c\u4f7f\u7528\u5176\u62c9\u53d6\u589e\u91cf\u6570\u636e\u5230Kafka\u7b49MQ\u505a\u4e2d\u95f4\u5b58\u50a8\uff0c\u518d\u4f7f\u7528Flink SQL\u8fdb\u884c\u540c\u6b65\uff0c\u8fd9\u5b9e\u9645\u5df2\u7ecf\u8fdd\u80cc\u4e86Flink CDC\u6700\u65e9\u51cf\u5c11\u94fe\u8def\u7684\u60f3\u6cd5\uff0c\u4f46\u662fFlink CDC\u7684\u5b9a\u4f4d\u53ea\u662f\u4e00\u4e2aConnector\uff0c\u65e0\u6cd5\u505a\u5168\u94fe\u8def\u7684\u9700\u6c42\uff0c\u6240\u4ee5\u8be5proposal\u5728Flink CDC\u793e\u533a\u4e2d\u6ca1\u6709\u88ab\u63d0\u51fa\uff0c\u6211\u4eec\u501f\u7740\u672c\u6b21\u91cd\u6784\uff0c\u5c06proposa\u63d0\u4ea4\u5230\u4e86SeaTunnel\u793e\u533a\u4e2d\u3002\\n    \\n5.  **\u652f\u6301\u5143\u4fe1\u606f\u7684\u81ea\u52a8\u53d1\u73b0\u4e0e\u5b58\u50a8**\uff1a\u8fd9\u4e00\u90e8\u5206\u7528\u6237\u5e94\u8be5\u6709\u6240\u4f53\u9a8c\uff0c\u5982Kafka\u8fd9\u7c7b\u5b58\u50a8\u5f15\u64ce\uff0c\u6ca1\u6709\u8bb0\u5f55\u6570\u636e\u7ed3\u6784\u7684\u529f\u80fd\uff0c\u4f46\u6211\u4eec\u5728\u8bfb\u53d6\u6570\u636e\u65f6\u53c8\u5fc5\u987b\u662f\u7ed3\u6784\u5316\u7684\uff0c\u5bfc\u81f4\u6bcf\u6b21\u8bfb\u53d6\u4e00\u4e2atopic\u4e4b\u524d\uff0c\u7528\u6237\u90fd\u5fc5\u987b\u5b9a\u4e49topic\u7684\u7ed3\u6784\u5316\u6570\u636e\u7c7b\u578b\uff0c\u6211\u4eec\u5e0c\u671b\u505a\u5230\u7528\u6237\u53ea\u9700\u8981\u5b8c\u6210\u4e00\u6b21\u914d\u7f6e\uff0c\u51cf\u5c11\u91cd\u590d\u7684\u64cd\u4f5c\u3002\\n    \\n\\n\u53ef\u80fd\u4e5f\u6709\u540c\u5b66\u6709\u7591\u60d1\u4e3a\u4ec0\u4e48\u6211\u4eec\u4e0d\u76f4\u63a5\u4f7f\u7528Apache Beam\uff0cBeam\u7684Source\u5206\u4e3aBOUNDED\u4e0eUNBOUNDED\uff0c\u4e5f\u5c31\u662f\u9700\u8981\u5b9e\u73b0\u4e24\u904d\uff0c\u5e76\u4e14\u6709\u4e9bSource\u4e0eSink\u7684\u7279\u6027\u4e5f\u4e0d\u652f\u6301\uff0c\u5177\u4f53\u6240\u9700\u7684\u7279\u6027\u5728\u540e\u9762\u4f1a\u63d0\u5230\uff1b\\n\\n## **03** Apache SeaTunnel(Incubating)\u91cd\u6784\u6574\u4f53\u7684\u8bbe\u8ba1\\n\\n\\n![](/image/20220531/ch/1.jpg)\\n\\n\\nApache SeaTunnel(Incubating) API\u603b\u4f53\u7ed3\u6784\u7684\u8bbe\u8ba1\u5982\u4e0a\u56fe\uff1b\\n\\n**Source & Sink API**\uff1a\u6570\u636e\u96c6\u6210\u7684\u6838\u5fc3API\u4e4b\u4e00\uff0c\u660e\u786eSource\u7684\u5206\u7247\u5e76\u884c\u903b\u8f91\u548cSink\u7684\u63d0\u4ea4\u903b\u8f91\uff0c\u7528\u4e8e\u5b9e\u73b0Connector\uff1b\\n\\n**Engine API**\uff1a\\n\\nTranslation: \u7ffb\u8bd1\u5c42\uff0c\u7528\u4e8e\u5c06SeaTunnel\u7684Souce\u4e0eSink API\u7ffb\u8bd1\u6210\u5f15\u64ce\u5185\u90e8\u53ef\u4ee5\u8fd0\u884c\u7684Connector\uff1b\\n\\n**Execution**\uff1a\u6267\u884c\u903b\u8f91\uff0c\u7528\u4e8e\u5b9a\u4e49Source\u3001Transform\u3001Sink\u7b49\u64cd\u4f5c\u5728\u5f15\u64ce\u5185\u90e8\u7684\u6267\u884c\u903b\u8f91\uff1b\\n\\n**Table API**\uff1a\\n\\n**Table SPI**\uff1a\u4e3b\u8981\u7528\u4e8e\u4ee5SPI\u7684\u65b9\u5f0f\u66b4\u9732Source\u4e0eSink\u63a5\u53e3\uff0c\u5e76\u660e\u786eConnector\u7684\u5fc5\u586b\u4e0e\u53ef\u9009\u53c2\u6570\u7b49\uff1b\\n\\n**DataType**\uff1aSeaTunnel\u7684\u6570\u636e\u7ed3\u6784\uff0c\u7528\u4e8e\u9694\u79bb\u5f15\u64ce\uff0c\u58f0\u660eTable Schema\u7b49\uff1b\\n\\n**Catalog**\uff1a\u7528\u4e8e\u83b7\u53d6Table Scheme\u3001Options\u7b49\uff1b\\n\\n**Catalog Storage**: \u7528\u4e8e\u5b58\u50a8\u7528\u6237\u5b9a\u4e49Kafka\u7b49\u975e\u7ed3\u6784\u5316\u5f15\u64ce\u7684Table Scheme\u7b49\uff1b\\n\\n\\n![](/image/20220531/ch/2.jpg)\\n\\n\\n**\u4ece\u4e0a\u56fe\u662f\u6211\u4eec\u73b0\u5728\u8bbe\u60f3\u7684\u6267\u884c\u6d41\u7a0b**\uff1a\\n\\n1.  \u4ece\u914d\u7f6e\u6587\u4ef6\u6216UI\u7b49\u65b9\u5f0f\u83b7\u53d6\u4efb\u52a1\u53c2\u6570\uff1b\\n    \\n2.  \u901a\u8fc7\u53c2\u6570\u4eceCatalog\u4e2d\u89e3\u6790\u5f97\u5230Table Schema\u3001Option\u7b49\u4fe1\u606f\uff1b\\n    \\n3.  \u4ee5SPI\u65b9\u5f0f\u62c9\u8d77SeaTunnel\u7684Connector\uff0c\u5e76\u6ce8\u5165Table\u4fe1\u606f\u7b49\uff1b\\n    \\n4.  \u5c06SeaTunnel\u7684Connector\u7ffb\u8bd1\u4e3a\u5f15\u64ce\u5185\u90e8\u7684Connector\uff1b\\n    \\n5.  \u6267\u884c\u5f15\u64ce\u7684\u4f5c\u4e1a\u903b\u8f91\uff0c\u56fe\u4e2d\u7684\u591a\u8868\u5206\u53d1\u76ee\u524d\u53ea\u5b58\u5728CDC\u6574\u5e93\u540c\u6b65\u573a\u666f\u4e0b\uff0c\u5176\u4ed6Connector\u90fd\u662f\u5355\u8868\uff0c\u4e0d\u9700\u8981\u5206\u53d1\u903b\u8f91\uff1b\\n    \\n\\n\u4ece\u4ee5\u4e0a\u53ef\u4ee5\u770b\u51fa\uff0c\u6700\u96be\u7684\u90e8\u5206\u662f**\u5982\u4f55\u5c06Apache SeaTunnel(Incubating) \u7684Source\u548cSink\u7ffb\u8bd1\u6210\u5f15\u64ce\u5185\u90e8\u7684Source\u548cSink\u3002**\\n\\n\u5f53\u4e0b\u8bb8\u591a\u7528\u6237\u4e0d\u4ec5\u628aApache SeaTunnel (Incubating) \u5f53\u505a\u4e00\u4e2a\u6570\u636e\u96c6\u6210\u65b9\u5411\u7684\u5de5\u5177\uff0c\u4e5f\u5f53\u505a\u6570\u4ed3\u65b9\u5411\u7684\u5de5\u5177\uff0c\u4f1a\u4f7f\u7528\u5f88\u591aSpark\u548cFlink\u7684SQL\uff0c\u6211\u4eec\u76ee\u524d\u5e0c\u671b\u80fd\u591f\u4fdd\u7559\u8fd9\u6837\u7684SQL\u80fd\u529b\uff0c\u8ba9\u7528\u6237\u5b9e\u73b0\u65e0\u7f1d\u5347\u7ea7\u3002\\n\\n\\n![](/image/20220531/ch/3.jpg)\\n\\n\\n\u6839\u636e\u6211\u4eec\u7684\u8c03\u7814\uff0c\u5982\u4e0a\u56fe\uff0c\u662f\u5bf9Source\u4e0eSink\u7684\u7406\u60f3\u6267\u884c\u903b\u8f91\uff0c\u7531\u4e8eSeaTunnel\u4ee5WaterDrop\u5b75\u5316\uff0c\u6240\u4ee5\u56fe\u4e0a\u7684\u672f\u8bed\u504f\u5411Spark\uff1b\\n\\n\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u5728Driver\u4e0a\u53ef\u4ee5\u8fd0\u884cSource\u548cSink\u7684\u534f\u8c03\u5668\uff0c\u7136\u540eWorker\u4e0a\u8fd0\u884cSource\u7684Reader\u548cSink\u7684Writer\u3002\u5728Source\u534f\u8c03\u5668\u65b9\u9762\uff0c\u6211\u4eec\u5e0c\u671b\u5b83\u80fd\u652f\u6301\u51e0\u4e2a\u80fd\u529b\u3002\\n\\n**\u4e00\u3001\u662f\u6570\u636e\u7684\u5206\u7247\u903b\u8f91**\uff0c\u53ef\u4ee5\u5c06\u5206\u7247\u52a8\u6001\u6dfb\u52a0\u5230Reader\u4e2d\u3002\\n\\n**\u4e8c\u3001\u662f\u53ef\u4ee5\u652f\u6301Reader\u7684\u534f\u8c03**\u3002SourceReader\u7528\u4e8e\u8bfb\u53d6\u6570\u636e\uff0c\u7136\u540e\u5c06\u6570\u636e\u53d1\u9001\u5230\u5f15\u64ce\u4e2d\u6d41\u8f6c\uff0c\u6700\u7ec8\u6d41\u8f6c\u5230Source Writer\u4e2d\u8fdb\u884c\u6570\u636e\u5199\u5165\uff0c\u540c\u65f6Writer\u53ef\u4ee5\u652f\u6301\u4e8c\u9636\u6bb5\u4e8b\u52a1\u63d0\u4ea4\uff0c\u5e76\u7531Sink\u7684\u534f\u8c03\u5668\u652f\u6301Iceberg\u7b49Connector\u7684\u805a\u5408\u63d0\u4ea4\u9700\u6c42\uff1b\\n\\n## **04** Source API\\n\\n\u901a\u8fc7\u6211\u4eec\u7684\u8c03\u7814\uff0c\u53d1\u73b0Source\u6240\u9700\u8981\u7684\u4ee5\u4e0b\u7279\u6027\uff1a\\n\\n1.  **\u7edf\u4e00\u79bb\u7ebf\u548c\u5b9e\u65f6API**\uff1aSource\u53ea\u5b9e\u73b0\u4e00\u6b21\uff0c\u540c\u65f6\u652f\u6301\u79bb\u7ebf\u548c\u5b9e\u65f6\uff1b\\n    \\n2.  **\u80fd\u591f\u652f\u6301\u5e76\u884c\u8bfb\u53d6**\uff1a\u6bd4\u5982Kafka\u6bcf\u4e00\u4e2a\u5206\u533a\u90fd\u751f\u6210\u4e00\u4e2a\u7684\u8bfb\u53d6\u5668\uff0c\u5e76\u884c\u7684\u6267\u884c\uff1b\\n    \\n3.  **\u652f\u6301\u52a8\u6001\u6dfb\u52a0\u5206\u7247**\uff1a\u6bd4\u5982Kafka\u5b9a\u4e8e\u4e00\u4e2atopic\u6b63\u5219\uff0c\u7531\u4e8e\u4e1a\u52a1\u91cf\u7684\u9700\u6c42\uff0c\u9700\u8981\u65b0\u589e\u4e00\u4e2atopic\uff0c\u8be5Source API\u53ef\u4ee5\u652f\u6301\u6211\u4eec\u52a8\u6001\u6dfb\u52a0\u5230\u4f5c\u4e1a\u4e2d\u3002\\n    \\n4.  **\u652f\u6301\u534f\u8c03\u8bfb\u53d6\u5668\u7684\u5de5\u4f5c**\uff1a\u8fd9\u4e2a\u76ee\u524d\u53ea\u53d1\u73b0\u5728CDC\u8fd9\u79cdConnector\u9700\u8981\u652f\u6301\u3002CDC\u76ee\u524d\u90fd\u662f\u57fa\u4e8eNetfilx\u7684DBlog\u5e76\u884c\u7b97\u6cd5\u53bb\u652f\u6301\uff0c\u8be5\u60c5\u51b5\u5728\u5168\u91cf\u540c\u6b65\u548c\u589e\u91cf\u540c\u6b65\u4e24\u4e2a\u9636\u6bb5\u7684\u5207\u6362\u65f6\u9700\u8981\u534f\u8c03\u8bfb\u53d6\u5668\u7684\u5de5\u4f5c\u3002\\n    \\n5.  **\u652f\u6301\u5355\u4e2a\u8bfb\u53d6\u5668\u5904\u7406\u591a\u5f20\u8868**\uff1a\u5373\u7531\u524d\u9762\u63d0\u5230\u7684\u652f\u6301\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u6570\u636e\u5e93\u6574\u5e93\u540c\u6b65\u9700\u6c42\uff1b\\n    \\n\\n\\n![](/image/20220531/ch/4.jpg)\\n\\n\\n\u5bf9\u5e94\u4ee5\u4e0a\u9700\u6c42\uff0c\u6211\u4eec\u505a\u51fa\u4e86\u57fa\u7840\u7684API\uff0c\u5982\u4e0a\u56fe\uff0c\u76ee\u524d\u4ee3\u7801\u4ee5\u63d0\u4ea4\u5230Apache SeaTunnel(Incubating)\u7684\u793e\u533a\u4e2dapi-draft\u5206\u652f\uff0c\u611f\u5174\u8da3\u7684\u53ef\u4ee5\u67e5\u770b\u4ee3\u7801\u8be6\u7ec6\u4e86\u89e3\u3002\\n\\n### **\u5982\u4f55\u9002\u914dSpark\u548cFlink\u5f15\u64ce**\\n\\nFlink\u4e0eSpark\u90fd\u5728\u540e\u9762\u7edf\u4e00\u4e86DataSet\u4e0eDataStream API\uff0c\u5373\u80fd\u591f\u652f\u6301\u524d\u4e24\u4e2a\u7279\u6027\uff0c\u90a3\u4e48\u5bf9\u4e8e\u5269\u4e0b\u76843\u4e2a\u7279\u6027\uff1a\\n\\n-   \u5982\u4f55\u652f\u6301\u52a8\u6001\u6dfb\u52a0\u5206\u7247\uff1f\\n    \\n-   \u5982\u4f55\u652f\u6301\u534f\u8c03\u8bfb\u53d6\u5668\uff1f\\n    \\n-   \u5982\u4f55\u652f\u6301\u5355\u4e2a\u8bfb\u53d6\u5668\u5904\u7406\u591a\u5f20\u8868\uff1f\\n    \\n\\n\u5e26\u7740\u95ee\u9898\uff0c\u8fdb\u5165\u76ee\u524d\u7684\u8bbe\u8ba1\u3002\\n\\n\\n![](/image/20220531/ch/5.jpg)\\n\\n\\n\u6211\u4eec\u53d1\u73b0\u9664\u4e86**CDC**\u4e4b\u5916\uff0c\u5176\u4ed6Connector\u662f\u4e0d\u9700\u8981\u534f\u8c03\u5668\u7684\uff0c\u9488\u5bf9\u4e0d\u9700\u8981\u534f\u8c03\u5668\u7684\uff0c\u6211\u4eec\u4f1a\u6709\u4e00\u4e2a\u652f\u6301\u5e76\u884c\u7684Source\uff0c\u5e76\u8fdb\u884c\u5f15\u64ce\u7ffb\u8bd1\u3002\\n\\n\u5982\u4e0a\u56fe\u4e2d\u5de6\u8fb9\u662f\u4e00\u4e2a**\u5206\u7247\u7684enumerator**\uff0c\u53ef\u4ee5\u5217\u4e3esource\u9700\u8981\u54ea\u4e9b\u5206\u7247\uff0c\u6709\u54ea\u4e9b\u5206\u7247\uff0c\u5b9e\u65f6\u8fdb\u884c\u5206\u7247\u7684\u679a\u4e3e\uff0c\u968f\u540e\u5c06\u6bcf\u4e2a\u5206\u7247\u5206\u53d1\u5230\u771f\u6b63\u7684\u6570\u636e\u8bfb\u53d6\u6a21\u5757SourceReader\u4e2d\u3002**\u5bf9\u4e8e\u79bb\u7ebf\u4e0e\u5b9e\u65f6\u4f5c\u4e1a\u7684\u533a\u5206\u4f7f\u7528Boundedness\u6807\u8bb0**\uff0cConnector\u53ef\u4ee5\u5728\u5206\u7247\u4e2d\u6807\u8bb0\u662f\u5426\u6709\u505c\u6b62\u7684Offset\uff0c\u5982Kafka\u53ef\u4ee5\u652f\u6301\u5b9e\u65f6\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u652f\u6301\u79bb\u7ebf\u3002ParallelSource\u53ef\u4ee5\u5728\u5f15\u64ce\u8bbe\u7f6e\u4efb\u610f\u5e76\u884c\u5ea6\uff0c\u4ee5\u652f\u6301\u5e76\u884c\u8bfb\u53d6\u3002\\n\\n\\n![](/image/20220531/ch/6.jpg)\\n\\n\\n\u5728\u9700\u8981\u534f\u8c03\u5668\u7684\u573a\u666f\uff0c\u5982\u4e0a\u56fe\uff0c\u9700\u8981\u5728Reader\u548cEnumerator\u4e4b\u95f4\u8fdb\u884cEvent\u4f20\u8f93\uff0c** Enumerator**\u901a\u8fc7Reader\u53d1\u9001\u7684Event\u8fdb\u884c\u534f\u8c03\u5de5\u4f5c\u3002**Coordinated Source**\u9700\u8981\u5728\u5f15\u64ce\u5c42\u9762\u4fdd\u8bc1\u5355\u5e76\u884c\u5ea6\uff0c\u4ee5\u4fdd\u8bc1\u6570\u636e\u7684\u4e00\u81f4\u6027\uff1b\u5f53\u7136\u8fd9\u4e5f\u4e0d\u80fd\u826f\u597d\u7684\u4f7f\u7528\u5f15\u64ce\u7684\u5185\u5b58\u7ba1\u7406\u673a\u5236\uff0c\u4f46\u662f\u53d6\u820d\u662f\u5fc5\u8981\u7684\uff1b\\n\\n**\u5bf9\u4e8e\u6700\u540e\u4e00\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5982\u4f55\u652f\u6301\u5355\u4e2a\u8bfb\u53d6\u5668\u5904\u7406\u591a\u5f20\u8868\u3002\u8fd9\u4f1a\u6d89\u53ca\u5230Table API\u5c42**\uff0c\u901a\u8fc7Catalog\u8bfb\u53d6\u5230\u4e86\u6240\u6709\u9700\u8981\u7684\u8868\u540e\uff0c\u6709\u4e9b\u8868\u53ef\u80fd\u5c5e\u4e8e\u4e00\u4e2a\u4f5c\u4e1a\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u94fe\u63a5\u53bb\u8bfb\u53d6\uff0c\u6709\u4e9b\u53ef\u80fd\u9700\u8981\u5206\u5f00\uff0c\u8fd9\u4e2a\u4f9d\u8d56\u4e8eSource\u662f\u600e\u4e48\u5b9e\u73b0\u7684\u3002\u57fa\u4e8e\u8fd9\u662f\u4e00\u4e2a\u7279\u6b8a\u9700\u6c42\uff0c\u6211\u4eec\u60f3\u8981\u51cf\u5c11\u666e\u901a\u5f00\u53d1\u8005\u7684\u96be\u5ea6\uff0c\u5728Table API\u8fd9\u4e00\u5c42\uff0c\u6211\u4eec\u4f1a\u63d0\u4f9b\u4e00\u4e2aSupportMultipleTable\u63a5\u53e3\uff0c\u7528\u4e8e\u58f0\u660eSource\u652f\u6301\u591a\u8868\u7684\u8bfb\u53d6\u3002Source\u5728\u5b9e\u73b0\u65f6\uff0c\u8981\u6839\u636e\u591a\u5f20\u8868\u5b9e\u73b0\u5bf9\u5e94\u7684\u53cd\u5e8f\u5217\u5316\u5668\u3002\u9488\u5bf9\u884d\u751f\u7684\u591a\u8868\u6570\u636e\u5982\u4f55\u5206\u79bb\uff0cFlink\u5c06\u91c7\u7528Side Output\u673a\u5236\uff0cSpark\u9884\u60f3\u4f7f\u7528Filter\u6216Partition\u673a\u5236\u3002\\n\\n## **05** Sink API\\n\\n\u76ee\u524dSink\u6240\u9700\u7684\u7279\u6027\u5e76\u4e0d\u662f\u5f88\u591a\uff0c**\u7ecf\u8fc7\u8c03\u7814\u76ee\u524d\u53d1\u73b0\u6709\u4e09\u4e2a\u9700\u6c42**\uff1a\\n\\n1.  \u5e42\u7b49\u5199\u5165\uff0c\u8fd9\u4e2a\u4e0d\u9700\u8981\u5199\u4ee3\u7801\uff0c\u4e3b\u8981\u770b\u5b58\u50a8\u5f15\u64ce\u662f\u5426\u80fd\u652f\u6301\u3002\\n    \\n2.  \u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u4e3b\u6d41\u662f\u4e8c\u9636\u6bb5\u63d0\u4ea4\uff0c\u5982Kafka\u90fd\u662f\u53ef\u4ee5\u652f\u6301\u5206\u5e03\u5f0f\u4e8b\u52a1\u7684\u3002\\n    \\n3.  \u805a\u5408\u63d0\u4ea4\uff0c\u5bf9\u4e8eIceberg\u3001hoodie\u7b49\u5b58\u50a8\u5f15\u64ce\u800c\u8a00\uff0c\u6211\u4eec\u4e0d\u5e0c\u671b\u6709\u5c0f\u6587\u4ef6\u95ee\u9898\uff0c\u4e8e\u662f\u671f\u671b\u5c06\u8fd9\u4e9b\u6587\u4ef6\u805a\u5408\u6210\u4e00\u4e2a\u6587\u4ef6\uff0c\u518d\u8fdb\u884c\u63d0\u4ea4\u3002\\n    \\n\\n\u57fa\u4e8e\u4ee5\u4e0a\u4e09\u4e2a\u9700\u6c42\uff0c\u6211\u4eec\u6709\u5bf9\u5e94\u7684**\u4e09\u4e2aAPI**\uff0c\u5206\u522b\u662f**SinkWriter\u3001SinkCommitter\u3001SinkAggregated Committer**\u3002SinkWriter\u662f\u4f5c\u4e3a\u57fa\u7840\u5199\u5165\uff0c\u53ef\u80fd\u662f\u5e42\u7b49\u5199\u5165\uff0c\u4e5f\u53ef\u80fd\u4e0d\u662f\u3002SinkCommitter\u652f\u6301\u4e8c\u9636\u6bb5\u63d0\u4ea4\u3002SinkAggregatedCommitter\u652f\u6301\u805a\u5408\u63d0\u4ea4\u3002\\n\\n\\n![](/image/20220531/ch/7.jpg)\\n\\n\\n\u7406\u60f3\u72b6\u6001\u4e0b\uff0c**AggregatedCommitter**\u5355\u5e76\u884c\u7684\u5728Driver\u4e2d\u8fd0\u884c\uff0cWriter\u4e0eCommitter\u8fd0\u884c\u5728Worker\u4e2d\uff0c\u53ef\u80fd\u6709\u591a\u4e2a\u5e76\u884c\u5ea6\uff0c\u6bcf\u4e2a\u5e76\u884c\u5ea6\u90fd\u6709\u81ea\u5df1\u7684\u9884\u63d0\u4ea4\u5de5\u4f5c\uff0c\u7136\u540e\u628a\u81ea\u5df1\u63d0\u4ea4\u7684\u4fe1\u606f\u53d1\u9001\u7ed9Aggregated Committer\u518d\u8fdb\u884c\u805a\u5408\u3002\\n\\n**\u76ee\u524dSpark\u548cFlink\u7684\u9ad8\u7248\u672c\u90fd\u652f\u6301\u5728Driver**(Job Manager)\u8fd0\u884cAggregatedCommitter\uff0cworker(Job Manager)\u8fd0\u884cwriter\u548cCommitter\u3002\\n\\n\\n![](/image/20220531/ch/8.jpg)\\n\\n\\n\u4f46\u662f\u5bf9\u4e8e**Flink\u4f4e\u7248\u672c**\uff0c\u65e0\u6cd5\u652f\u6301AggregatedCommitter\u5728JM\u4e2d\u8fd0\u884c\uff0c\u6211\u4eec\u4e5f\u8fdb\u884c\u7ffb\u8bd1\u9002\u914d\u7684\u8bbe\u8ba1\u3002Writer\u4e0eCommitter\u4f1a\u4f5c\u4e3a\u524d\u7f6e\u7684\u7b97\u5b50\uff0c\u4f7f\u7528Flink\u7684ProcessFunction\u8fdb\u884c\u5305\u88f9\uff0c\u652f\u6301\u5e76\u53d1\u7684\u9884\u63d0\u4ea4\u4e0e\u5199\u5165\u5de5\u4f5c\uff0c\u57fa\u4e8eFlink\u7684Checkpoint\u673a\u5236\u5b9e\u73b0\u4e8c\u9636\u6bb5\u63d0\u4ea4\uff0c\u8fd9\u4e5f\u662f\u76ee\u524dFlink\u7684\u5f88\u591aConnector\u76842PC\u5b9e\u73b0\u65b9\u5f0f\u3002\u8fd9\u4e2aProcessFunction\u4f1a\u5c06\u9884\u63d0\u4ea4\u4fe1\u606f\u53d1\u9001\u5230\u4e0b\u6e38\u7684Aggregated Committer\u4e2d\uff0cAggregated Committer\u53ef\u4ee5\u91c7\u7528SinkFunction\u6216Process Function\u7b49\u7b97\u5b50\u5305\u88f9\uff0c\u5f53\u7136\uff0c**\u6211\u4eec\u9700\u8981\u4fdd\u8bc1AggregatedCommitter\u53ea\u4f1a\u542f\u52a8\u4e00\u4e2a\uff0c\u5373\u5355\u5e76\u884c\u5ea6**\uff0c\u5426\u5219\u805a\u5408\u63d0\u4ea4\u7684\u903b\u8f91\u5c31\u4f1a\u51fa\u73b0\u95ee\u9898\u3002\\n\\n\u611f\u8c22\u5404\u4f4d\u7684\u89c2\u770b\uff0c\u5982\u679c\u5927\u5bb6\u5bf9\u5177\u4f53\u5b9e\u73b0\u611f\u5174\u8da3\uff0c\u53ef\u4ee5\u53bb Apache SeaTunnel (Incubating) \u7684\u793e\u533a\u67e5\u770b**api-draft**\u5206\u652f\u4ee3\u7801\uff0c\u8c22\u8c22\u5927\u5bb6\u3002"},{"id":"/2022/05/10/ClickHouse","metadata":{"permalink":"/zh-CN/blog/2022/05/10/ClickHouse","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-05-10-ClickHouse.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2022-05-10-ClickHouse.md","title":"\u767e\u4ebf\u7ea7\u6570\u636e\u540c\u6b65\uff0c\u5982\u4f55\u57fa\u4e8e SeaTunnel \u7684 ClickHouse \u5b9e\u73b0\uff1f","description":"\u4f5c\u8005 | Apache SeaTunnel(Incubating) Contributor \u8303\u4f73","date":"2022-05-10T00:00:00.000Z","formattedDate":"2022\u5e745\u670810\u65e5","tags":[],"readingTime":9.77,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"Apache SeaTunnel \u4e0e\u8ba1\u7b97\u5f15\u64ce\u7684\u89e3\u8026\u4e4b\u9053\uff0c\u91cd\u6784API\u6211\u4eec\u505a\u4e86\u4e9b\u4ec0\u4e48","permalink":"/zh-CN/blog/2022/05/31/engine"},"nextItem":{"title":"SeaTunnel \u5728\u5b69\u5b50\u738b\u7684\u9009\u578b\u8fc7\u7a0b\u53ca\u5e94\u7528\u6539\u9020\u5b9e\u8df5","permalink":"/zh-CN/blog/2022/05/01/_Kidswant"}},"content":"![](/image/20220510/ch/0.jpg)\\n\\n\\n\\n\u4f5c\u8005 | Apache SeaTunnel(Incubating) Contributor \u8303\u4f73\\n\\n\u6574\u7406 | \u6d4b\u8bd5\u5de5\u7a0b\u5e08 \u51af\u79c0\u5170\\n\\n\u5bf9\u4e8e\u767e\u4ebf\u7ea7\u6279\u6570\u636e\u7684\u5bfc\u5165\uff0c\u4f20\u7edf\u7684 JDBC \u65b9\u5f0f\u5728\u4e00\u4e9b\u6d77\u91cf\u6570\u636e\u540c\u6b65\u573a\u666f\u4e0b\u7684\u8868\u73b0\u5e76\u4e0d\u5c3d\u5982\u4eba\u610f\u3002\u4e3a\u4e86\u63d0\u4f9b\u66f4\u5feb\u7684\u5199\u5165\u901f\u5ea6\uff0cApache SeaTunnel(Incubating) \u5728\u521a\u521a\u53d1\u5e03\u7684 2.1.1 \u7248\u672c\u4e2d\u63d0\u4f9b\u4e86 ClickhouseFile-Connector \u7684\u652f\u6301\uff0c\u4ee5\u5b9e\u73b0 Bulk load \u6570\u636e\u5199\u5165\u3002\\n\\nBulk load \u6307\u628a\u6d77\u91cf\u6570\u636e\u540c\u6b65\u5230\u76ee\u6807 DB \u4e2d\uff0c\u76ee\u524d SeaTunnel \u5df2\u5b9e\u73b0\u6570\u636e\u540c\u6b65\u5230 ClickHouse \u4e2d\u3002\\n\\n\u5728 Apache SeaTunnel(Incubating) 4 \u6708 Meetup \u4e0a\uff0cApache SeaTunnel(Incubating) Contributor \u8303\u4f73\u5206\u4eab\u4e86\u300a\u57fa\u4e8e SeaTunnel \u7684 ClickHouse bulk load \u5b9e\u73b0\u300b\uff0c\u8be6\u7ec6\u8bb2\u89e3\u4e86 ClickHouseFile \u9ad8\u6548\u5904\u7406\u6d77\u91cf\u6570\u636e\u7684\u5177\u4f53\u5b9e\u73b0\u539f\u7406\u548c\u6d41\u7a0b\u3002\\n\\n\u611f\u8c22\u672c\u6587\u6574\u7406\u5fd7\u613f\u8005 \u6d4b\u8bd5\u5de5\u7a0b\u5e08 \u51af\u79c0\u5170 \u5bf9 Apache SeaTunnel(Incubating) \u9879\u76ee\u7684\u652f\u6301\uff01\\n\\n\u672c\u6b21\u6f14\u8bb2\u4e3b\u8981\u5305\u542b\u4e03\u4e2a\u90e8\u5206\uff1a\\n\\n- ClickHouse Sink \u73b0\u72b6\\n    \\n- ClickHouse Sink \u5f31\u573a\u666f\\n    \\n- ClickHouseFile \u63d2\u4ef6\u4ecb\u7ecd\\n    \\n- ClickHouseFile \u6838\u5fc3\u6280\u672f\u70b9\\n    \\n- ClickHouseFile \u63d2\u4ef6\u7684\u5b9e\u73b0\u89e3\u6790\\n    \\n- \u63d2\u4ef6\u80fd\u529b\u5bf9\u6bd4\\n    \\n- \u540e\u671f\u4f18\u5316\u65b9\u5411\\n   \\n\\n\\n![](/image/20220510/ch/0-1.png)\\n\\n\\n\\n\u200b\u8303 \u4f73\u767d\u9cb8\u5f00\u6e90 \u9ad8\u7ea7\u5de5\u7a0b\u5e08\\n\\n# 01 ClickHouse Sink \u73b0\u72b6\\n\\n\u73b0\u9636\u6bb5\uff0cSeaTunnel \u628a\u6570\u636e\u540c\u6b65\u5230 ClickHouse \u7684\u6d41\u7a0b\u662f\uff1a\u53ea\u8981\u662f SeaTunnel \u652f\u6301\u7684\u6570\u636e\u6e90\uff0c\u90fd\u53ef\u4ee5\u628a\u6570\u636e\u62bd\u53d6\u51fa\u6765\uff0c\u62bd\u53d6\u51fa\u6765\u4e4b\u540e\uff0c\u7ecf\u8fc7\u8f6c\u6362\uff08\u4e5f\u53ef\u4ee5\u4e0d\u8f6c\u6362\uff09\uff0c\u76f4\u63a5\u628a\u6e90\u6570\u636e\u5199\u5165 ClickHouse sink connector \u4e2d\uff0c\u518d\u901a\u8fc7 JDBC \u5199\u5165\u5230 ClickHouse \u7684\u670d\u52a1\u5668\u4e2d\u3002\\n\\n\\n![](/image/20220510/ch/1.png)\\n\\n\\n\u4f46\u662f\uff0c\u901a\u8fc7\u4f20\u7edf\u7684 JDBC \u5199\u5165\u5230 ClickHouse \u670d\u52a1\u5668\u4e2d\u4f1a\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\u3002\\n\\n\u9996\u5148\uff0c\u73b0\u9636\u6bb5\u4f7f\u7528\u7684\u5de5\u5177\u662f ClickHouse \u63d0\u4f9b\u7684\u9a71\u52a8\uff0c\u5b9e\u73b0\u65b9\u5f0f\u662f\u901a\u8fc7 HTTP\uff0c\u7136\u800c HTTP \u5728\u67d0\u4e9b\u573a\u666f\u4e0b\uff0c\u5b9e\u73b0\u6548\u7387\u4e0d\u9ad8\u3002\u5176\u6b21\u662f\u6d77\u91cf\u6570\u636e\uff0c\u5982\u679c\u6709\u91cd\u590d\u6570\u636e\u6216\u8005\u4e00\u6b21\u6027\u5199\u5165\u5927\u91cf\u6570\u636e\uff0c\u4f7f\u7528\u4f20\u7edf\u7684\u65b9\u5f0f\u662f\u751f\u6210\u5bf9\u5e94\u7684\u63d2\u5165\u8bed\u53e5\uff0c\u901a\u8fc7 HTTP \u53d1\u9001\u5230 ClickHouse \u670d\u52a1\u5668\u7aef\uff0c\u5728\u670d\u52a1\u5668\u7aef\u6765\u8fdb\u884c\u9010\u6761\u6216\u5206\u6279\u6b21\u89e3\u6790\u3001\u6267\u884c\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6570\u636e\u538b\u7f29\u3002\\n\\n\u6700\u540e\u5c31\u662f\u6211\u4eec\u901a\u5e38\u4f1a\u9047\u5230\u7684\u95ee\u9898\uff0c\u6570\u636e\u91cf\u8fc7\u5927\u53ef\u80fd\u5bfc\u81f4 SeaTunnel \u7aef OOM\uff0c\u6216\u8005\u670d\u52a1\u5668\u7aef\u56e0\u4e3a\u5199\u5165\u6570\u636e\u91cf\u8fc7\u5927\uff0c\u9891\u7387\u8fc7\u9ad8\uff0c\u5bfc\u81f4\u670d\u52a1\u5668\u7aef\u6302\u6389\u3002\\n\\n\u4e8e\u662f\u6211\u4eec\u601d\u8003\uff0c\u662f\u5426\u6709\u6bd4 HTTP \u66f4\u5feb\u7684\u53d1\u9001\u65b9\u5f0f\uff1f\u5982\u679c\u53ef\u4ee5\u5728 SeaTunnel \u7aef\u505a\u6570\u636e\u9884\u5904\u7406\u6216\u6570\u636e\u538b\u7f29\uff0c\u90a3\u4e48\u7f51\u7edc\u5e26\u5bbd\u538b\u529b\u4f1a\u964d\u4f4e\uff0c\u4f20\u8f93\u901f\u7387\u4e5f\u4f1a\u63d0\u9ad8\u3002\\n\\n# 02 ClickHouse Sink \u7684\u5f31\u573a\u666f\\n\\n\u5982\u679c\u4f7f\u7528 HTTP \u4f20\u8f93\u534f\u8bae\uff0c\u5f53\u6570\u636e\u91cf\u8fc7\u5927\uff0c\u6279\u5904\u7406\u4ee5\u5fae\u6279\u7684\u5f62\u5f0f\u53d1\u9001\u8bf7\u6c42\uff0cHTTP \u53ef\u80fd\u5904\u7406\u4e0d\u8fc7\u6765\uff1b\\n\\n\u592a\u591a\u7684 insert \u8bf7\u6c42\uff0c\u670d\u52a1\u5668\u538b\u529b\u5927\u3002\u5047\u8bbe\u5e26\u5bbd\u53ef\u4ee5\u627f\u53d7\u5927\u91cf\u7684\u8bf7\u6c42\uff0c\u4f46\u670d\u52a1\u5668\u7aef\u4e0d\u4e00\u5b9a\u80fd\u627f\u8f7d\u3002\u7ebf\u4e0a\u7684\u670d\u52a1\u5668\u4e0d\u4ec5\u9700\u8981\u6570\u636e\u63d2\u5165\uff0c\u66f4\u91cd\u8981\u7684\u662f\u67e5\u8be2\u6570\u636e\u4e3a\u5176\u4ed6\u4e1a\u52a1\u56e2\u961f\u4f7f\u7528\u3002\u82e5\u56e0\u4e3a\u63d2\u5165\u6570\u636e\u8fc7\u591a\u5bfc\u81f4\u670d\u52a1\u5668\u96c6\u7fa4\u5b95\u673a\uff0c\u662f\u5f97\u4e0d\u507f\u5931\u7684\u3002\\n\\n# 03 ClickHouse File \u6838\u5fc3\u6280\u672f\u70b9\\n\\n\u9488\u5bf9\u8fd9\u4e9b ClickHouse \u7684\u5f31\u573a\u666f\uff0c\u6211\u4eec\u60f3\uff0c\u6709\u6ca1\u6709\u4e00\u79cd\u65b9\u5f0f\uff0c\u65e2\u80fd\u5728 Spark \u7aef\u5c31\u80fd\u5b8c\u6210\u6570\u636e\u538b\u7f29\uff0c\u8fd8\u53ef\u4ee5\u5728\u6570\u636e\u5199\u5165\u65f6\u4e0d\u589e\u52a0 Server \u7684\u8d44\u6e90\u8d1f\u8f7d\uff0c\u5e76\u4e14\u80fd\u5feb\u901f\u5199\u5165\u6d77\u91cf\u6570\u636e\uff1f\u4e8e\u662f\u6211\u4eec\u5f00\u53d1\u4e86 ClickHouseFile \u63d2\u4ef6\u6765\u6ee1\u8db3\u8fd9\u4e9b\u9700\u6c42\u3002\\n\\nClickHouseFile \u63d2\u4ef6\u7684\u5173\u952e\u6280\u672f\u662f ClickHouse -local\u3002ClickHouse-local \u6a21\u5f0f\u53ef\u4ee5\u8ba9\u7528\u6237\u80fd\u591f\u5bf9\u672c\u5730\u6587\u4ef6\u6267\u884c\u5feb\u901f\u5904\u7406\uff0c\u800c\u65e0\u9700\u90e8\u7f72\u548c\u914d\u7f6e ClickHouse \u670d\u52a1\u5668\u3002ClickHouse-local \u4f7f\u7528\u4e0e ClickHouse Server \u76f8\u540c\u7684\u6838\u5fc3\uff0c\u56e0\u6b64\u5b83\u652f\u6301\u5927\u591a\u6570\u529f\u80fd\u4ee5\u53ca\u76f8\u540c\u7684\u683c\u5f0f\u548c\u8868\u5f15\u64ce\u3002\\n\\n\u56e0\u4e3a\u6709\u8fd9 2 \u4e2a\u7279\u70b9,\u8fd9\u610f\u5473\u7740\u7528\u6237\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u672c\u5730\u6587\u4ef6\uff0c\u800c\u65e0\u9700\u5728 ClickHouse \u670d\u52a1\u5668\u7aef\u505a\u5904\u7406\u3002\u7531\u4e8e\u662f\u76f8\u540c\u7684\u683c\u5f0f\uff0c\u6211\u4eec\u5728\u8fdc\u7aef\u6216\u8005 SeaTunnel \u7aef\u8fdb\u884c\u7684\u64cd\u4f5c\u6240\u4ea7\u751f\u7684\u6570\u636e\u548c\u670d\u52a1\u5668\u7aef\u662f\u65e0\u7f1d\u517c\u5bb9\u7684\uff0c\u53ef\u4ee5\u4f7f\u7528 ClickHouse local \u6765\u8fdb\u884c\u6570\u636e\u5199\u5165\u3002ClickHouse local \u662f\u5b9e\u73b0 ClickHouseFile \u7684\u6838\u5fc3\u6280\u672f\u70b9\uff0c\u56e0\u4e3a\u6709\u4e86\u8fd9\u4e2a\u63d2\u4ef6\uff0c\u73b0\u9636\u6bb5\u624d\u80fd\u591f\u5b9e\u73b0 ClickHouse file \u8fde\u63a5\u5668\u3002\\n\\nClickHouse local \u6838\u5fc3\u4f7f\u7528\u65b9\u5f0f\uff1a\\n\\n\\n![](/image/20220510/ch/2.png)\\n\\n\\n\u7b2c\u4e00\u884c\uff1a\u5c06\u6570\u636e\u901a\u8fc7 Linux \u7ba1\u9053\u4f20\u9012\u7ed9 ClickHouse-local \u7a0b\u5e8f\u7684 test_table \u8868\u3002\\n\\n\u7b2c\u4e8c\u81f3\u4e94\u884c\uff1a\u521b\u5efa\u4e00\u4e2a result_table \u8868\u7528\u4e8e\u63a5\u6536\u6570\u636e\u3002\\n\\n\u7b2c\u516d\u884c\uff1a\u5c06\u6570\u636e\u4ece test\\\\_table \u5230 result\\\\_table \u8868\u3002\\n\\n\u7b2c\u4e03\u884c\uff1a\u5b9a\u4e49\u6570\u636e\u5904\u7406\u7684\u78c1\u76d8\u8def\u5f84\u3002\\n\\n\u901a\u8fc7\u8c03\u7528 Clickhouse-local \u7ec4\u4ef6\uff0c\u5b9e\u73b0\u5728 Apache SeaTunnel(Incubating) \u7aef\u5b8c\u6210\u6570\u636e\u6587\u4ef6\u7684\u751f\u6210\uff0c\u4ee5\u53ca\u6570\u636e\u538b\u7f29\u7b49\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002\u518d\u901a\u8fc7\u548c Server \u8fdb\u884c\u901a\u4fe1\uff0c\u5c06\u751f\u6210\u7684\u6570\u636e\u76f4\u63a5\u53d1\u9001\u5230 Clickhouse \u7684\u4e0d\u540c\u8282\u70b9\uff0c\u518d\u5c06\u6570\u636e\u6587\u4ef6\u63d0\u4f9b\u7ed9\u8282\u70b9\u67e5\u8be2\u3002\\n\\n\u539f\u9636\u6bb5\u548c\u73b0\u9636\u6bb5\u5b9e\u73b0\u65b9\u5f0f\u5bf9\u6bd4\uff1a\\n\\n\\n![](/image/20220510/ch/3.png)\\n\\n\\n\u539f\u6765\u662f Spark \u628a\u6570\u636e\u5305\u62ec insert \u8bed\u53e5\uff0c\u53d1\u9001\u7ed9\u670d\u52a1\u5668\u7aef\uff0c\u670d\u52a1\u5668\u7aef\u505a SQL \u7684\u89e3\u6790\uff0c\u8868\u7684\u6570\u636e\u6587\u4ef6\u751f\u6210\u3001\u538b\u7f29\uff0c\u751f\u6210\u5bf9\u5e94\u7684\u6587\u4ef6\u3001\u5efa\u7acb\u5bf9\u5e94\u7d22\u5f15\u3002\u82e5\u4f7f\u7528 ClickHouse local \u6280\u672f\uff0c\u5219\u7531 SeaTunnel \u7aef\u505a\u6570\u636e\u6587\u4ef6\u7684\u751f\u6210\u3001\u6587\u4ef6\u538b\u7f29\uff0c\u4ee5\u53ca\u7d22\u5f15\u7684\u521b\u5efa\uff0c\u6700\u7ec8\u4ea7\u51fa\u5c31\u662f\u7ed9\u670d\u52a1\u5668\u7aef\u4f7f\u7528\u7684\u6587\u4ef6\u6216\u6587\u4ef6\u5939\uff0c\u540c\u6b65\u7ed9\u670d\u52a1\u5668\u540e\uff0c\u670d\u52a1\u5668\u5c31\u53ea\u9700\u5bf9\u6570\u636e\u67e5\u8be2\uff0c\u4e0d\u9700\u8981\u505a\u989d\u5916\u7684\u64cd\u4f5c\u3002\\n\\n# 04 \u6838\u5fc3\u6280\u672f\u70b9\\n\\n\\n![](/image/20220510/ch/4.png)\\n\\n\\n\u4ee5\u4e0a\u6d41\u7a0b\u53ef\u4ee5\u4fc3\u4f7f\u6570\u636e\u540c\u6b65\u66f4\u52a0\u9ad8\u6548\uff0c\u5f97\u76ca\u4e8e\u6211\u4eec\u5bf9\u5176\u4e2d\u7684\u4e09\u70b9\u4f18\u5316\u3002\\n\\n\u7b2c\u4e00\uff0c\u6570\u636e\u5b9e\u9645\u4e0a\u5e08\u4ece\u7ba1\u9053\u4f20\u8f93\u5230 ClickHouseFile\uff0c\u5728\u957f\u5ea6\u548c\u5185\u5b58\u4e0a\u4f1a\u6709\u9650\u5236\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06 ClickHouse connector\uff0c\u4e5f\u5c31\u662f sink \u7aef\u6536\u5230\u7684\u6570\u636e\u901a\u8fc7 MMAP \u6280\u672f\u5199\u5165\u4e34\u65f6\u6587\u4ef6\uff0c\u518d\u7531 ClickHouse local \u8bfb\u53d6\u4e34\u65f6\u6587\u4ef6\u7684\u6570\u636e\uff0c\u751f\u6210\u6211\u4eec\u7684\u76ee\u6807 local file\uff0c\u4ee5\u8fbe\u5230\u589e\u91cf\u8bfb\u53d6\u6570\u636e\u7684\u6548\u679c\uff0c\u89e3\u51b3 OM \u7684\u95ee\u9898\u3002\\n\\n\\n![](/image/20220510/ch/5.png)\\n\\n\\n\u7b2c\u4e8c\uff0c\u652f\u6301\u5206\u7247\u3002\u56e0\u4e3a\u5982\u679c\u5728\u96c6\u7fa4\u4e2d\u4f7f\u7528\uff0c\u5982\u679c\u53ea\u751f\u6210\u4e00\u4e2a\u6587\u4ef6\u6216\u6587\u4ef6\u5939\uff0c\u5b9e\u9645\u4e0a\u6587\u4ef6\u53ea\u5206\u53d1\u5230\u4e00\u4e2a\u8282\u70b9\u4e0a\uff0c\u4f1a\u5927\u5927\u964d\u4f4e\u67e5\u8be2\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u5206\u7247\u652f\u6301\uff0c\u7528\u6237\u53ef\u4ee5\u5728\u914d\u7f6e\u6587\u4ef6\u5939\u4e2d\u8bbe\u7f6e\u5206\u7247\u7684 key\uff0c\u7b97\u6cd5\u4f1a\u5c06\u6570\u636e\u5206\u4e3a\u591a\u4e2a log file\uff0c\u5199\u5165\u5230\u4e0d\u540c\u7684\u96c6\u7fa4\u8282\u70b9\u4e2d\uff0c\u5927\u5e45\u63d0\u5347\u8bfb\u53d6\u6027\u80fd\u3002\\n\\n\\n![](/image/20220510/ch/6.png)\\n\\n\\n\u7b2c\u4e09\u4e2a\u91cd\u8981\u7684\u4f18\u5316\u662f\u6587\u4ef6\u4f20\u8f93\uff0c\u76ee\u524d SeaTunnel \u652f\u6301\u4e24\u79cd\u6587\u4ef6\u4f20\u8f93\u65b9\u5f0f\uff0c\u4e00\u79cd\u662f SCP\uff0c\u5176\u7279\u70b9\u662f\u5b89\u5168\u3001\u901a\u7528\u3001\u65e0\u9700\u989d\u5916\u914d\u7f6e\uff1b\u53e6\u4e00\u79cd\u662f RSYNC\uff0c\u5176\u6709\u70b9\u4e8b\u5feb\u901f\u9ad8\u6548\uff0c\u652f\u6301\u65ad\u70b9\u7eed\u4f20\uff0c\u4f46\u9700\u8981\u989d\u5916\u914d\u7f6e\uff0c\u7528\u6237\u53ef\u4ee5\u6839\u636e\u9700\u8981\u9009\u62e9\u9002\u5408\u81ea\u5df1\u7684\u65b9\u5f0f\u3002\\n\\n# 05 \u63d2\u4ef6\u5b9e\u73b0\u89e3\u6790\\n\\n\u6982\u62ec\u800c\u8a00\uff0cClickHouseFile \u7684\u603b\u4f53\u5b9e\u73b0\u6d41\u7a0b\u5982\u4e0b\uff1a\\n\\n\\n![](/image/20220510/ch/7.png)\\n\\n\\n- \u7f13\u5b58\u6570\u636e\uff0c\u7f13\u5b58\u5230 ClickHouse sink \u7aef\uff1b\\n    \\n- \u8c03\u7528\u672c\u5730\u7684 ClickHouse-local \u751f\u6210\u6587\u4ef6\uff1b\\n    \\n- \u5c06\u6570\u636e\u53d1\u9001\u5230 ClickHouse \u670d\u52a1\u7aef\uff1b\\n    \\n- \u6267\u884c ATTACH \u547d\u4ee4\\n    \\n\\n\u901a\u8fc7\u4ee5\u4e0a\u56db\u4e2a\u6b65\u9aa4\uff0c\u751f\u6210\u7684\u6570\u636e\u8fbe\u5230\u53ef\u67e5\u8be2\u7684\u72b6\u6001\u3002\\n\\n# 06 \u63d2\u4ef6\u80fd\u529b\u5bf9\u6bd4\\n\\n\\n![](/image/20220510/ch/8.png)\\n\\n\\n\u4ece\u6570\u636e\u4f20\u8f93\u89d2\u5ea6\u6765\u8bf4\uff0cClickHouseFile \u66f4\u9002\u7528\u4e8e\u6d77\u91cf\u6570\u636e\uff0c\u4f18\u52bf\u5728\u4e8e\u4e0d\u9700\u8981\u989d\u5916\u7684\u914d\u7f6e\uff0c\u901a\u7528\u6027\u5f3a\uff0c\u800c ClickHouseFile \u914d\u7f6e\u6bd4\u8f83\u590d\u6742\uff0c\u76ee\u524d\u652f\u6301\u7684 engine \u8f83\u5c11\uff1b\\n\\n\u5c31\u73af\u5883\u590d\u6742\u5ea6\u6765\u8bf4\uff0cClickHouse \u66f4\u9002\u5408\u73af\u5883\u590d\u6742\u5ea6\u9ad8\u7684\u60c5\u51b5\uff0c\u4e0d\u9700\u8981\u989d\u5916\u914d\u7f6e\u5c31\u80fd\u76f4\u63a5\u8fd0\u884c\uff1b\\n\\n\u5728\u901a\u7528\u6027\u4e0a\uff0cClickHouse \u7531\u4e8e\u662f SeaTunnel \u5b98\u65b9\u652f\u6301\u7684 JDBC diver\uff0c\u57fa\u672c\u4e0a\u652f\u6301\u6240\u6709\u7684 engine \u7684\u6570\u636e\u5199\u5165\uff0cClickHouseFile \u652f\u6301\u7684 engine \u76f8\u5bf9\u8f83\u5c11\uff1b\u4ece\u670d\u52a1\u5668\u538b\u529b\u65b9\u9762\u6765\u8bf4\uff0cClickHouseFile \u7684\u4f18\u52bf\u5728\u6d77\u91cf\u6570\u636e\u4f20\u8f93\u65f6\u5c31\u4f53\u73b0\u51fa\u6765\u4e86\uff0c\u4e0d\u4f1a\u5bf9\u670d\u52a1\u5668\u9020\u6210\u592a\u5927\u7684\u538b\u529b\u3002\\n\\n\u4f46\u8fd9\u4e8c\u8005\u5e76\u4e0d\u662f\u7ade\u4e89\u5173\u7cfb\uff0c\u9700\u8981\u6839\u636e\u4f7f\u7528\u573a\u666f\u6765\u9009\u62e9\u3002\\n\\n# 07 \u540e\u7eed\u8ba1\u5212\\n\\n\u76ee\u524d\u867d\u7136 SeaTunnel \u652f\u6301 ClickHouseFile \u63d2\u4ef6\uff0c\u4f46\u662f\u8fd8\u6709\u5f88\u591a\u5730\u65b9\u9700\u8981\u4f18\u5316\uff0c\u4e3b\u8981\u5305\u62ec\uff1a\\n\\n- Rsync \u652f\u6301\uff1b\\n    \\n- Exactly-Once \u652f\u6301\uff1b\\n    \\n- \u652f\u6301 Zero Copy \u4f20\u8f93\u6570\u636e\u6587\u4ef6\uff1b\\n    \\n- \u66f4\u591a Engine \u7684\u652f\u6301"},{"id":"/2022/05/01/_Kidswant","metadata":{"permalink":"/zh-CN/blog/2022/05/01/_Kidswant","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-05-01_Kidswant.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2022-05-01_Kidswant.md","title":"SeaTunnel \u5728\u5b69\u5b50\u738b\u7684\u9009\u578b\u8fc7\u7a0b\u53ca\u5e94\u7528\u6539\u9020\u5b9e\u8df5","description":"\u5728Apache SeaTunnel(Incubating) 4 \u6708Meetup\u4e0a\uff0c\u5b69\u5b50\u738b\u5927\u6570\u636e\u4e13\u5bb6\u3001OLAP\u5e73\u53f0\u67b6\u6784\u5e08 \u8881\u6d2a\u519b \u4e3a\u6211\u4eec\u5e26\u6765\u4e86\u300aApache SeaTunnel (Incubating)\u5728\u5b69\u5b50\u738b\u7684\u5e94\u7528\u5b9e\u8df5\u300b\u3002","date":"2022-05-01T00:00:00.000Z","formattedDate":"2022\u5e745\u67081\u65e5","tags":[],"readingTime":13.685,"hasTruncateMarker":false,"authors":[],"frontMatter":{},"prevItem":{"title":"\u767e\u4ebf\u7ea7\u6570\u636e\u540c\u6b65\uff0c\u5982\u4f55\u57fa\u4e8e SeaTunnel \u7684 ClickHouse \u5b9e\u73b0\uff1f","permalink":"/zh-CN/blog/2022/05/10/ClickHouse"},"nextItem":{"title":"\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","permalink":"/zh-CN/blog/\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0"}},"content":"![](/image/20220501/ch/0.png)\\n\\n\\n\\n\u5728Apache SeaTunnel(Incubating) 4 \u6708Meetup\u4e0a\uff0c\u5b69\u5b50\u738b\u5927\u6570\u636e\u4e13\u5bb6\u3001OLAP\u5e73\u53f0\u67b6\u6784\u5e08 \u8881\u6d2a\u519b \u4e3a\u6211\u4eec\u5e26\u6765\u4e86\u300aApache SeaTunnel (Incubating)\u5728\u5b69\u5b50\u738b\u7684\u5e94\u7528\u5b9e\u8df5\u300b\u3002\\n\\n\\n\u672c\u6b21\u6f14\u8bb2\u4e3b\u8981\u5305\u542b\u4e94\u4e2a\u90e8\u5206\uff1a\\n\\n- \u5b69\u5b50\u738b\u5f15\u5165Apache SeaTunnel (Incubating)\u7684\u80cc\u666f\u4ecb\u7ecd\\n\\n- \u5927\u6570\u636e\u5904\u7406\u4e3b\u6d41\u5de5\u5177\u5bf9\u6bd4\u5206\u6790\\n\\n- Apache SeaTunnel (Incubating)\u7684\u843d\u5730\u5b9e\u8df5\\n\\n- Apache SeaTunnel (Incubating)\u6539\u9020\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\\n\\n- \u5bf9\u5b69\u5b50\u738b\u672a\u6765\u53d1\u5c55\u65b9\u5411\u7684\u9884\u6d4b\u5c55\u671b\\n\\n\\n![](/image/20220501/ch/0-1.png)\\n\\n\\n\u8881\u6d2a\u519b\\n\\n\u5b69\u5b50\u738b \u5927\u6570\u636e\u4e13\u5bb6\u3001OLAP \u5e73\u53f0\u67b6\u6784\u5e08\u3002\u591a\u5e74\u5927\u6570\u636e\u5e73\u53f0\u7814\u53d1\u7ba1\u7406\u7ecf\u9a8c\uff0c\u5728\u6570\u636e\u8d44\u4ea7\u3001\u8840\u7f18\u56fe\u8c31\u3001\u6570\u636e\u6cbb\u7406\u3001OLAP \u7b49\u9886\u57df\u6709\u7740\u4e30\u5bcc\u7684\u7814\u7a76\u7ecf\u9a8c\u3002\\n\\n\\n## 01 \u80cc\u666f\u4ecb\u7ecd\\n\\n\\n![](/image/20220501/ch/1.png)\\n\\n\\n\u76ee\u524d\u5b69\u5b50\u738b\u7684OLAP\u5e73\u53f0\u4e3b\u8981\u5305\u542b\u5143\u6570\u636e\u5c42\u3001\u4efb\u52a1\u5c42\u3001\u5b58\u50a8\u5c42\u3001SQL\u5c42\u3001\u8c03\u5ea6\u5c42\u3001\u670d\u52a1\u5c42\u4ee5\u53ca\u76d1\u63a7\u5c42\u4e03\u90e8\u5206\uff0c\u672c\u6b21\u5206\u4eab\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u5c42\u4e2d\u7684\u79bb\u7ebf\u4efb\u52a1\u3002\\n\\n\\n\u5176\u5b9e\u5b69\u5b50\u738b\u5185\u90e8\u6709\u4e00\u5957\u5b8c\u6574\u7684\u91c7\u96c6\u63a8\u9001\u7cfb\u7edf\uff0c\u4f46\u7531\u4e8e\u4e00\u4e9b\u5386\u53f2\u9057\u7559\u95ee\u9898\uff0c\u516c\u53f8\u73b0\u6709\u7684\u5e73\u53f0\u65e0\u6cd5\u5feb\u901f\u652f\u6301OLAP\u5e73\u53f0\u4e0a\u7ebf\uff0c\u56e0\u6b64\u5f53\u65f6\u516c\u53f8\u53ea\u80fd\u9009\u62e9\u653e\u5f03\u81ea\u8eab\u7684\u5e73\u53f0\uff0c\u8f6c\u800c\u7740\u624b\u7814\u53d1\u65b0\u7684\u7cfb\u7edf\u3002\\n\\n\\n\u5f53\u65f6\u6446\u5728OLAP\u9762\u524d\u7684\u6709\u4e09\u4e2a\u9009\u62e9\uff1a\\n\\n\\n1\u3001\u7ed9\u4e88\u91c7\u96c6\u63a8\u9001\u7cfb\u7edf\u505a\u4e8c\u6b21\u7814\u53d1\uff1b\\n\\n2\u3001\u5b8c\u5168\u81ea\u7814\uff1b\\n\\n3\u3001\u53c2\u4e0e\u5f00\u6e90\u9879\u76ee\u3002\\n\\n\\n## 02 \u5927\u6570\u636e\u5904\u7406\u4e3b\u6d41\u5de5\u5177\u5bf9\u6bd4\u5206\u6790\\n\\n\\n\u800c\u8fd9\u4e09\u9879\u9009\u62e9\u5374\u5404\u6709\u4f18\u52a3\u3002\u82e5\u91c7\u57fa\u4e8e\u91c7\u96c6\u63a8\u9001\u505a\u4e8c\u6b21\u7814\u53d1\uff0c\u5176\u4f18\u70b9\u662f\u6709\u524d\u4eba\u7684\u7ecf\u9a8c\uff0c\u80fd\u591f\u907f\u514d\u91cd\u590d\u8e29\u5751\u3002\u4f46\u7f3a\u70b9\u662f\u4ee3\u7801\u91cf\u5927\uff0c\u7814\u8bfb\u65f6\u95f4\u3001\u7814\u8bfb\u5468\u671f\u8f83\u957f\uff0c\u800c\u4e14\u62bd\u8c61\u4ee3\u7801\u8f83\u5c11\uff0c\u4e0e\u4e1a\u52a1\u7ed1\u5b9a\u7684\u5b9a\u5236\u5316\u529f\u80fd\u8f83\u591a\uff0c\u8fd9\u4e5f\u5bfc\u81f4\u4e86\u5176\u4e8c\u5f00\u7684\u96be\u5ea6\u8f83\u5927\u3002\\n\\n\\n\u82e5\u5b8c\u5168\u81ea\u7814\uff0c\u5176\u4f18\u70b9\u7b2c\u4e00\u662f\u5f00\u53d1\u8fc7\u7a0b\u81ea\u4e3b\u53ef\u63a7\uff0c\u7b2c\u4e8c\u662f\u53ef\u4ee5\u901a\u8fc7Spark\u7b49\u4e00\u4e9b\u5f15\u64ce\u505a\u8d34\u5408\u6211\u4eec\u81ea\u8eab\u7684\u67b6\u6784\uff0c\u4f46\u7f3a\u70b9\u662f\u53ef\u80fd\u4f1a\u906d\u9047\u4e00\u4e9b\u672a\u77e5\u7684\u95ee\u9898\u3002\\n\\n\\n\u6700\u540e\u5982\u679c\u4f7f\u7528\u5f00\u6e90\u6846\u67b6\uff0c\u5176\u4f18\u70b9\u4e00\u662f\u62bd\u8c61\u4ee3\u7801\u8f83\u591a\uff0c\u4e8c\u662f\u7ecf\u8fc7\u5176\u4ed6\u5927\u5382\u6216\u516c\u53f8\u7684\u9a8c\u8bc1\uff0c\u6846\u67b6\u5728\u6027\u80fd\u548c\u7a33\u5b9a\u65b9\u9762\u80fd\u591f\u5f97\u5230\u4fdd\u969c\u3002\u56e0\u6b64\u5b69\u5b50\u738b\u5728OLAP\u6570\u636e\u540c\u6b65\u521d\u671f\uff0c\u6211\u4eec\u4e3b\u8981\u7814\u7a76\u4e86DATAX\u3001Sqoop\u548cSeaTunnel\u8fd9\u4e09\u4e2a\u5f00\u6e90\u6570\u636e\u540c\u6b65\u5de5\u5177\u3002\\n\\n\\n\\n![](/image/20220501/ch/2.png)\\n\\n\\n\u4ece\u8111\u56fe\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0cSqoop\u7684\u4e3b\u8981\u529f\u80fd\u662f\u9488\u5bf9RDB\u7684\u6570\u636e\u540c\u6b65\uff0c\u5176\u5b9e\u73b0\u65b9\u5f0f\u662f\u57fa\u4e8eMAP/REDUCE\u3002Sqoop\u62e5\u6709\u4e30\u5bcc\u7684\u53c2\u6570\u548c\u547d\u4ee4\u884c\u53ef\u4ee5\u53bb\u6267\u884c\u5404\u79cd\u64cd\u4f5c\u3002Sqoop\u7684\u4f18\u70b9\u5728\u4e8e\u5b83\u9996\u5148\u8d34\u5408Hadoop\u751f\u6001\uff0c\u5e76\u5df2\u7ecf\u652f\u6301\u5927\u90e8\u5206RDB\u5230HIVE\u4efb\u610f\u6e90\u7684\u8f6c\u6362\uff0c\u62e5\u6709\u5b8c\u6574\u7684\u547d\u4ee4\u96c6\u548cAPI\u7684\u5206\u5e03\u5f0f\u6570\u636e\u540c\u6b65\u5de5\u5177\u3002\\n\\n\\n\u4f46\u5176\u7f3a\u70b9\u662fSqoop\u53ea\u652f\u6301RDB\u7684\u6570\u636e\u540c\u6b65\uff0c\u5e76\u4e14\u5bf9\u4e8e\u6570\u636e\u6587\u4ef6\u6709\u4e00\u5b9a\u7684\u9650\u5236\uff0c\u4ee5\u53ca\u8fd8\u6ca1\u6709\u6570\u636e\u6e05\u6d17\u7684\u6982\u5ff5\u3002\\n\\n\\n\\n![](/image/20220501/ch/3.png)\\n\\n\\n\\nDataX\u7684\u4e3b\u8981\u529f\u80fd\u662f\u4efb\u610f\u6e90\u7684\u6570\u636e\u540c\u6b65\uff0c\u901a\u8fc7\u914d\u7f6e\u5316\u6587\u4ef6+\u591a\u7ebf\u7a0b\u7684\u65b9\u5f0f\u5b9e\u73b0\uff0c\u4e3b\u8981\u5206\u4e3a\u4e09\u4e2a\u6d41\u7a0b\uff1aReader\u3001Framework\u548cWriter\uff0c\u5176\u4e2dFramework\u4e3b\u8981\u8d77\u5230\u901a\u4fe1\u548c\u7559\u7a7a\u7684\u4f5c\u7528\u3002\\n\\n\\nDataX\u7684\u4f18\u70b9\u662f\u5b83\u91c7\u7528\u4e86\u63d2\u4ef6\u5f0f\u7684\u5f00\u53d1\uff0c\u62e5\u6709\u81ea\u5df1\u7684\u6d41\u63a7\u548c\u6570\u636e\u7ba1\u63a7\uff0c\u5728\u793e\u533a\u6d3b\u8dc3\u5ea6\u4e0a\uff0cDataX\u7684\u5b98\u7f51\u4e0a\u63d0\u4f9b\u4e86\u8bb8\u591a\u4e0d\u540c\u6e90\u7684\u6570\u636e\u63a8\u9001\u3002\u4f46DataX\u7684\u7f3a\u70b9\u5728\u4e8e\u5b83\u57fa\u4e8e\u5185\u5b58\uff0c\u5bf9\u6570\u636e\u91cf\u53ef\u80fd\u5b58\u5728\u9650\u5236\u3002\\n\\n\\n\\n![](/image/20220501/ch/4.png)\\n\\n\\n\\nApache SeaTunnel (Incubating)\u505a\u7684\u4e5f\u662f\u4efb\u610f\u6e90\u7684\u6570\u636e\u540c\u6b65\uff0c\u5b9e\u73b0\u6d41\u7a0b\u5206\u4e3asource\u3001transform\u548csink\u4e09\u6b65\uff0c\u57fa\u4e8e\u914d\u7f6e\u6587\u4ef6\u3001Spark\u6216Flink\u5b9e\u73b0\u3002\u5176\u4f18\u70b9\u662f\u76ee\u524d\u5b98\u7f512.1.0\u6709\u975e\u5e38\u591a\u7684\u63d2\u4ef6\u548c\u6e90\u7684\u63a8\u9001\uff0c\u57fa\u4e8e\u63d2\u4ef6\u5f0f\u7684\u601d\u60f3\u4e5f\u4f7f\u5176\u975e\u5e38\u5bb9\u6613\u6269\u5c55\uff0c\u62e5\u62b1Spark\u548cFlink\u7684\u540c\u65f6\u4e5f\u505a\u5230\u4e86\u5206\u5e03\u5f0f\u7684\u67b6\u6784\u3002\u8981\u8bf4Apache SeaTunnel (Incubating)\u552f\u4e00\u7684\u7f3a\u70b9\u53ef\u80fd\u662f\u76ee\u524d\u7f3a\u5c11IP\u7684\u8c03\u7528\uff0cUI\u754c\u9762\u9700\u8981\u81ea\u5df1\u505a\u7ba1\u63a7\u3002\\n\\n\\n\u7efc\u4e0a\u6240\u8ff0\uff0cSqoop\u867d\u7136\u662f\u5206\u5e03\u5f0f\uff0c\u4f46\u662f\u4ec5\u652f\u6301RDB\u548cHIVE\u3001Hbase\u4e4b\u95f4\u7684\u6570\u636e\u540c\u6b65\u4e14\u6269\u5c55\u80fd\u529b\u5dee\uff0c\u4e0d\u5229\u4e8e\u4e8c\u5f00\u3002DataX\u6269\u5c55\u6027\u597d\uff0c\u6574\u4f53\u6027\u7a33\u5b9a\uff0c\u4f46\u7531\u4e8e\u662f\u5355\u673a\u7248\uff0c\u65e0\u6cd5\u5206\u5e03\u5f0f\u96c6\u7fa4\u90e8\u7f72\uff0c\u4e14\u6570\u636e\u62bd\u53d6\u80fd\u529b\u548c\u673a\u5668\u6027\u80fd\u6709\u5f3a\u4f9d\u8d56\u5173\u7cfb\u3002\u800cSeaTunnel\u548cDataX\u7c7b\u4f3c\u5e76\u5f25\u8865\u4e86DataX\u975e\u5206\u5e03\u5f0f\u7684\u95ee\u9898\uff0c\u5bf9\u4e8e\u5b9e\u65f6\u6d41\u4e5f\u505a\u4e86\u5f88\u597d\u7684\u652f\u6301\uff0c\u867d\u7136\u662f\u65b0\u4ea7\u54c1\uff0c\u4f46\u793e\u533a\u6d3b\u8dc3\u5ea6\u9ad8\u3002\u57fa\u4e8e\u662f\u5426\u652f\u6301\u5206\u5e03\u5f0f\u3001\u662f\u5426\u9700\u8981\u5355\u72ec\u673a\u5668\u90e8\u7f72\u7b49\u8bf8\u591a\u56e0\u7d20\u7684\u8003\u91cf\uff0c\u6700\u540e\u6211\u4eec\u9009\u62e9\u4e86SeaTunnel\u3002\\n\\n\\n## 03 Apache SeaTunnel (Incubating)\u7684\u843d\u5730\u5b9e\u8df5\\n\\n\\n\u5728Apache SeaTunnel (Incubating)\u7684\u5b98\u7f51\u6211\u4eec\u53ef\u4ee5\u770b\u5230Apache SeaTunnel (Incubating)\u7684\u57fa\u7840\u6d41\u7a0b\u5305\u62ecsource\u3001transform\u548csink\u4e09\u90e8\u5206\u3002\u6839\u636e\u5b98\u7f51\u7684\u6307\u5357\uff0cApache SeaTunnel (Incubating)\u7684\u542f\u52a8\u9700\u8981\u914d\u7f6e\u811a\u672c\uff0c\u4f46\u7ecf\u8fc7\u6211\u4eec\u7684\u7814\u7a76\u53d1\u73b0\uff0cApache SeaTunnel (Incubating)\u7684\u6700\u7ec8\u6267\u884c\u662f\u4f9d\u8d56config\u6587\u4ef6\u7684spark-submit\u63d0\u4ea4\u7684\u4e00\u4e2aApplication\u5e94\u7528\u3002\\n\\n\\n\u8fd9\u79cd\u521d\u59cb\u5316\u65b9\u5f0f\u867d\u7136\u7b80\u5355\uff0c\u4f46\u5b58\u5728\u5fc5\u987b\u4f9d\u8d56Config\u6587\u4ef6\u7684\u95ee\u9898\uff0c\u6bcf\u6b21\u8fd0\u884c\u4efb\u52a1\u540e\u90fd\u4f1a\u751f\u6210\u518d\u8fdb\u884c\u6e05\u9664\uff0c\u867d\u7136\u53ef\u4ee5\u5728\u8c03\u5ea6\u811a\u672c\u4e2d\u52a8\u6001\u751f\u6210\uff0c\u4f46\u4e5f\u4ea7\u751f\u4e86\u4e24\u4e2a\u95ee\u9898\u30021\u3001\u9891\u7e41\u7684\u78c1\u76d8\u64cd\u4f5c\u662f\u5426\u6709\u610f\u4e49\uff1b2\u3001\u662f\u5426\u5b58\u5728\u66f4\u4e3a\u9ad8\u6548\u7684\u65b9\u5f0f\u652f\u6301Apache SeaTunnel (Incubating)\u7684\u8fd0\u884c\u3002\\n\\n\\n\\n![](/image/20220501/ch/5.png)\\n\\n\\n\\n\u57fa\u4e8e\u4ee5\u4e0a\u8003\u91cf\uff0c\u5728\u6700\u7ec8\u7684\u8bbe\u8ba1\u65b9\u6848\u4e2d\uff0c\u6211\u4eec\u589e\u52a0\u4e86\u4e00\u4e2a\u7edf\u4e00\u914d\u7f6e\u6a21\u677f\u5e73\u53f0\u6a21\u5757\u3002\u8c03\u5ea6\u65f6\u53ea\u9700\u8981\u53d1\u8d77\u4e00\u4e2a\u63d0\u4ea4\u547d\u4ee4\uff0c\u7531Apache SeaTunnel (Incubating)\u81ea\u8eab\u53bb\u7edf\u4e00\u914d\u7f6e\u6a21\u677f\u5e73\u53f0\u4e2d\u62c9\u53d6\u914d\u7f6e\u4fe1\u606f\uff0c\u518d\u53bb\u88c5\u8f7d\u548c\u521d\u59cb\u5316\u53c2\u6570\u3002\\n\\n\\n\\n![](/image/20220501/ch/5-1.png)\\n\\n\\n\\n\u4e0a\u56fe\u5c55\u793a\u7684\u4fbf\u662f\u5b69\u5b50\u738bOLAP\u7684\u4e1a\u52a1\u6d41\u7a0b\uff0c\u4e3b\u8981\u5206\u4e3a\u4e09\u5757\u3002\u6570\u636e\u4eceParquet\uff0c\u5373Hive\uff0c\u901a\u8fc7Parquet\u8868\u7684\u65b9\u5f0f\u5230KYLIN\u548cCK source\u7684\u6574\u4f53\u6d41\u7a0b\u3002\\n\\n\\n\\n![](/image/20220501/ch/7.png)\\n\\n\\n\\n\u8fd9\u662f\u6211\u4eec\u5efa\u6a21\u7684\u9875\u9762\uff0c\u4e3b\u8981\u901a\u8fc7\u62d6\u62c9\u62fd\u7684\u65b9\u5f0f\u751f\u6210\u6700\u7ec8\u6a21\u578b\uff0c\u6bcf\u4e2a\u8868\u4e4b\u95f4\u901a\u8fc7\u4e00\u4e9b\u4ea4\u6613\u64cd\u4f5c\uff0c\u53f3\u4fa7\u662f\u9488\u5bf9Apache SeaTunnel (Incubating)\u7684\u5fae\u5904\u7406\u3002\\n\\n\\n\\n![](/image/20220501/ch/8.jpg)\\n\\n\\n\\n\u56e0\u6b64\u6211\u4eec\u6700\u7ec8\u63d0\u4ea4\u7684\u547d\u4ee4\u5982\u4e0a\uff0c\u5176\u4e2d\u6807\u7ea2\u7684\u9996\u5148\u662f\u3010-conf customconfig/jars\u3011\uff0c\u6307\u7528\u6237\u53ef\u4ee5\u518d\u7edf\u4e00\u914d\u7f6e\u6a21\u677f\u5e73\u53f0\u8fdb\u884c\u5904\u7406\uff0c\u6216\u8005\u5efa\u6a21\u65f6\u5355\u72ec\u6307\u5b9a\u3002\u6700\u540e\u6807\u7ea2\u7684\u3010421 $start_time $end_time $taskType\u3011Unicode\uff0c\u5c5e\u4e8e\u552f\u4e00\u7f16\u7801\u3002\\n\\n\\n\u4e0b\u65b9\u56fe\u5de6\u5c31\u662f\u6211\u4eec\u6700\u7ec8\u8c03\u5ea6\u811a\u672c\u63d0\u4ea4\u768438\u4e2a\u547d\u4ee4\uff0c\u4e0b\u65b9\u56fe\u53f3\u662f\u9488\u5bf9Apache SeaTunnel (Incubating)\u505a\u7684\u6539\u9020\uff0c\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u8f83\u4e3a\u7279\u6b8a\u7684\u540d\u4e3aWaterdropContext\u7684\u5de5\u5177\u7c7b\u3002\u53ef\u4ee5\u9996\u5148\u5224\u65adUnicode\u662f\u5426\u5b58\u5728\uff0c\u518d\u901a\u8fc7Unicode_code\u6765\u83b7\u53d6\u4e0d\u540c\u6a21\u677f\u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u907f\u514d\u4e86config\u6587\u4ef6\u7684\u64cd\u4f5c\u3002\\n\\n\\n\u5728\u6700\u540e\u7684reportMeta\u5219\u662f\u7528\u4e8e\u5728\u4efb\u52a1\u6267\u884c\u5b8c\u6210\u540e\u4e0a\u62a5\u4e00\u4e9b\u4fe1\u606f\uff0c\u8fd9\u4e5f\u4f1a\u5728Apache SeaTunnel (Incubating)\u4e2d\u5b8c\u6210\u3002\\n\\n\\n\\n![](/image/20220501/ch/9.png)\\n\\n\\n\\n![](/image/20220501/ch/10.png)\\n\\n\\n\\n![](/image/20220501/ch/11.png)\\n\\n\\n\\n\u5728\u6700\u7ec8\u5b8c\u6210\u7684config\u6587\u4ef6\u5982\u4e0a\uff0c\u503c\u5f97\u6ce8\u610f\u7684\u662f\u5728transform\u65b9\u9762\uff0c\u5b69\u5b50\u738b\u505a\u4e86\u4e00\u4e9b\u6539\u9020\u3002\u9996\u5148\u662f\u9488\u5bf9\u624b\u673a\u6216\u8005\u8eab\u4efd\u8bc1\u53f7\u7b49\u505a\u8131\u654f\u5904\u7406\uff0c\u5982\u679c\u7528\u6237\u6307\u5b9a\u5b57\u6bb5\uff0c\u5c31\u6309\u7167\u5b57\u6bb5\u505a\uff0c\u5982\u679c\u4e0d\u6307\u5b9a\u5b57\u6bb5\u5c31\u626b\u63cf\u6240\u6709\u5b57\u6bb5\uff0c\u7136\u540e\u6839\u636e\u6a21\u5f0f\u5339\u914d\uff0c\u8fdb\u884c\u8131\u654f\u52a0\u5bc6\u3002\\n\\n\\n\u7b2c\u4e8ctransform\u8fd8\u652f\u6301\u81ea\u5b9a\u4e49\u5904\u7406\uff0c\u5982\u4e0a\u6587\u8bf4\u9053OLAP\u5efa\u6a21\u7684\u65f6\u5019\u8bf4\u5230\u3002\u52a0\u5165\u4e86HideStr\uff0c\u53ef\u4ee5\u4fdd\u7559\u4e00\u4e32\u5b57\u7b26\u7684\u524d\u5341\u4e2a\u5b57\u6bb5\uff0c\u52a0\u5bc6\u540e\u65b9\u7684\u6240\u6709\u5b57\u7b26\uff0c\u5728\u6570\u636e\u5b89\u5168\u4e0a\u6709\u6240\u4fdd\u969c\u3002\\n\\n\\n\u7136\u540e\uff0c\u5728sink\u7aef\uff0c\u6211\u4eec\u4e3a\u4e86\u652f\u6301\u4efb\u52a1\u7684\u5e42\u7b49\u6027\uff0c\u6211\u4eec\u52a0\u5165\u4e86pre_sql\uff0c\u8fd9\u4e3b\u8981\u5b8c\u6210\u7684\u4efb\u52a1\u662f\u6570\u636e\u7684\u5220\u9664\uff0c\u6216\u5206\u533a\u7684\u5220\u9664\uff0c\u56e0\u4e3a\u4efb\u52a1\u5728\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u80fd\u53ea\u8fd0\u884c\u4e00\u6b21\uff0c\u4e00\u65e6\u51fa\u73b0\u91cd\u8dd1\u6216\u8865\u6570\u7b49\u64cd\u4f5c\uff0c\u5c31\u9700\u8981\u8fd9\u4e00\u90e8\u5206\u4e3a\u6570\u636e\u7684\u4e0d\u540c\u548c\u6b63\u786e\u6027\u505a\u8003\u91cf\u3002\\n\\n\\n\u5728\u56fe\u53f3\u65b9\u7684\u4e00\u4e2aClickhouse\u7684Sink\u7aef\uff0c\u8fd9\u91cc\u6211\u4eec\u52a0\u5165\u4e86\u4e00\u4e2ais_senseless_mode\uff0c\u5b83\u7ec4\u6210\u4e86\u4e00\u4e2a\u8bfb\u5199\u5206\u79bb\u7684\u65e0\u611f\u6a21\u5f0f\uff0c\u7528\u6237\u5728\u67e5\u8be2\u548c\u8865\u6570\u7684\u65f6\u5019\u4e0d\u611f\u77e5\u6574\u4f53\u533a\u57df\uff0c\u800c\u662f\u7528\u5230CK\u7684\u5206\u533a\u8f6c\u6362\uff0c\u5373\u540d\u4e3aMOVE PARTITION TO TABLE\u7684\u547d\u4ee4\u8fdb\u884c\u64cd\u4f5c\u7684\u3002\\n\\n\\n\\n\u6b64\u5904\u7279\u522b\u8bf4\u660eKYLIN\u7684Sink\u7aef\uff0cKYLIN\u662f\u4e00\u4e2a\u975e\u5e38\u7279\u6b8a\u7684\u6e90\uff0c\u62e5\u6709\u81ea\u5df1\u4e00\u6574\u5957\u6570\u636e\u5f55\u5165\u7684\u903b\u8f91\uff0c\u800c\u4e14\uff0c\u4ed6\u6709\u81ea\u5df1\u7684\u76d1\u63a7\u9875\u9762\uff0c\u56e0\u6b64\u6211\u4eec\u7ed9\u4e88KYLIN\u7684\u6539\u9020\u53ea\u662f\u7b80\u5355\u5730\u8c03\u7528\u5176API\u64cd\u4f5c\uff0c\u5728\u4f7f\u7528KYLIN\u65f6\u4e5f\u53ea\u662f\u7b80\u5355\u7684API\u8c03\u7528\u548c\u4e0d\u65ad\u8f6e\u8be2\u7684\u72b6\u6001\uff0c\u6240\u4ee5KYLIN\u8fd9\u5757\u7684\u8d44\u6e90\u5728\u7edf\u4e00\u6a21\u677f\u914d\u7f6e\u5e73\u53f0\u5c31\u88ab\u9650\u5236\u5730\u5f88\u5c0f\u3002\\n\\n\\n\\n![](/image/20220501/ch/12.jpg)\\n\\n\\n\\n## 04 Apache SeaTunnel (Incubating)\u6539\u9020\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\\n\\n\\n1\u3001OOM&Too many Parts\\n\\n\\n\u95ee\u9898\u901a\u5e38\u4f1a\u51fa\u73b0\u5728Hive\u5230Hive\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5373\u4f7f\u6211\u4eec\u901a\u8fc7\u4e86\u81ea\u52a8\u8d44\u6e90\u7684\u5206\u914d\uff0c\u4f46\u4e5f\u5b58\u5728\u6570\u636e\u7a81\u7136\u95f4\u53d8\u5927\u7684\u60c5\u51b5\uff0c\u6bd4\u5982\u5728\u4e3e\u529e\u4e86\u591a\u6b21\u6d3b\u52a8\u4e4b\u540e\u3002\u8fd9\u6837\u7684\u95ee\u9898\u5176\u5b9e\u53ea\u80fd\u901a\u8fc7\u624b\u52a8\u52a8\u6001\u5730\u8c03\u53c2\uff0c\u8c03\u6574\u6570\u636e\u540c\u6b65\u6279\u91cf\u65f6\u95f4\u6765\u907f\u514d\u3002\u672a\u6765\u6211\u4eec\u53ef\u80fd\u5c3d\u529b\u53bb\u5b8c\u6210\u5bf9\u4e8e\u6570\u636e\u91cf\u7684\u638c\u63e1\uff0c\u505a\u5230\u7cbe\u7ec6\u7684\u63a7\u5236\u3002\\n\\n\\n2\u3001\u5b57\u6bb5\u3001\u7c7b\u578b\u4e0d\u4e00\u81f4\u95ee\u9898\\n\\n\\n\u6a21\u578b\u4e0a\u7ebf\u540e\uff0c\u4efb\u52a1\u4f9d\u8d56\u7684\u4e0a\u6e38\u8868\u6216\u8005\u5b57\u6bb5\uff0c\u7528\u6237\u90fd\u4f1a\u505a\u4e00\u4e9b\u4fee\u6539\uff0c\u8fd9\u4e9b\u4fee\u6539\u82e5\u65e0\u6cd5\u611f\u77e5\uff0c\u53ef\u80fd\u5bfc\u81f4\u4efb\u52a1\u7684\u5931\u8d25\u3002\u76ee\u524d\u89e3\u51b3\u65b9\u6cd5\u662f\u4f9d\u6258\u8840\u7f18+\u5feb\u7167\u7684\u65b9\u5f0f\u8fdb\u884c\u63d0\u524d\u611f\u77e5\u6765\u907f\u514d\u9519\u8bef\u3002\\n\\n\\n3\u3001\u81ea\u5b9a\u4e49\u6570\u636e\u6e90&\u81ea\u5b9a\u4e49\u5206\u9694\u7b26\\n\\n\\n\u5982\u8d22\u52a1\u90e8\u95e8\u9700\u8981\u5355\u72ec\u4f7f\u7528\u7684\u5206\u5272\u7b26\uff0c\u6216\u662fjar\u4fe1\u606f\uff0c\u73b0\u5728\u7528\u6237\u53ef\u4ee5\u81ea\u5df1\u5728\u7edf\u4e00\u914d\u7f6e\u6a21\u677f\u5e73\u53f0\u6307\u5b9a\u52a0\u8f7d\u989d\u5916jar\u4fe1\u606f\u4ee5\u53ca\u5206\u5272\u7b26\u4fe1\u606f\u3002\\n\\n\\n4\u3001\u6570\u636e\u503e\u659c\u95ee\u9898\\n\\n\\n\u8fd9\u53ef\u80fd\u56e0\u4e3a\u7528\u6237\u81ea\u5df1\u8bbe\u7f6e\u4e86\u5e76\u884c\u5ea6\uff0c\u4f46\u65e0\u6cd5\u505a\u5230\u5c3d\u5584\u5c3d\u7f8e\u3002\u8fd9\u4e00\u5757\u6211\u4eec\u6682\u65f6\u8fd8\u6ca1\u6709\u5b8c\u6210\u5904\u7406\uff0c\u540e\u7eed\u7684\u601d\u8def\u53ef\u80fd\u5728Source\u6a21\u5757\u4e2d\u6dfb\u52a0post\u5904\u7406\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u6253\u6563\uff0c\u5b8c\u6210\u503e\u659c\u3002\\n\\n\\n5\u3001KYLIN\u5168\u5c40\u5b57\u5178\u9501\u95ee\u9898\\n\\n\\n\u968f\u7740\u4e1a\u52a1\u53d1\u5c55\uff0c\u4e00\u4e2acube\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u4f7f\u7528\uff0c\u5c31\u80fd\u9700\u8981\u5efa\u7acb\u591a\u4e2acube\uff0c\u5982\u679c\u591a\u4e2acube\u4e4b\u95f4\u7528\u4e86\u76f8\u540c\u7684\u5b57\u6bb5\uff0c\u5c31\u4f1a\u9047\u5230KYLIN\u5168\u5c40\u5b57\u5178\u9501\u7684\u95ee\u9898\u3002\u76ee\u524d\u89e3\u51b3\u7684\u601d\u8def\u662f\u628a\u4e24\u4e2a\u6216\u591a\u4e2a\u4efb\u52a1\u4e4b\u95f4\u7684\u8c03\u5ea6\u65f6\u95f4\u8fdb\u884c\u9694\u5f00\uff0c\u5982\u679c\u65e0\u6cd5\u9694\u5f00\uff0c\u53ef\u4ee5\u505a\u4e00\u4e2a\u5206\u5e03\u5f0f\u9501\u7684\u63a7\u5236\u3002KYLIN\u7684sink\u7aef\u5fc5\u987b\u8981\u62ff\u5230\u9501\u624d\u80fd\u8fd0\u884c\u3002\\n\\n\\n05 \u5bf9\u5b69\u5b50\u738b\u672a\u6765\u53d1\u5c55\u65b9\u5411\u7684\u9884\u6d4b\u5c55\u671b\\n\\n\\n- \u591a\u6e90\u6570\u636e\u540c\u6b65\uff0c\u672a\u6765\u53ef\u80fd\u9488\u5bf9RDB\u6e90\u8fdb\u884c\u5904\u7406\\n\\n- \u57fa\u4e8e\u5b9e\u65f6Flink\u7684\u5b9e\u73b0\\n\\n- \u63a5\u7ba1\u5df2\u6709\u91c7\u96c6\u8c03\u5ea6\u5e73\u53f0\uff08\u4e3b\u8981\u89e3\u51b3\u5206\u5e93\u5206\u8868\u7684\u95ee\u9898\uff09\\n\\n- \u6570\u636e\u8d28\u91cf\u6821\u9a8c\uff0c\u50cf\u4e00\u4e9b\u7a7a\u503c\u3001\u6574\u4e2a\u6570\u636e\u7684\u7a7a\u7f6e\u7387\u3001\u4e3b\u65f6\u95f4\u7684\u5224\u65ad\u7b49\\n\\n\\n\u6211\u7684\u5206\u4eab\u5c31\u5230\u8fd9\u91cc\uff0c\u5e0c\u671b\u4ee5\u540e\u53ef\u4ee5\u548c\u793e\u533a\u591a\u591a\u4ea4\u6d41\uff0c\u5171\u540c\u8fdb\u6b65\uff0c\u8c22\u8c22\uff01"},{"id":"\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","metadata":{"permalink":"/zh-CN/blog/\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-04-08-Innovation_of_Data_Integration_Technology_in_Intelligent_Era.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2022-04-08-Innovation_of_Data_Integration_Technology_in_Intelligent_Era.md","title":"\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","description":"1","date":"2022-04-08T00:00:00.000Z","formattedDate":"2022\u5e744\u67088\u65e5","tags":[{"label":"Meetup","permalink":"/zh-CN/blog/tags/meetup"}],"readingTime":4.19,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","title":"\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","tags":["Meetup"]},"prevItem":{"title":"SeaTunnel \u5728\u5b69\u5b50\u738b\u7684\u9009\u578b\u8fc7\u7a0b\u53ca\u5e94\u7528\u6539\u9020\u5b9e\u8df5","permalink":"/zh-CN/blog/2022/05/01/_Kidswant"},"nextItem":{"title":"2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall","permalink":"/zh-CN/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall"}},"content":"![1](/image/20220416/1.png)\\n\\n\\n\u53ef\u7ba1\u7406\uff0c\u53ef\u8c03\u7528\uff0c\u53ef\u8ba1\u7b97\uff0c\u53ef\u53d8\u73b0\u7684\u6570\u636e\u8d44\u6e90\u624d\u80fd\u6210\u4e3a\u8d44\u4ea7\uff0c\u4fe1\u606f\u7cfb\u7edf\u7684\u4e92\u8054\u4e92\u901a\u4f7f\u5f97\u591a\u6e90\u548c\u591a\u7ef4\u5ea6\u7684\u6570\u636e\u96c6\u6210\u9700\u6c42\u5de8\u5927\uff0c\u8fd9\u5c31\u5bf9\u6570\u636e\u5904\u7406\u548c\u96c6\u6210\u7684\u5de5\u5177\u63d0\u51fa\u4e86\u4e25\u82db\u7684\u8981\u6c42\u3002\\n\\n\u667a\u80fd\u5316\u65f6\u4ee3\uff0c\u5728\u201c\u667a\u6167\u57ce\u5e02\u201d\u3001\u201c\u667a\u6167\u6cbb\u7406\u201d\u3001\u201c\u4ea7\u54c1\u667a\u80fd\u5316\u201d\u7b49\u7684\u8d8b\u52bf\u4e0b\uff0c\u4f01\u4e1a\u5927\u591a\u9762\u4e34\u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u63a8\u9001\uff0c\u63d0\u9ad8\u5e73\u53f0\u8d28\u91cf\uff0c\u4ee5\u53ca\u4fdd\u969c\u6570\u636e\u5b89\u5168\u7684\u6311\u6218\u3002\u9009\u5bf9\u6570\u636e\u96c6\u6210\u5de5\u5177\u548c\u5e73\u53f0\uff0c\u6570\u636e\u624d\u80fd\u53d1\u6325\u51fa\u505a\u5927\u7684\u4f5c\u7528\u3002\\n\\nApache SeaTunnel (Incubating) \u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u9ad8\u6027\u80fd\u3001\u5206\u5e03\u5f0f\u3001\u6d77\u91cf\u6570\u636e\u96c6\u6210\u6846\u67b6\uff0c\u81f4\u529b\u4e8e\u8ba9\u6570\u636e\u540c\u6b65\u66f4\u7b80\u5355\uff0c\u66f4\u9ad8\u6548\uff0c\u52a0\u5feb\u5206\u5e03\u5f0f\u6570\u636e\u5904\u7406\u80fd\u529b\u5728\u751f\u4ea7\u73af\u5883\u843d\u5730\u3002\\n\\n\u5728 Apache SeaTunnel(Incubating) Meetup\uff082022\xa0\u5e74\xa04\xa0\u6708\xa016\u65e5\uff09\uff0cApache SeaTunnel(Incubating) \u793e\u533a\u5c06\u9080\u8bf7\u4e86 Apache SeaTunnel(Incubating)\u7684\u8d44\u6df1\u7528\u6237\uff0c\u5206\u4eab Apache SeaTunnel(Incubating)\u5728\u667a\u80fd\u5316\u751f\u4ea7\u73af\u5883\u4e2d\u843d\u5730\u7684\u6700\u4f73\u5b9e\u8df5\u3002\u6b64\u5916\uff0c\u8fd8\u4f1a\u6709\u8d21\u732e\u8005\u73b0\u573a\u8fdb\u884c Apache SeaTunnel(Incubating)\u7684\u6e90\u7801\u89e3\u6790\uff0c\u8ba9\u4f60\u5bf9 Apache SeaTunnel(Incubating)\u6709\u4e00\u4e2a\u66f4\u52a0\u5168\u9762\u800c\u6df1\u5165\u7684\u4e86\u89e3\u3002\\n\\n\u65e0\u8bba\u4f60\u662f\u5bf9 Apache SeaTunnel(Incubating)\u62b1\u6709\u5174\u8da3\u7684\u521d\u5b66\u8005\uff0c\u8fd8\u662f\u5728\u65e5\u5e38\u7684\u751f\u4ea7\u5b9e\u8df5\u4e2d\u906d\u9047\u4e86\u590d\u6742\u68d8\u624b\u7684\u90e8\u7f72\u95ee\u9898\uff0c\u90fd\u53ef\u4ee5\u6765\u5230\u8fd9\u91cc\uff0c\u4e0e\u6211\u4eec\u7684\u8bb2\u5e08\u8fd1\u8ddd\u79bb\u6c9f\u901a\uff0c\u5f97\u5230\u4f60\u60f3\u8981\u7684\u7b54\u6848\u3002\\n\\n## 01 \u62a5 \u540d \u901a \u9053\\n\\nApache SeaTunnel (Incubating) Meetup | 4 \u6708\u7ebf\u4e0a\u76f4\u64ad\u62a5\u540d\u901a\u9053\u5df2\u5f00\u542f\uff0c\u8d76\u5feb\u9884\u7ea6\u5427\uff01\\n\\n\u65f6\u95f4\uff1a2022-4-16 14:00-17:00\\n\\n\u5f62\u5f0f\uff1a\u7ebf\u4e0a\u76f4\u64ad\\n\\n\u70b9\u51fb\u94fe\u63a5\u6216\u626b\u7801\u9884\u7ea6\u62a5\u540d\uff08\u514d\u8d39\uff09\uff1a\\n\\n![2](/image/20220416/2.png)\\n\\n\\n\u626b\u7801\u9884\u7ea6\u62a5\u540d\\n\\n![3](/image/20220416/4.png)\\n\\n\\n\u626b\u7801\u8fdb\u76f4\u64ad\u7fa4\\n\\n\\n## 02 \u6d3b \u52a8 \u4eae \u70b9\\n\\n* \u884c\u4e1a\u6848\u4f8b\u8be6\u89e3\\n* \u7279\u8272\u529f\u80fd\u5206\u6790\\n* \u4e00\u7ebf\u4f01\u4e1a\u8e29\u5751\u5fc3\u5f97\\n* \u5f00\u6e90\u793e\u533a\u5b9e\u6218\u653b\u7565\\n* \u884c\u4e1a\u6280\u672f\u4e13\u5bb6\u9762\u5bf9\u9762 Q&A\\n* \u60ca\u559c\u793c\u54c1\u9001\u4e0d\u505c\\n## 03 \u6d3b \u52a8 \u8bae \u7a0b\\n\\n\u6d3b\u52a8\u5f53\u5929\uff0c\u5c06\u6709\u6765\u81ea\u5b69\u5b50\u738b\u3001oppo \u7684\u5de5\u7a0b\u5e08\u73b0\u573a\u5206\u4eab\u6765\u81ea\u5382\u5546\u7684\u4e00\u7ebf\u524d\u6cbf\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u8fd8\u6709\u6765\u81ea\u767d\u9cb8\u5f00\u6e90\u7684\u9ad8\u7ea7\u5de5\u7a0b\u5e08\u5bf9 Apache SeaTunnel(Incubating)\u7684\u91cd\u8981\u529f\u80fd\u66f4\u65b0\u8fdb\u884c\u201c\u786c\u6838\u201d\u8bb2\u89e3\uff0c\u5e72\u8d27\u6ee1\u6ee1\u3002\\n\\n![4](/image/20220416/5.png)\\n\\n\\n\u8881\u6d2a\u519b\xa0\u5b69\u5b50\u738b \u5927\u6570\u636e\u4e13\u5bb6\u3001OLAP \u5e73\u53f0\u67b6\u6784\u5e08\\n\\n\u591a\u5e74\u5927\u6570\u636e\u5e73\u53f0\u7814\u53d1\u7ba1\u7406\u7ecf\u9a8c\uff0c\u5728\u6570\u636e\u8d44\u4ea7\u3001\u8840\u7f18\u56fe\u8c31\u3001\u6570\u636e\u6cbb\u7406\u3001OLAP \u7b49\u9886\u57df\u6709\u7740\u4e30\u5bcc\u7684\u7814\u7a76\u7ecf\u9a8c\\n\\n\u6f14\u8bb2\u65f6\u95f4\uff1a14:00-14:40\\n\\n\u6f14\u8bb2\u9898\u76ee\uff1aApache SeaTunnel(Incubating) \u5728\u5b69\u5b50\u738b\u7684\u5e94\u7528\u5b9e\u8df5\\n\\n\u6f14\u8bb2\u6982\u8981\uff1a \u5982\u4f55\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u63a8\u9001\uff1f\u5982\u4f55\u63d0\u9ad8\u5e73\u53f0\u8d28\u91cf\uff1f\u5982\u4f55\u4fdd\u969c\u6570\u636e\u5b89\u5168\uff1f\u5b69\u5b50\u738b\u53c8\u5bf9 Apache SeaTunnel(Incubating)\u505a\u4e86\u54ea\u4e9b\u6539\u9020\uff1f\\n\\n![6](/image/20220416/6.png)\\n\\n\\n\u8303\u4f73\xa0\u767d\u9cb8\u5f00\u6e90\xa0 \u9ad8\u7ea7\u5de5\u7a0b\u5e08 Apache SeaTunnel Contributor\\n\\n\u6f14\u8bb2\u65f6\u95f4\uff1a 14:40-15:20\\n\\n\u6f14\u8bb2\u9898\u76ee\uff1a \u57fa\u4e8e Apache SeaTunnel(Incubating)\u7684 Clickhouse Bulk Load \u5b9e\u73b0\\n\\n\u6f14\u8bb2\u6982\u8981\uff1a \u901a\u8fc7\u6269\u5c55 Apache SeaTunnel(Incubating)\u7684 Connector\u5b9e\u73b0 Clickhouse\u7684 bulk load \u6570\u636e\u540c\u6b65\u529f\u80fd\u3002\\n\\n\\n![7](/image/20220416/7.png)\\n\\n\\n\u738b\u5b50\u8d85\xa0oppo\xa0\u9ad8\u7ea7\u540e\u7aef\u5de5\u7a0b\u5e08\\n\\n\u6f14\u8bb2\u65f6\u95f4\uff1a 15:50-16:30\\n\\n\u6f14\u8bb2\u9898\u76ee\uff1a oppo\u667a\u80fd\u63a8\u8350\u6837\u672c\u4e2d\u5fc3\u57fa\u4e8e Apache SeaTunnel(Incubating)\u7684\u6280\u672f\u9769\u65b0\\n\\n\u6f14\u8bb2\u6982\u8981\uff1a \u4ecb\u7ecd oppo \u667a\u80fd\u63a8\u8350\u673a\u5668\u5b66\u4e60\u6837\u672c\u6d41\u7a0b\u7684\u6f14\u8fdb\u53ca Apache \xa0SeaTunnel(Incubating)\xa0\u5728\u5176\u4e2d\u53d1\u6325\u7684\u4f5c\u7528\u3002\\n\\n\u9664\u4e86\u7cbe\u5f69\u7684\u6f14\u8bb2\u4e4b\u5916\uff0c\u73b0\u573a\u8fd8\u8bbe\u7f6e\u4e86\u591a\u4e2a\u62bd\u5956\u73af\u8282\uff0c\u53c2\u4e0e\u62bd\u5956\u6709\u673a\u4f1a\u83b7\u5f97\xa0Apache\xa0SeaTunnel(Incubating)\xa0\u7cbe\u7f8e\u5b9a\u5236\u793c\u54c1\uff0c\u656c\u8bf7\u671f\u5f85~"},{"id":"2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall","metadata":{"permalink":"/zh-CN/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2022-03-18-2-1-0-release.md","source":"@site/blog/2022-03-18-2-1-0-release.md","title":"2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall","description":"On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator.","date":"2022-03-18T00:00:00.000Z","formattedDate":"2022\u5e743\u670818\u65e5","tags":[{"label":"2.1.0","permalink":"/zh-CN/blog/tags/2-1-0"},{"label":"Release","permalink":"/zh-CN/blog/tags/release"}],"readingTime":5.005,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall","title":"2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall","tags":["2.1.0","Release"]},"prevItem":{"title":"\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0","permalink":"/zh-CN/blog/\u667a\u80fd\u5316\u65f6\u4ee3\u7684\u6570\u636e\u96c6\u6210\u6280\u672f\u9769\u65b0"},"nextItem":{"title":"\u5982\u4f55\u5feb\u901f\u5730\u628a HDFS \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","permalink":"/zh-CN/blog/hdfs-to-clickhouse"}},"content":"On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator.\\n\\n**Note:** A\xa0**software license**\xa0is a legal instrument governing the use or redistribution of software. A typical software license grants the\xa0licensee, typically an\xa0end-user, permission to use one or more copies of the software in ways where such a use would otherwise potentially constitute copyright infringement of the software owner\'s\xa0exclusive rights\xa0under copyright. Effectively, a software license is a contract between the software developer and the user that guarantees the user will not be sued within the scope of the license. \\n\\nBefore and after entering the incubator, we spent a lot of time sorting through the external dependencies of the entire project to ensure compliance. It is important to note that the choice of License for open source software does not necessarily mean that the project itself is compliant. While the stringent version control process of ASF ensures compliance and legal distribution of the software license maximumly.\\n\\n## Release Note\\n\\nWe bring the following **key features**to this release:\\n\\n1. The kernel of the microkernel plug-in architecture is overall optimized, which is mainly in Java. And a lot of improvements are made to command line parameter parsing, plug-in loading, etc. At the same time, the users (or contributors) can choose the language to develop plug-in extensions, which greatly reduces the development threshold of plug-ins.\\n2. Overall support for Flink, while the users are free to choose the underlying engine. This version also brings a large number of Flink plug-ins and welcomes anyone to contribute more.\\n3. Provide local development fast startup environment support (example), allow contributors or users quickly and smoothly start without changing any code to facilitate rapid local development debugging. This is certainly exciting news for contributors or users who need to customize their plugins. In fact, we\'ve had a large number of contributors use this approach to quickly test the plugin in our pre-release testing.\\n4. With Docker container installation provided, users can deploy and install Apache SeaTunnel(Incubating) via Docker extremely fast, and we will iterate around Docker & K8s in the future, any interesting proposal on this is welcomed.\\n## Specific release notes\uff1a\\n\\n### [Features]\\n\\n* Use JCommander to do command line parameter parsing, making developers focus on the logic itself.\\n* Flink is upgraded from 1.9 to 1.13.5, keeping compatibility with older versions and preparing for subsequent CDC.\\n* Support for Doris, Hudi, Phoenix, Druid, and other Connector plugins, and you can find complete plugin support here [plugins-supported-by-seatunnel]([https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel](https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel)).\\n* Local development extremely fast starts environment support. It can be achieved by using the example module without modifying any code, which is convenient for local debugging.\\n* Support for installing and trying out Apache SeaTunnel(Incubating) via Docker containers.\\n* SQL component supports SET statements and configuration variables.\\n* Config module refactoring to facilitate understanding for the contributors while ensuring code compliance (License) of the project.\\n* Project structure realigned to fit the new Roadmap.\\n* CI&CD support, code quality automation control (more plans will be carried out to support CI&CD development).\\n\\n## Acknowledgments\\n\\nThanks to the following contributors who participated in this version release (GitHub IDs, in no particular order).\\n\\nAl-assad, BenJFan, CalvinKirs, JNSimba, JiangTChen, Rianico, TyrantLucifer, Yves-yuan, ZhangchengHu0923, agendazhang, an-shi-chi-fan, asdf2014, bigdataf, chaozwn, choucmei, dailidong, dongzl, felix-thinkingdata, fengyuceNv, garyelephant, kalencaya, kezhenxu94, legendtkl, leo65535, liujinhui1994, mans2singh, marklightning, mosence, nielifeng, ououtt, ruanwenjun, simon824, totalo, wntp, wolfboys, wuchunfu, xbkaishui, xtr1993, yx91490, zhangbutao, zhaomin1423, zhongjiajie, zhuangchong, zixi0825.\\n\\nAlso sincere gratitude to our Mentors: Zhenxu Ke, Willem Jiang, William Guo, LiDong Dai, Ted Liu, Kevin, JB for their help!\\n\\n## Planning for the next few releases:\\n\\n* CDC support.\\n* Support for the monitoring system.\\n* UI system support.\\n* More Connector and efficient Sink support, such as ClickHouse support will be available in the next release soon.\\nThe follow-up **Features** are decided by the community consensus, and we sincerely appeal to more participation in the community construction.\\n\\nWe need your attention and contributions:)\\n\\n## Community Status\\n\\n### Recent Development\\n\\nSince entering the Apache incubator, the contributor group has grown from 13 to 55 and continues to grow, with the average weekly community commits remaining at 20+. \\n\\nThree contributors from different companies (Lei Xie, HuaJie Wang, Chunfu Wu) have been invited to become Committers on account of their contributions to the community. \\n\\nWe held two Meetups, where instructors from Bilibili, OPPO, Vipshop, and other companies shared their large-scale production practices based on SeaTunnel in their companies (we will hold one meetup monthly in the future, and welcome SeaTunnel users or contributors to come and share their stories about SeaTunnel).\\n\\n### Users of Apache SeaTunnel(Incubating)\\n\\nNote: Only registered users are included.\\n\\nRegistered users of Apache SeaTunnel(Incubating) are shown below. If you are also using Apache SeaTunnel(Incubating), too, welcome to register on [Who is using SeaTunne](https://github.com/apache/incubator-seatunnel/issues/686)!\\n\\n<div align=\\"center\\">\\n\\n<img src=\\"/image/20220321/1.png\\"/>\\n\\n</div>\\n\\n## PPMC\'s Word\\n\\nLiFeng Nie, PPMC of Apache SeaTunnel(Incubating), commented on the first Apache version release. \\n\\nFrom the first day entering Apache Incubating, we have been working hard to learn the Apache Way and various Apache policies. Although the first release took a lot of time (mainly for compliance), we think it was well worth it, and that\'s one of the reasons we chose to enter Apache. We need to give our users peace of mind, and Apache is certainly the best choice, with its almost demanding license control that allows users to avoid compliance issues as much as possible and ensure that the software is circulating reasonably and legally. In addition, its practice of the Apache Way, such as public service mission, pragmatism, community over code, openness and consensus decision-making, and meritocracy, can drive the Apache SeaTunnel(Incubating) community to become more open, transparent, and diverse."},{"id":"hdfs-to-clickhouse","metadata":{"permalink":"/zh-CN/blog/hdfs-to-clickhouse","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2021-12-30-hdfs-to-clickhouse.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-hdfs-to-clickhouse.md","title":"\u5982\u4f55\u5feb\u901f\u5730\u628a HDFS \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","description":"ClickHouse \u662f\u9762\u5411 OLAP \u7684\u5206\u5e03\u5f0f\u5217\u5f0f DBMS\u3002\u6211\u4eec\u90e8\u95e8\u76ee\u524d\u5df2\u7ecf\u628a\u6240\u6709\u6570\u636e\u5206\u6790\u76f8\u5173\u7684\u65e5\u5fd7\u6570\u636e\u5b58\u50a8\u81f3 ClickHouse \u8fd9\u4e2a\u4f18\u79c0\u7684\u6570\u636e\u4ed3\u5e93\u4e4b\u4e2d\uff0c\u5f53\u524d\u65e5\u6570\u636e\u91cf\u8fbe\u5230\u4e86 300 \u4ebf\u3002","date":"2021-12-30T00:00:00.000Z","formattedDate":"2021\u5e7412\u670830\u65e5","tags":[{"label":"HDFS","permalink":"/zh-CN/blog/tags/hdfs"},{"label":"ClickHouse","permalink":"/zh-CN/blog/tags/click-house"}],"readingTime":7.13,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"hdfs-to-clickhouse","title":"\u5982\u4f55\u5feb\u901f\u5730\u628a HDFS \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","tags":["HDFS","ClickHouse"]},"prevItem":{"title":"2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall","permalink":"/zh-CN/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall"},"nextItem":{"title":"\u5982\u4f55\u5feb\u901f\u5730\u628a Hive \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","permalink":"/zh-CN/blog/hive-to-clickhouse"}},"content":"ClickHouse \u662f\u9762\u5411 OLAP \u7684\u5206\u5e03\u5f0f\u5217\u5f0f DBMS\u3002\u6211\u4eec\u90e8\u95e8\u76ee\u524d\u5df2\u7ecf\u628a\u6240\u6709\u6570\u636e\u5206\u6790\u76f8\u5173\u7684\u65e5\u5fd7\u6570\u636e\u5b58\u50a8\u81f3 ClickHouse \u8fd9\u4e2a\u4f18\u79c0\u7684\u6570\u636e\u4ed3\u5e93\u4e4b\u4e2d\uff0c\u5f53\u524d\u65e5\u6570\u636e\u91cf\u8fbe\u5230\u4e86 300 \u4ebf\u3002\\n\\n\u4e4b\u524d\u4ecb\u7ecd\u7684\u6709\u5173\u6570\u636e\u5904\u7406\u5165\u5e93\u7684\u7ecf\u9a8c\u90fd\u662f\u57fa\u4e8e\u5b9e\u65f6\u6570\u636e\u6d41\uff0c\u6570\u636e\u5b58\u50a8\u5728 Kafka \u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 Java \u6216\u8005 Golang \u5c06\u6570\u636e\u4ece Kafka \u4e2d\u8bfb\u53d6\u3001\u89e3\u6790\u3001\u6e05\u6d17\u4e4b\u540e\u5199\u5165 ClickHouse \u4e2d\uff0c\u8fd9\u6837\u53ef\u4ee5\u5b9e\u73b0\u6570\u636e\u7684\u5feb\u901f\u63a5\u5165\u3002\u7136\u800c\u5728\u5f88\u591a\u540c\u5b66\u7684\u4f7f\u7528\u573a\u666f\u4e2d\uff0c\u6570\u636e\u90fd\u4e0d\u662f\u5b9e\u65f6\u7684\uff0c\u53ef\u80fd\u9700\u8981\u5c06 HDFS \u6216\u8005\u662f Hive \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse\u3002\u6709\u7684\u540c\u5b66\u901a\u8fc7\u7f16\u5199 Spark \u7a0b\u5e8f\u6765\u5b9e\u73b0\u6570\u636e\u7684\u5bfc\u5165\uff0c\u90a3\u4e48\u662f\u5426\u6709\u66f4\u7b80\u5355\u3001\u9ad8\u6548\u7684\u65b9\u6cd5\u5462\u3002\\n\\n\u76ee\u524d\u5f00\u6e90\u793e\u533a\u4e0a\u6709\u4e00\u6b3e\u5de5\u5177 **Seatunnel**\uff0c\u9879\u76ee\u5730\u5740 [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel)\uff0c\u53ef\u4ee5\u5feb\u901f\u5730\u5c06 HDFS \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse\u3002\\n\\n## HDFS To ClickHouse\\n\\n\u5047\u8bbe\u6211\u4eec\u7684\u65e5\u5fd7\u5b58\u50a8\u5728 HDFS \u4e2d\uff0c\u6211\u4eec\u9700\u8981\u5c06\u65e5\u5fd7\u8fdb\u884c\u89e3\u6790\u5e76\u7b5b\u9009\u51fa\u6211\u4eec\u5173\u5fc3\u7684\u5b57\u6bb5\uff0c\u5c06\u5bf9\u5e94\u7684\u5b57\u6bb5\u5199\u5165 ClickHouse \u7684\u8868\u4e2d\u3002\\n\\n### Log Sample\\n\\n\u6211\u4eec\u5728 HDFS \u4e2d\u5b58\u50a8\u7684\u65e5\u5fd7\u683c\u5f0f\u5982\u4e0b\uff0c \u662f\u5f88\u5e38\u89c1\u7684 Nginx \u65e5\u5fd7\\n\\n```shell\\n10.41.1.28 github.com 114.250.140.241 0.001s \\"127.0.0.1:80\\" [26/Oct/2018:03:09:32 +0800] \\"GET /Apache/Seatunnel HTTP/1.1\\" 200 0 \\"-\\" - \\"Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)\\" \\"196\\" \\"-\\" \\"mainpage\\" \\"443\\" \\"-\\" \\"172.16.181.129\\"\\n```\\n\\n### ClickHouse Schema\\n\\n\u6211\u4eec\u7684 ClickHouse \u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff0c\u6211\u4eec\u7684\u8868\u6309\u65e5\u8fdb\u884c\u5206\u533a\\n\\n```shell\\nCREATE TABLE cms.cms_msg\\n(\\n    date Date, \\n    datetime DateTime, \\n    url String, \\n    request_time Float32, \\n    status String, \\n    hostname String, \\n    domain String, \\n    remote_addr String, \\n    data_size Int32, \\n    pool String\\n) ENGINE = MergeTree PARTITION BY date ORDER BY date SETTINGS index_granularity = 16384\\n```\\n\\n## Seatunnel with ClickHouse\\n\\n\u63a5\u4e0b\u6765\u4f1a\u7ed9\u5927\u5bb6\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u6211\u4eec\u5982\u4f55\u901a\u8fc7 Seatunnel \u6ee1\u8db3\u4e0a\u8ff0\u9700\u6c42\uff0c\u5c06 HDFS \u4e2d\u7684\u6570\u636e\u5199\u5165 ClickHouse \u4e2d\u3002\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) \u662f\u4e00\u4e2a\u975e\u5e38\u6613\u7528\uff0c\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6d77\u91cf\u6570\u636e\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u4ea7\u54c1\uff0c\u5b83\u6784\u5efa\u5728Spark\u4e4b\u4e0a\u3002Seatunnel \u62e5\u6709\u7740\u975e\u5e38\u4e30\u5bcc\u7684\u63d2\u4ef6\uff0c\u652f\u6301\u4ece Kafka\u3001HDFS\u3001Kudu \u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u8fdb\u884c\u5404\u79cd\u5404\u6837\u7684\u6570\u636e\u5904\u7406\uff0c\u5e76\u5c06\u7ed3\u679c\u5199\u5165 ClickHouse\u3001Elasticsearch \u6216\u8005 Kafka \u4e2d\u3002\\n\\n### Prerequisites\\n\\n\u9996\u5148\u6211\u4eec\u9700\u8981\u5b89\u88c5 Seatunnel\uff0c\u5b89\u88c5\u5341\u5206\u7b80\u5355\uff0c\u65e0\u9700\u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\\n\\n1. \u51c6\u5907 Spark \u73af\u5883\\n2. \u5b89\u88c5 Seatunnel\\n3. \u914d\u7f6e Seatunnel\\n\\n\u4ee5\u4e0b\u662f\u7b80\u6613\u6b65\u9aa4\uff0c\u5177\u4f53\u5b89\u88c5\u53ef\u4ee5\u53c2\u7167 [Quick Start](/docs/quick-start)\\n\\n```shell\\ncd /usr/local\\n\\nwget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\n\\nwget https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip\\n\\nunzip seatunnel-1.1.1.zip\\n\\ncd seatunnel-1.1.1\\nvim config/seatunnel-env.sh\\n\\n# \u6307\u5b9aSpark\u5b89\u88c5\u8def\u5f84\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}\\n```\\n\\n### seatunnel Pipeline\\n\\n\u6211\u4eec\u4ec5\u9700\u8981\u7f16\u5199\u4e00\u4e2a seatunnel Pipeline \u7684\u914d\u7f6e\u6587\u4ef6\u5373\u53ef\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\u3002\\n\\n\u914d\u7f6e\u6587\u4ef6\u5305\u62ec\u56db\u4e2a\u90e8\u5206\uff0c\u5206\u522b\u662f Spark\u3001Input\u3001filter \u548c Output\u3002\\n\\n#### Spark\\n\\n\u8fd9\u4e00\u90e8\u5206\u662f Spark \u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u914d\u7f6e Spark \u6267\u884c\u65f6\u6240\u9700\u7684\u8d44\u6e90\u5927\u5c0f\u3002\\n\\n```shell\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n```\\n\\n#### Input\\n\\n\u8fd9\u4e00\u90e8\u5206\u5b9a\u4e49\u6570\u636e\u6e90\uff0c\u5982\u4e0b\u662f\u4ece HDFS \u6587\u4ef6\u4e2d\u8bfb\u53d6 text \u683c\u5f0f\u6570\u636e\u7684\u914d\u7f6e\u6848\u4f8b\u3002\\n\\n```shell\\ninput {\\n    hdfs {\\n        path = \\"hdfs://nomanode:8020/rowlog/accesslog\\"\\n        table_name = \\"access_log\\"\\n        format = \\"text\\"\\n    }\\n}\\n```\\n\\n#### Filter\\n\\n\u5728 Filter \u90e8\u5206\uff0c\u8fd9\u91cc\u6211\u4eec\u914d\u7f6e\u4e00\u7cfb\u5217\u7684\u8f6c\u5316\uff0c\u5305\u62ec\u6b63\u5219\u89e3\u6790\u5c06\u65e5\u5fd7\u8fdb\u884c\u62c6\u5206\u3001\u65f6\u95f4\u8f6c\u6362\u5c06 HTTPDATE \u8f6c\u5316\u4e3a ClickHouse \u652f\u6301\u7684\u65e5\u671f\u683c\u5f0f\u3001\u5bf9 Number \u7c7b\u578b\u7684\u5b57\u6bb5\u8fdb\u884c\u7c7b\u578b\u8f6c\u6362\u4ee5\u53ca\u901a\u8fc7 SQL \u8fdb\u884c\u5b57\u6bb5\u7b5b\u51cf\u7b49\\n\\n```shell\\nfilter {\\n    # \u4f7f\u7528\u6b63\u5219\u89e3\u6790\u539f\u59cb\u65e5\u5fd7\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{IP:ha_ip}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\\\\\\"\\\\\\\\s%{DATA:uid}\\\\\\\\s%{DATA:session_id}\\\\\\\\s\\\\\\"%{DATA:pool}\\\\\\"\\\\\\\\s\\\\\\"%{DATA:tag2}\\\\\\"\\\\\\\\s%{DATA:tag3}\\\\\\\\s%{DATA:tag4}\'\\n    }\\n\\n    # \u5c06\\"dd/MMM/yyyy:HH:mm:ss Z\\"\u683c\u5f0f\u7684\u6570\u636e\u8f6c\u6362\u4e3a\\n    # \\"yyyy/MM/dd HH:mm:ss\\"\u683c\u5f0f\u7684\u6570\u636e\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy/MM/dd HH:mm:ss\\"\\n    }\\n\\n    # \u4f7f\u7528SQL\u7b5b\u9009\u5173\u6ce8\u7684\u5b57\u6bb5\uff0c\u5e76\u5bf9\u5b57\u6bb5\u8fdb\u884c\u5904\u7406\\n    # \u751a\u81f3\u53ef\u4ee5\u901a\u8fc7\u8fc7\u6ee4\u6761\u4ef6\u8fc7\u6ee4\u6389\u4e0d\u5173\u5fc3\u7684\u6570\u636e\\n    sql {\\n        table_name = \\"access\\"\\n        sql = \\"select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access\\"\\n    }\\n}\\n```\\n\\n#### Output\\n\\n\u6700\u540e\u6211\u4eec\u5c06\u5904\u7406\u597d\u7684\u7ed3\u6784\u5316\u6570\u636e\u5199\u5165 ClickHouse\\n\\n```shell\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"access_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"uri\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\n### Running seatunnel\\n\\n\u6211\u4eec\u5c06\u4e0a\u8ff0\u56db\u90e8\u5206\u914d\u7f6e\u7ec4\u5408\u6210\u4e3a\u6211\u4eec\u7684\u914d\u7f6e\u6587\u4ef6 `config/batch.conf`\u3002\\n\\n```shell\\nvim config/batch.conf\\n```\\n\\n```shell\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n\\ninput {\\n    hdfs {\\n        path = \\"hdfs://nomanode:8020/rowlog/accesslog\\"\\n        table_name = \\"access_log\\"\\n        format = \\"text\\"\\n    }\\n}\\n\\nfilter {\\n    # \u4f7f\u7528\u6b63\u5219\u89e3\u6790\u539f\u59cb\u65e5\u5fd7\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{IP:ha_ip}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\\\\\\"\\\\\\\\s%{DATA:uid}\\\\\\\\s%{DATA:session_id}\\\\\\\\s\\\\\\"%{DATA:pool}\\\\\\"\\\\\\\\s\\\\\\"%{DATA:tag2}\\\\\\"\\\\\\\\s%{DATA:tag3}\\\\\\\\s%{DATA:tag4}\'\\n    }\\n\\n    # \u5c06\\"dd/MMM/yyyy:HH:mm:ss Z\\"\u683c\u5f0f\u7684\u6570\u636e\u8f6c\u6362\u4e3a\\n    # \\"yyyy/MM/dd HH:mm:ss\\"\u683c\u5f0f\u7684\u6570\u636e\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy/MM/dd HH:mm:ss\\"\\n    }\\n\\n    # \u4f7f\u7528SQL\u7b5b\u9009\u5173\u6ce8\u7684\u5b57\u6bb5\uff0c\u5e76\u5bf9\u5b57\u6bb5\u8fdb\u884c\u5904\u7406\\n    # \u751a\u81f3\u53ef\u4ee5\u901a\u8fc7\u8fc7\u6ee4\u6761\u4ef6\u8fc7\u6ee4\u6389\u4e0d\u5173\u5fc3\u7684\u6570\u636e\\n    sql {\\n        table_name = \\"access\\"\\n        sql = \\"select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access\\"\\n    }\\n}\\n\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"access_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"uri\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\n\u6267\u884c\u547d\u4ee4\uff0c\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd0\u884c Seatunnel\uff0c\u5373\u53ef\u5c06\u6570\u636e\u5199\u5165 ClickHouse\u3002\u8fd9\u91cc\u6211\u4eec\u4ee5\u672c\u5730\u6a21\u5f0f\u4e3a\u4f8b\u3002\\n\\n```shell\\n./bin/start-seatunnel.sh --config config/batch.conf -e client -m \'local[2]\'\\n```\\n\\n## Conclusion\\n\\n\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528 Seatunnel \u5c06 HDFS \u4e2d\u7684 Nginx \u65e5\u5fd7\u6587\u4ef6\u5bfc\u5165 ClickHouse \u4e2d\u3002\u4ec5\u901a\u8fc7\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6\u4fbf\u53ef\u5feb\u901f\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\uff0c\u65e0\u9700\u7f16\u5199\u4efb\u4f55\u4ee3\u7801\u3002\u9664\u4e86\u652f\u6301 HDFS \u6570\u636e\u6e90\u4e4b\u5916\uff0cSeatunnel \u540c\u6837\u652f\u6301\u5c06\u6570\u636e\u4ece Kafka \u4e2d\u5b9e\u65f6\u8bfb\u53d6\u5904\u7406\u5199\u5165 ClickHouse \u4e2d\u3002\u6211\u4eec\u7684\u4e0b\u4e00\u7bc7\u6587\u7ae0\u5c06\u4f1a\u4ecb\u7ecd\uff0c\u5982\u4f55\u5c06 Hive \u4e2d\u7684\u6570\u636e\u5feb\u901f\u5bfc\u5165 ClickHouse \u4e2d\u3002\\n\\n\u5f53\u7136\uff0cSeatunnel \u4e0d\u4ec5\u4ec5\u662f ClickHouse \u6570\u636e\u5199\u5165\u7684\u5de5\u5177\uff0c\u5728 Elasticsearch \u4ee5\u53ca Kafka\u7b49 \u6570\u636e\u6e90\u7684\u5199\u5165\u4e0a\u540c\u6837\u53ef\u4ee5\u626e\u6f14\u76f8\u5f53\u91cd\u8981\u7684\u89d2\u8272\u3002\\n\\n\u5e0c\u671b\u4e86\u89e3 Seatunnel \u548c ClickHouse\u3001Elasticsearch\u3001Kafka \u7ed3\u5408\u4f7f\u7528\u7684\u66f4\u591a\u529f\u80fd\u548c\u6848\u4f8b\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fdb\u5165\u5b98\u7f51 [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n-- Power by [InterestingLab](https://github.com/InterestingLab)"},{"id":"hive-to-clickhouse","metadata":{"permalink":"/zh-CN/blog/hive-to-clickhouse","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2021-12-30-hive-to-clickhouse.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-hive-to-clickhouse.md","title":"\u5982\u4f55\u5feb\u901f\u5730\u628a Hive \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","description":"ClickHouse\u662f\u9762\u5411OLAP\u7684\u5206\u5e03\u5f0f\u5217\u5f0fDBMS\u3002\u6211\u4eec\u90e8\u95e8\u76ee\u524d\u5df2\u7ecf\u628a\u6240\u6709\u6570\u636e\u5206\u6790\u76f8\u5173\u7684\u65e5\u5fd7\u6570\u636e\u5b58\u50a8\u81f3ClickHouse\u8fd9\u4e2a\u4f18\u79c0\u7684\u6570\u636e\u4ed3\u5e93\u4e4b\u4e2d\uff0c\u5f53\u524d\u65e5\u6570\u636e\u91cf\u8fbe\u5230\u4e86300\u4ebf\u3002","date":"2021-12-30T00:00:00.000Z","formattedDate":"2021\u5e7412\u670830\u65e5","tags":[{"label":"Hive","permalink":"/zh-CN/blog/tags/hive"},{"label":"ClickHouse","permalink":"/zh-CN/blog/tags/click-house"}],"readingTime":5.525,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"hive-to-clickhouse","title":"\u5982\u4f55\u5feb\u901f\u5730\u628a Hive \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","tags":["Hive","ClickHouse"]},"prevItem":{"title":"\u5982\u4f55\u5feb\u901f\u5730\u628a HDFS \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","permalink":"/zh-CN/blog/hdfs-to-clickhouse"},"nextItem":{"title":"\u5982\u4f55\u4f7f\u7528 Spark \u5feb\u901f\u5c06\u6570\u636e\u5199\u5165 Elasticsearch","permalink":"/zh-CN/blog/spark-execute-elasticsearch"}},"content":"ClickHouse\u662f\u9762\u5411OLAP\u7684\u5206\u5e03\u5f0f\u5217\u5f0fDBMS\u3002\u6211\u4eec\u90e8\u95e8\u76ee\u524d\u5df2\u7ecf\u628a\u6240\u6709\u6570\u636e\u5206\u6790\u76f8\u5173\u7684\u65e5\u5fd7\u6570\u636e\u5b58\u50a8\u81f3ClickHouse\u8fd9\u4e2a\u4f18\u79c0\u7684\u6570\u636e\u4ed3\u5e93\u4e4b\u4e2d\uff0c\u5f53\u524d\u65e5\u6570\u636e\u91cf\u8fbe\u5230\u4e86300\u4ebf\u3002\\n\\n\u5728\u4e4b\u524d\u7684\u6587\u7ae0 [\u5982\u4f55\u5feb\u901f\u5730\u628aHDFS\u4e2d\u7684\u6570\u636e\u5bfc\u5165ClickHouse](i18n/zh-CN/docusaurus-plugin-content-blog/current/2021-12-30-hdfs-to-clickhouse.mdtent-blog/current/2021-12-30-hdfs-to-clickhouse.md) \u4e2d\u6211\u4eec\u63d0\u5230\u8fc7\u4f7f\u7528 Seatunnel [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel) \u5bf9HDFS\u4e2d\u7684\u6570\u636e\u7ecf\u8fc7\u5f88\u7b80\u5355\u7684\u64cd\u4f5c\u5c31\u53ef\u4ee5\u5c06\u6570\u636e\u5199\u5165ClickHouse\u3002HDFS\u4e2d\u7684\u6570\u636e\u4e00\u822c\u662f\u975e\u7ed3\u6784\u5316\u7684\u6570\u636e\uff0c\u90a3\u4e48\u9488\u5bf9\u5b58\u50a8\u5728Hive\u4e2d\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u6211\u4eec\u5e94\u8be5\u600e\u4e48\u64cd\u4f5c\u5462\uff1f\\n\\n![](/doc/image_zh/hive-logo.png)\\n\\n## Hive to ClickHouse\\n\\n\u5047\u5b9a\u6211\u4eec\u7684\u6570\u636e\u5df2\u7ecf\u5b58\u50a8\u5728Hive\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u8bfb\u53d6Hive\u8868\u4e2d\u7684\u6570\u636e\u5e76\u7b5b\u9009\u51fa\u6211\u4eec\u5173\u5fc3\u7684\u5b57\u6bb5\uff0c\u6216\u8005\u5bf9\u5b57\u6bb5\u8fdb\u884c\u8f6c\u6362\uff0c\u6700\u540e\u5c06\u5bf9\u5e94\u7684\u5b57\u6bb5\u5199\u5165ClickHouse\u7684\u8868\u4e2d\u3002\\n\\n### Hive Schema\\n\\n\u6211\u4eec\u5728Hive\u4e2d\u5b58\u50a8\u7684\u6570\u636e\u8868\u7ed3\u6784\u5982\u4e0b\uff0c\u5b58\u50a8\u7684\u662f\u5f88\u5e38\u89c1\u7684Nginx\u65e5\u5fd7\\n\\n```\\nCREATE TABLE `nginx_msg_detail`(\\n   `hostname` string,\\n   `domain` string,\\n   `remote_addr` string,\\n   `request_time` float,\\n   `datetime` string,\\n   `url` string,\\n   `status` int,\\n   `data_size` int,\\n   `referer` string,\\n   `cookie_info` string,\\n   `user_agent` string,\\n   `minute` string)\\n PARTITIONED BY (\\n   `date` string,\\n   `hour` string)\\n\\n```\\n\\n### ClickHouse Schema\\n\\n\u6211\u4eec\u7684ClickHouse\u5efa\u8868\u8bed\u53e5\u5982\u4e0b\uff0c\u6211\u4eec\u7684\u8868\u6309\u65e5\u8fdb\u884c\u5206\u533a\\n\\n```\\nCREATE TABLE cms.cms_msg\\n(\\n    date Date,\\n    datetime DateTime,\\n    url String,\\n    request_time Float32,\\n    status String,\\n    hostname String,\\n    domain String,\\n    remote_addr String,\\n    data_size Int32\\n) ENGINE = MergeTree PARTITION BY date ORDER BY (date, hostname) SETTINGS index_granularity = 16384\\n```\\n\\n## Seatunnel with ClickHouse\\n\\n\u63a5\u4e0b\u6765\u4f1a\u7ed9\u5927\u5bb6\u4ecb\u7ecd\uff0c\u6211\u4eec\u5982\u4f55\u901a\u8fc7 Seatunnel \u5c06Hive\u4e2d\u7684\u6570\u636e\u5199\u5165ClickHouse\u4e2d\u3002\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) \u662f\u4e00\u4e2a\u975e\u5e38\u6613\u7528\uff0c\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6d77\u91cf\u6570\u636e\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u4ea7\u54c1\uff0c\u5b83\u6784\u5efa\u5728Spark\u4e4b\u4e0a\u3002Seatunnel \u62e5\u6709\u7740\u975e\u5e38\u4e30\u5bcc\u7684\u63d2\u4ef6\uff0c\u652f\u6301\u4eceKafka\u3001HDFS\u3001Kudu\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u8fdb\u884c\u5404\u79cd\u5404\u6837\u7684\u6570\u636e\u5904\u7406\uff0c\u5e76\u5c06\u7ed3\u679c\u5199\u5165ClickHouse\u3001Elasticsearch\u6216\u8005Kafka\u4e2d\u3002\\n\\nSeatunnel\u7684\u73af\u5883\u51c6\u5907\u4ee5\u53ca\u5b89\u88c5\u6b65\u9aa4\u8fd9\u91cc\u5c31\u4e0d\u4e00\u4e00\u8d58\u8ff0\u4e86\uff0c\u5177\u4f53\u5b89\u88c5\u6b65\u9aa4\u53ef\u4ee5\u53c2\u8003\u4e0a\u4e00\u7bc7\u6587\u7ae0\u6216\u8005\u8bbf\u95ee [Seatunnel Docs](/docs/intro/about)\\n\\n### Seatunnel Pipeline\\n\\n\u6211\u4eec\u4ec5\u9700\u8981\u7f16\u5199\u4e00\u4e2aSeatunnel Pipeline\u7684\u914d\u7f6e\u6587\u4ef6\u5373\u53ef\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\u3002\\n\\n\u914d\u7f6e\u6587\u4ef6\u5305\u62ec\u56db\u4e2a\u90e8\u5206\uff0c\u5206\u522b\u662fSpark\u3001Input\u3001filter\u548cOutput\u3002\\n\\n#### Spark\\n\\n\\n\u8fd9\u4e00\u90e8\u5206\u662fSpark\u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u914d\u7f6eSpark\u6267\u884c\u65f6\u6240\u9700\u7684\u8d44\u6e90\u5927\u5c0f\u3002\\n```\\nspark {\\n  // \u8fd9\u4e2a\u914d\u7f6e\u5fc5\u9700\u586b\u5199\\n  spark.sql.catalogImplementation = \\"hive\\"\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n```\\n\\n#### Input\\n\\n\u8fd9\u4e00\u90e8\u5206\u5b9a\u4e49\u6570\u636e\u6e90\uff0c\u5982\u4e0b\u662f\u4eceHive\u6587\u4ef6\u4e2d\u8bfb\u53d6text\u683c\u5f0f\u6570\u636e\u7684\u914d\u7f6e\u6848\u4f8b\u3002\\n\\n```\\ninput {\\n    hive {\\n        pre_sql = \\"select * from access.nginx_msg_detail\\"\\n        table_name = \\"access_log\\"\\n    }\\n}\\n```\\n\\n\u770b\uff0c\u5f88\u7b80\u5355\u7684\u4e00\u4e2a\u914d\u7f6e\u5c31\u53ef\u4ee5\u4eceHive\u4e2d\u8bfb\u53d6\u6570\u636e\u4e86\u3002\u5176\u4e2d`pre_sql`\u662f\u4eceHive\u4e2d\u8bfb\u53d6\u6570\u636eSQL\uff0c`table_name`\u662f\u5c06\u8bfb\u53d6\u540e\u7684\u6570\u636e\uff0c\u6ce8\u518c\u6210\u4e3aSpark\u4e2d\u4e34\u65f6\u8868\u7684\u8868\u540d\uff0c\u53ef\u4e3a\u4efb\u610f\u5b57\u6bb5\u3002\\n\\n\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u5fc5\u987b\u4fdd\u8bc1hive\u7684metastore\u662f\u5728\u670d\u52a1\u72b6\u6001\u3002\\n\\n\u5728Cluster\u3001Client\u3001Local\u6a21\u5f0f\u4e0b\u8fd0\u884c\u65f6\uff0c\u5fc5\u987b\u628a`hive-site.xml`\u6587\u4ef6\u7f6e\u4e8e\u63d0\u4ea4\u4efb\u52a1\u8282\u70b9\u7684$HADOOP_CONF\u76ee\u5f55\u4e0b\\n\\n#### Filter\\n\\n\u5728Filter\u90e8\u5206\uff0c\u8fd9\u91cc\u6211\u4eec\u914d\u7f6e\u4e00\u7cfb\u5217\u7684\u8f6c\u5316\uff0c\u6211\u4eec\u8fd9\u91cc\u628a\u4e0d\u9700\u8981\u7684minute\u548chour\u5b57\u6bb5\u4e22\u5f03\u3002\u5f53\u7136\u6211\u4eec\u4e5f\u53ef\u4ee5\u5728\u8bfb\u53d6Hive\u7684\u65f6\u5019\u901a\u8fc7`pre_sql`\u4e0d\u8bfb\u53d6\u8fd9\u4e9b\u5b57\u6bb5\\n\\n```\\nfilter {\\n    remove {\\n        source_field = [\\"minute\\", \\"hour\\"]\\n    }\\n}\\n```\\n\\n#### Output\\n\u6700\u540e\u6211\u4eec\u5c06\u5904\u7406\u597d\u7684\u7ed3\u6784\u5316\u6570\u636e\u5199\u5165ClickHouse\\n\\n```\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"nginx_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"url\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\n### Running Seatunnel\\n\\n\u6211\u4eec\u5c06\u4e0a\u8ff0\u56db\u90e8\u5206\u914d\u7f6e\u7ec4\u5408\u6210\u4e3a\u6211\u4eec\u7684\u914d\u7f6e\u6587\u4ef6`config/batch.conf`\u3002\\n\\n    vim config/batch.conf\\n\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n  // \u8fd9\u4e2a\u914d\u7f6e\u5fc5\u9700\u586b\u5199\\n  spark.sql.catalogImplementation = \\"hive\\"\\n}\\ninput {\\n    hive {\\n        pre_sql = \\"select * from access.nginx_msg_detail\\"\\n        table_name = \\"access_log\\"\\n    }\\n}\\nfilter {\\n    remove {\\n        source_field = [\\"minute\\", \\"hour\\"]\\n    }\\n}\\noutput {\\n    clickhouse {\\n        host = \\"your.clickhouse.host:8123\\"\\n        database = \\"seatunnel\\"\\n        table = \\"access_log\\"\\n        fields = [\\"date\\", \\"datetime\\", \\"hostname\\", \\"uri\\", \\"http_code\\", \\"request_time\\", \\"data_size\\", \\"domain\\"]\\n        username = \\"username\\"\\n        password = \\"password\\"\\n    }\\n}\\n```\\n\\n\u6267\u884c\u547d\u4ee4\uff0c\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd0\u884c Seatunnel\uff0c\u5373\u53ef\u5c06\u6570\u636e\u5199\u5165ClickHouse\u3002\u8fd9\u91cc\u6211\u4eec\u4ee5\u672c\u5730\u6a21\u5f0f\u4e3a\u4f8b\u3002\\n\\n    ./bin/start-seatunnel.sh --config config/batch.conf -e client -m \'local[2]\'\\n\\n\\n## Conclusion\\n\\n\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528 Seatunnel \u5c06Hive\u4e2d\u7684\u6570\u636e\u5bfc\u5165ClickHouse\u4e2d\u3002\u4ec5\u4ec5\u901a\u8fc7\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6\u4fbf\u53ef\u5feb\u901f\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\uff0c\u65e0\u9700\u7f16\u5199\u4efb\u4f55\u4ee3\u7801\uff0c\u5341\u5206\u7b80\u5355\u3002\\n\\n\u5e0c\u671b\u4e86\u89e3 Seatunnel \u4e0eClickHouse\u3001Elasticsearch\u3001Kafka\u3001Hadoop\u7ed3\u5408\u4f7f\u7528\u7684\u66f4\u591a\u529f\u80fd\u548c\u6848\u4f8b\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fdb\u5165\u5b98\u7f51 [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n-- Power by [InterestingLab](https://github.com/InterestingLab)"},{"id":"spark-execute-elasticsearch","metadata":{"permalink":"/zh-CN/blog/spark-execute-elasticsearch","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2021-12-30-spark-execute-elasticsearch.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-spark-execute-elasticsearch.md","title":"\u5982\u4f55\u4f7f\u7528 Spark \u5feb\u901f\u5c06\u6570\u636e\u5199\u5165 Elasticsearch","description":"\u8bf4\u5230\u6570\u636e\u5199\u5165 Elasticsearch\uff0c\u6700\u5148\u60f3\u5230\u7684\u80af\u5b9a\u662fLogstash\u3002Logstash\u56e0\u4e3a\u5176\u7b80\u5355\u4e0a\u624b\u3001\u53ef\u6269\u5c55\u3001\u53ef\u4f38\u7f29\u7b49\u4f18\u70b9\u88ab\u5e7f\u5927\u7528\u6237\u63a5\u53d7\u3002\u4f46\u662f\u5c3a\u6709\u6240\u77ed\uff0c\u5bf8\u6709\u6240\u957f\uff0cLogstash\u80af\u5b9a\u4e5f\u6709\u5b83\u65e0\u6cd5\u9002\u7528\u7684\u5e94\u7528\u573a\u666f\uff0c\u6bd4\u5982\uff1a","date":"2021-12-30T00:00:00.000Z","formattedDate":"2021\u5e7412\u670830\u65e5","tags":[{"label":"Spark","permalink":"/zh-CN/blog/tags/spark"},{"label":"Kafka","permalink":"/zh-CN/blog/tags/kafka"},{"label":"Elasticsearch","permalink":"/zh-CN/blog/tags/elasticsearch"}],"readingTime":7.415,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"spark-execute-elasticsearch","title":"\u5982\u4f55\u4f7f\u7528 Spark \u5feb\u901f\u5c06\u6570\u636e\u5199\u5165 Elasticsearch","tags":["Spark","Kafka","Elasticsearch"]},"prevItem":{"title":"\u5982\u4f55\u5feb\u901f\u5730\u628a Hive \u4e2d\u7684\u6570\u636e\u5bfc\u5165 ClickHouse","permalink":"/zh-CN/blog/hive-to-clickhouse"},"nextItem":{"title":"\u600e\u4e48\u7528 Spark \u5728 TiDB \u4e0a\u505a OLAP \u5206\u6790","permalink":"/zh-CN/blog/spark-execute-tidb"}},"content":"\u8bf4\u5230\u6570\u636e\u5199\u5165 Elasticsearch\uff0c\u6700\u5148\u60f3\u5230\u7684\u80af\u5b9a\u662fLogstash\u3002Logstash\u56e0\u4e3a\u5176\u7b80\u5355\u4e0a\u624b\u3001\u53ef\u6269\u5c55\u3001\u53ef\u4f38\u7f29\u7b49\u4f18\u70b9\u88ab\u5e7f\u5927\u7528\u6237\u63a5\u53d7\u3002\u4f46\u662f\u5c3a\u6709\u6240\u77ed\uff0c\u5bf8\u6709\u6240\u957f\uff0cLogstash\u80af\u5b9a\u4e5f\u6709\u5b83\u65e0\u6cd5\u9002\u7528\u7684\u5e94\u7528\u573a\u666f\uff0c\u6bd4\u5982\uff1a\\n\\n* \u6d77\u91cf\u6570\u636eETL\\n* \u6d77\u91cf\u6570\u636e\u805a\u5408\\n* \u591a\u6e90\u6570\u636e\u5904\u7406\\n\\n\u4e3a\u4e86\u6ee1\u8db3\u8fd9\u4e9b\u573a\u666f\uff0c\u5f88\u591a\u540c\u5b66\u90fd\u4f1a\u9009\u62e9Spark\uff0c\u501f\u52a9Spark\u7b97\u5b50\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u6700\u540e\u5c06\u5904\u7406\u7ed3\u679c\u5199\u5165Elasticsearch\u3002\\n\\n\u6211\u4eec\u90e8\u95e8\u4e4b\u524d\u5229\u7528Spark\u5bf9Nginx\u65e5\u5fd7\u8fdb\u884c\u5206\u6790\uff0c\u7edf\u8ba1\u6211\u4eec\u7684Web\u670d\u52a1\u8bbf\u95ee\u60c5\u51b5\uff0c\u5c06Nginx\u65e5\u5fd7\u6bcf\u5206\u949f\u805a\u5408\u4e00\u6b21\u6700\u540e\u5c06\u7ed3\u679c\u5199\u5165Elasticsearch\uff0c\u7136\u540e\u5229\u7528Kibana\u914d\u7f6e\u5b9e\u65f6\u76d1\u63a7Dashboard\u3002Elasticsearch\u548cKibana\u90fd\u5f88\u65b9\u4fbf\u3001\u5b9e\u7528\uff0c\u4f46\u662f\u968f\u7740\u7c7b\u4f3c\u9700\u6c42\u8d8a\u6765\u8d8a\u591a\uff0c\u5982\u4f55\u5feb\u901f\u901a\u8fc7Spark\u5c06\u6570\u636e\u5199\u5165Elasticsearch\u6210\u4e3a\u4e86\u6211\u4eec\u7684\u4e00\u5927\u95ee\u9898\u3002\\n\\n\u4eca\u5929\u7ed9\u5927\u5bb6\u63a8\u8350\u4e00\u6b3e\u80fd\u591f\u5b9e\u73b0\u6570\u636e\u5feb\u901f\u5199\u5165\u7684\u9ed1\u79d1\u6280 Seatunnel [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel) \u4e00\u4e2a\u975e\u5e38\u6613\u7528\uff0c\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6d77\u91cf\u6570\u636e\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u4ea7\u54c1\uff0c\u5b83\u6784\u5efa\u5728Spark\u4e4b\u4e0a\uff0c\u7b80\u5355\u6613\u7528\uff0c\u7075\u6d3b\u914d\u7f6e\uff0c\u65e0\u9700\u5f00\u53d1\u3002\\n\\n![](/doc/image_zh/wd-struct.png)\\n\\n\\n## Kafka to Elasticsearch\\n\\n\u548cLogstash\u4e00\u6837\uff0cSeatunnel\u540c\u6837\u652f\u6301\u591a\u79cd\u7c7b\u578b\u7684\u6570\u636e\u8f93\u5165\uff0c\u8fd9\u91cc\u6211\u4eec\u4ee5\u6700\u5e38\u89c1\u7684Kakfa\u4f5c\u4e3a\u8f93\u5165\u6e90\u4e3a\u4f8b\uff0c\u8bb2\u89e3\u5982\u4f55\u4f7f\u7528 Seatunnel \u5c06\u6570\u636e\u5feb\u901f\u5199\u5165Elasticsearch\\n\\n### Log Sample\\n\\n\u539f\u59cb\u65e5\u5fd7\u683c\u5f0f\u5982\u4e0b:\\n```\\n127.0.0.1 elasticsearch.cn 114.250.140.241 0.001s \\"127.0.0.1:80\\" [26/Oct/2018:21:54:32 +0800] \\"GET /article HTTP/1.1\\" 200 123 \\"-\\" - \\"Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)\\"\\n```\\n\\n### Elasticsearch Document\\n\\n\u6211\u4eec\u60f3\u8981\u7edf\u8ba1\uff0c\u4e00\u5206\u949f\u6bcf\u4e2a\u57df\u540d\u7684\u8bbf\u95ee\u60c5\u51b5\uff0c\u805a\u5408\u5b8c\u7684\u6570\u636e\u6709\u4ee5\u4e0b\u5b57\u6bb5:\\n```\\ndomain String\\nhostname String\\nstatus int\\ndatetime String\\ncount int\\n```\\n\\n## Seatunnel with Elasticsearch\\n\\n\u63a5\u4e0b\u6765\u4f1a\u7ed9\u5927\u5bb6\u8be6\u7ec6\u4ecb\u7ecd\uff0c\u6211\u4eec\u5982\u4f55\u901a\u8fc7 Seatunnel \u8bfb\u53d6Kafka\u4e2d\u7684\u6570\u636e\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u89e3\u6790\u4ee5\u53ca\u805a\u5408\uff0c\u6700\u540e\u5c06\u5904\u7406\u7ed3\u679c\u5199\u5165Elasticsearch\u4e2d\u3002\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) \u540c\u6837\u62e5\u6709\u7740\u975e\u5e38\u4e30\u5bcc\u7684\u63d2\u4ef6\uff0c\u652f\u6301\u4eceKafka\u3001HDFS\u3001Hive\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u8fdb\u884c\u5404\u79cd\u5404\u6837\u7684\u6570\u636e\u5904\u7406\uff0c\u5e76\u5c06\u7ed3\u679c\u5199\u5165Elasticsearch\u3001Kudu\u6216\u8005Kafka\u4e2d\u3002\\n\\n### Prerequisites\\n\\n\u9996\u5148\u6211\u4eec\u9700\u8981\u5b89\u88c5seatunnel\uff0c\u5b89\u88c5\u5341\u5206\u7b80\u5355\uff0c\u65e0\u9700\u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\\n1. \u51c6\u5907Spark\u73af\u5883\\n2. \u5b89\u88c5 Seatunnel\\n3. \u914d\u7f6e Seatunnel\\n\\n\u4ee5\u4e0b\u662f\u7b80\u6613\u6b65\u9aa4\uff0c\u5177\u4f53\u5b89\u88c5\u53ef\u4ee5\u53c2\u7167 [Quick Start](/docs/quick-start)\\n\\n```yaml\\ncd /usr/local\\nwget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\nwget https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip\\nunzip seatunnel-1.1.1.zip\\ncd seatunnel-1.1.1\\n\\nvim config/seatunnel-env.sh\\n# \u6307\u5b9aSpark\u5b89\u88c5\u8def\u5f84\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}\\n```\\n\\n### Seatunnel Pipeline\\n\\n\u4e0eLogstash\u4e00\u6837\uff0c\u6211\u4eec\u4ec5\u9700\u8981\u7f16\u5199\u4e00\u4e2aSeatunnel Pipeline\u7684\u914d\u7f6e\u6587\u4ef6\u5373\u53ef\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\uff0c\u76f8\u4fe1\u4e86\u89e3Logstash\u7684\u670b\u53cb\u53ef\u4ee5\u5f88\u5feb\u5165\u624b Seatunnel \u914d\u7f6e\u3002\\n\\n\u914d\u7f6e\u6587\u4ef6\u5305\u62ec\u56db\u4e2a\u90e8\u5206\uff0c\u5206\u522b\u662fSpark\u3001Input\u3001filter\u548cOutput\u3002\\n\\n#### Spark\\n\\n\\n\u8fd9\u4e00\u90e8\u5206\u662fSpark\u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u914d\u7f6eSpark\u6267\u884c\u65f6\u6240\u9700\u7684\u8d44\u6e90\u5927\u5c0f\u3002\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n  spark.streaming.batchDuration = 5\\n}\\n```\\n\\n#### Input\\n\\n\u8fd9\u4e00\u90e8\u5206\u5b9a\u4e49\u6570\u636e\u6e90\uff0c\u5982\u4e0b\u662f\u4eceKafka\u4e2d\u8bfb\u53d6\u6570\u636e\u7684\u914d\u7f6e\u6848\u4f8b\uff0c\\n\\n```\\nkafkaStream {\\n    topics = \\"seatunnel-es\\"\\n    consumer.bootstrap.servers = \\"localhost:9092\\"\\n    consumer.group.id = \\"seatunnel_es_group\\"\\n    consumer.rebalance.max.retries = 100\\n}\\n```\\n\\n#### Filter\\n\\n\u5728Filter\u90e8\u5206\uff0c\u8fd9\u91cc\u6211\u4eec\u914d\u7f6e\u4e00\u7cfb\u5217\u7684\u8f6c\u5316\uff0c\u5305\u62ec\u6b63\u5219\u89e3\u6790\u5c06\u65e5\u5fd7\u8fdb\u884c\u62c6\u5206\u3001\u65f6\u95f4\u8f6c\u6362\u5c06HTTPDATE\u8f6c\u5316\u4e3aElasticsearch\u652f\u6301\u7684\u65e5\u671f\u683c\u5f0f\u3001\u5bf9Number\u7c7b\u578b\u7684\u5b57\u6bb5\u8fdb\u884c\u7c7b\u578b\u8f6c\u6362\u4ee5\u53ca\u901a\u8fc7SQL\u8fdb\u884c\u6570\u636e\u805a\u5408\\n```yaml\\nfilter {\\n    # \u4f7f\u7528\u6b63\u5219\u89e3\u6790\u539f\u59cb\u65e5\u5fd7\\n    # \u6700\u5f00\u59cb\u6570\u636e\u90fd\u5728raw_message\u5b57\u6bb5\u4e2d\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{NOTSPACE:hostname}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\'\\n   }\\n    # \u5c06\\"dd/MMM/yyyy:HH:mm:ss Z\\"\u683c\u5f0f\u7684\u6570\u636e\u8f6c\u6362\u4e3a\\n    # Elasticsearch\u4e2d\u652f\u6301\u7684\u683c\u5f0f\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy-MM-dd\'T\'HH:mm:ss.SSS+08:00\\"\\n    }\\n    ## \u5229\u7528SQL\u5bf9\u6570\u636e\u8fdb\u884c\u805a\u5408\\n    sql {\\n        table_name = \\"access_log\\"\\n        sql = \\"select domain, hostname, int(status), datetime, count(*) from access_log group by domain, hostname, status, datetime\\"\\n    }\\n }\\n```\\n\\n#### Output\\n\u6700\u540e\u6211\u4eec\u5c06\u5904\u7406\u597d\u7684\u7ed3\u6784\u5316\u6570\u636e\u5199\u5165Elasticsearch\u3002\\n\\n```yaml\\noutput {\\n    elasticsearch {\\n        hosts = [\\"localhost:9200\\"]\\n        index = \\"seatunnel-${now}\\"\\n        es.batch.size.entries = 100000\\n        index_time_format = \\"yyyy.MM.dd\\"\\n    }\\n}\\n```\\n\\n### Running Seatunnel\\n\\n\u6211\u4eec\u5c06\u4e0a\u8ff0\u56db\u90e8\u5206\u914d\u7f6e\u7ec4\u5408\u6210\u4e3a\u6211\u4eec\u7684\u914d\u7f6e\u6587\u4ef6 `config/batch.conf`\u3002\\n\\n    vim config/batch.conf\\n\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n  spark.streaming.batchDuration = 5\\n}\\ninput {\\n    kafkaStream {\\n        topics = \\"seatunnel-es\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        consumer.group.id = \\"seatunnel_es_group\\"\\n        consumer.rebalance.max.retries = 100\\n    }\\n}\\nfilter {\\n    # \u4f7f\u7528\u6b63\u5219\u89e3\u6790\u539f\u59cb\u65e5\u5fd7\\n    # \u6700\u5f00\u59cb\u6570\u636e\u90fd\u5728raw_message\u5b57\u6bb5\u4e2d\\n    grok {\\n        source_field = \\"raw_message\\"\\n        pattern = \'%{IP:hostname}\\\\\\\\s%{NOTSPACE:domain}\\\\\\\\s%{IP:remote_addr}\\\\\\\\s%{NUMBER:request_time}s\\\\\\\\s\\\\\\"%{DATA:upstream_ip}\\\\\\"\\\\\\\\s\\\\\\\\[%{HTTPDATE:timestamp}\\\\\\\\]\\\\\\\\s\\\\\\"%{NOTSPACE:method}\\\\\\\\s%{DATA:url}\\\\\\\\s%{NOTSPACE:http_ver}\\\\\\"\\\\\\\\s%{NUMBER:status}\\\\\\\\s%{NUMBER:body_bytes_send}\\\\\\\\s%{DATA:referer}\\\\\\\\s%{NOTSPACE:cookie_info}\\\\\\\\s\\\\\\"%{DATA:user_agent}\'\\n   }\\n    # \u5c06\\"dd/MMM/yyyy:HH:mm:ss Z\\"\u683c\u5f0f\u7684\u6570\u636e\u8f6c\u6362\u4e3a\\n    # Elasticsearch\u4e2d\u652f\u6301\u7684\u683c\u5f0f\\n    date {\\n        source_field = \\"timestamp\\"\\n        target_field = \\"datetime\\"\\n        source_time_format = \\"dd/MMM/yyyy:HH:mm:ss Z\\"\\n        target_time_format = \\"yyyy-MM-dd\'T\'HH:mm:00.SSS+08:00\\"\\n    }\\n    ## \u5229\u7528SQL\u5bf9\u6570\u636e\u8fdb\u884c\u805a\u5408\\n    sql {\\n        table_name = \\"access_log\\"\\n        sql = \\"select domain, hostname, status, datetime, count(*) from access_log group by domain, hostname, status, datetime\\"\\n    }\\n }\\noutput {\\n    elasticsearch {\\n        hosts = [\\"localhost:9200\\"]\\n        index = \\"seatunnel-${now}\\"\\n        es.batch.size.entries = 100000\\n        index_time_format = \\"yyyy.MM.dd\\"\\n    }\\n}\\n```\\n\\n\u6267\u884c\u547d\u4ee4\uff0c\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd0\u884c Seatunnel\uff0c\u5373\u53ef\u5c06\u6570\u636e\u5199\u5165Elasticsearch\u3002\u8fd9\u91cc\u6211\u4eec\u4ee5\u672c\u5730\u6a21\u5f0f\u4e3a\u4f8b\u3002\\n\\n    ./bin/start-seatunnel.sh --config config/batch.conf -e client -m \'local[2]\'\\n\\n\u6700\u540e\uff0c\u5199\u5165Elasticsearch\u4e2d\u7684\u6570\u636e\u5982\u4e0b\uff0c\u518d\u914d\u4e0aKibana\u5c31\u53ef\u4ee5\u5b9e\u73b0Web\u670d\u52a1\u7684\u5b9e\u65f6\u76d1\u63a7\u4e86^_^.\\n\\n```\\n\\"_source\\": {\\n    \\"domain\\": \\"elasticsearch.cn\\",\\n    \\"hostname\\": \\"localhost\\",\\n    \\"status\\": \\"200\\",\\n    \\"datetime\\": \\"2018-11-26T21:54:00.000+08:00\\",\\n    \\"count\\": 26\\n  }\\n```\\n\\n## Conclusion\\n\\n\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7 Seatunnel \u5c06Kafka\u4e2d\u7684\u6570\u636e\u5199\u5165Elasticsearch\u4e2d\u3002\u4ec5\u4ec5\u901a\u8fc7\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6\u4fbf\u53ef\u5feb\u901f\u8fd0\u884c\u4e00\u4e2aSpark Application\uff0c\u5b8c\u6210\u6570\u636e\u7684\u5904\u7406\u3001\u5199\u5165\uff0c\u65e0\u9700\u7f16\u5199\u4efb\u4f55\u4ee3\u7801\uff0c\u5341\u5206\u7b80\u5355\u3002\\n\\n\u5f53\u6570\u636e\u5904\u7406\u8fc7\u7a0b\u4e2d\u6709\u9047\u5230Logstash\u65e0\u6cd5\u652f\u6301\u7684\u573a\u666f\u6216\u8005Logstah\u6027\u80fd\u65e0\u6cd5\u8fbe\u5230\u9884\u671f\u7684\u60c5\u51b5\u4e0b\uff0c\u90fd\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528 Seatunnel \u89e3\u51b3\u95ee\u9898\u3002\\n\\n\u5e0c\u671b\u4e86\u89e3 Seatunnel \u4e0eElasticsearch\u3001Kafka\u3001Hadoop\u7ed3\u5408\u4f7f\u7528\u7684\u66f4\u591a\u529f\u80fd\u548c\u6848\u4f8b\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fdb\u5165\u5b98\u7f51 [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n\\n**\u6211\u4eec\u8fd1\u671f\u4f1a\u518d\u53d1\u5e03\u4e00\u7bc7\u300a\u5982\u4f55\u7528Spark\u548cElasticsearch\u505a\u4ea4\u4e92\u5f0f\u6570\u636e\u5206\u6790\u300b\uff0c\u656c\u8bf7\u671f\u5f85.**\\n\\n## Contract us\\n* \u90ae\u4ef6\u5217\u8868 : **dev@seatunnel.apache.org**. \u53d1\u9001\u4efb\u610f\u5185\u5bb9\u81f3 `dev-subscribe@seatunnel.apache.org`\uff0c \u6309\u7167\u56de\u590d\u8ba2\u9605\u90ae\u4ef6\u5217\u8868\u3002\\n* Slack: \u53d1\u9001 `Request to join SeaTunnel slack` \u90ae\u4ef6\u5230\u90ae\u4ef6\u5217\u8868 (`dev@seatunnel.apache.org`), \u6211\u4eec\u4f1a\u9080\u8bf7\u4f60\u52a0\u5165\uff08\u5728\u6b64\u4e4b\u524d\u8bf7\u786e\u8ba4\u5df2\u7ecf\u6ce8\u518cSlack\uff09.\\n* [bilibili B\u7ad9 \u89c6\u9891](https://space.bilibili.com/1542095008)"},{"id":"spark-execute-tidb","metadata":{"permalink":"/zh-CN/blog/spark-execute-tidb","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2021-12-30-spark-execute-tidb.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-spark-execute-tidb.md","title":"\u600e\u4e48\u7528 Spark \u5728 TiDB \u4e0a\u505a OLAP \u5206\u6790","description":"TiDB \u662f\u4e00\u6b3e\u5b9a\u4f4d\u4e8e\u5728\u7ebf\u4e8b\u52a1\u5904\u7406/\u5728\u7ebf\u5206\u6790\u5904\u7406\u7684\u878d\u5408\u578b\u6570\u636e\u5e93\u4ea7\u54c1\uff0c\u5b9e\u73b0\u4e86\u4e00\u952e\u6c34\u5e73\u4f38\u7f29\uff0c\u5f3a\u4e00\u81f4\u6027\u7684\u591a\u526f\u672c\u6570\u636e\u5b89\u5168\uff0c\u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u5b9e\u65f6 OLAP \u7b49\u91cd\u8981\u7279\u6027\u3002","date":"2021-12-30T00:00:00.000Z","formattedDate":"2021\u5e7412\u670830\u65e5","tags":[{"label":"Spark","permalink":"/zh-CN/blog/tags/spark"},{"label":"TiDB","permalink":"/zh-CN/blog/tags/ti-db"}],"readingTime":8.58,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"spark-execute-tidb","title":"\u600e\u4e48\u7528 Spark \u5728 TiDB \u4e0a\u505a OLAP \u5206\u6790","tags":["Spark","TiDB"]},"prevItem":{"title":"\u5982\u4f55\u4f7f\u7528 Spark \u5feb\u901f\u5c06\u6570\u636e\u5199\u5165 Elasticsearch","permalink":"/zh-CN/blog/spark-execute-elasticsearch"},"nextItem":{"title":"\u5982\u4f55\u652f\u6301\u7684 Spark StructuredStreaming","permalink":"/zh-CN/blog/spark-structured-streaming"}},"content":"![](https://download.pingcap.com/images/tidb-planet.jpg)\\n\\n[TiDB](https://github.com/pingcap/tidb) \u662f\u4e00\u6b3e\u5b9a\u4f4d\u4e8e\u5728\u7ebf\u4e8b\u52a1\u5904\u7406/\u5728\u7ebf\u5206\u6790\u5904\u7406\u7684\u878d\u5408\u578b\u6570\u636e\u5e93\u4ea7\u54c1\uff0c\u5b9e\u73b0\u4e86\u4e00\u952e\u6c34\u5e73\u4f38\u7f29\uff0c\u5f3a\u4e00\u81f4\u6027\u7684\u591a\u526f\u672c\u6570\u636e\u5b89\u5168\uff0c\u5206\u5e03\u5f0f\u4e8b\u52a1\uff0c\u5b9e\u65f6 OLAP \u7b49\u91cd\u8981\u7279\u6027\u3002\\n\\nTiSpark \u662f PingCAP \u4e3a\u89e3\u51b3\u7528\u6237\u590d\u6742 OLAP \u9700\u6c42\u800c\u63a8\u51fa\u7684\u4ea7\u54c1\u3002\u5b83\u501f\u52a9 Spark \u5e73\u53f0\uff0c\u540c\u65f6\u878d\u5408 TiKV \u5206\u5e03\u5f0f\u96c6\u7fa4\u7684\u4f18\u52bf\u3002\\n\\n\u76f4\u63a5\u4f7f\u7528 TiSpark \u5b8c\u6210 OLAP \u64cd\u4f5c\u9700\u8981\u4e86\u89e3 Spark\uff0c\u8fd8\u9700\u8981\u4e00\u4e9b\u5f00\u53d1\u5de5\u4f5c\u3002\u90a3\u4e48\uff0c\u6709\u6ca1\u6709\u4e00\u4e9b\u5f00\u7bb1\u5373\u7528\u7684\u5de5\u5177\u80fd\u5e2e\u6211\u4eec\u66f4\u5feb\u901f\u5730\u4f7f\u7528 TiSpark \u5728 TiDB \u4e0a\u5b8c\u6210 OLAP \u5206\u6790\u5462\uff1f\\n\\n\u76ee\u524d\u5f00\u6e90\u793e\u533a\u4e0a\u6709\u4e00\u6b3e\u5de5\u5177 **Seatunnel**\uff0c\u9879\u76ee\u5730\u5740 [https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel) \uff0c\u53ef\u4ee5\u57fa\u4e8eSpark\uff0c\u5728 TiSpark \u7684\u57fa\u7840\u4e0a\u5feb\u901f\u5b9e\u73b0 TiDB \u6570\u636e\u8bfb\u53d6\u548c OLAP \u5206\u6790\u3002\\n\\n\\n## \u4f7f\u7528 Seatunnel \u64cd\u4f5cTiDB\\n\\n\u5728\u6211\u4eec\u7ebf\u4e0a\u6709\u8fd9\u4e48\u4e00\u4e2a\u9700\u6c42\uff0c\u4ece TiDB \u4e2d\u8bfb\u53d6\u67d0\u4e00\u5929\u7684\u7f51\u7ad9\u8bbf\u95ee\u6570\u636e\uff0c\u7edf\u8ba1\u6bcf\u4e2a\u57df\u540d\u4ee5\u53ca\u670d\u52a1\u8fd4\u56de\u72b6\u6001\u7801\u7684\u8bbf\u95ee\u6b21\u6570\uff0c\u6700\u540e\u5c06\u7edf\u8ba1\u7ed3\u679c\u5199\u5165 TiDB \u53e6\u5916\u4e00\u4e2a\u8868\u4e2d\u3002 \u6211\u4eec\u6765\u770b\u770b Seatunnel \u662f\u5982\u4f55\u5b9e\u73b0\u8fd9\u4e48\u4e00\u4e2a\u529f\u80fd\u7684\u3002\\n\\n### Seatunnel\\n\\n[Seatunnel](https://github.com/apache/incubator-seatunnel) \u662f\u4e00\u4e2a\u975e\u5e38\u6613\u7528\uff0c\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6d77\u91cf\u6570\u636e\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u4ea7\u54c1\uff0c\u5b83\u6784\u5efa\u5728 Spark \u4e4b\u4e0a\u3002Seatunnel \u62e5\u6709\u7740\u975e\u5e38\u4e30\u5bcc\u7684\u63d2\u4ef6\uff0c\u652f\u6301\u4ece TiDB\u3001Kafka\u3001HDFS\u3001Kudu \u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u8fdb\u884c\u5404\u79cd\u5404\u6837\u7684\u6570\u636e\u5904\u7406\uff0c\u7136\u540e\u5c06\u7ed3\u679c\u5199\u5165 TiDB\u3001ClickHouse\u3001Elasticsearch \u6216\u8005 Kafka \u4e2d\u3002\\n\\n\\n#### \u51c6\u5907\u5de5\u4f5c\\n\\n##### 1. TiDB \u8868\u7ed3\u6784\u4ecb\u7ecd\\n\\n**Input**\uff08\u5b58\u50a8\u8bbf\u95ee\u65e5\u5fd7\u7684\u8868\uff09\\n\\n```\\nCREATE TABLE access_log (\\n    domain VARCHAR(255),\\n    datetime VARCHAR(63),\\n    remote_addr VARCHAR(63),\\n    http_ver VARCHAR(15),\\n    body_bytes_send INT,\\n    status INT,\\n    request_time FLOAT,\\n    url TEXT\\n)\\n```\\n\\n```\\n+-----------------+--------------+------+------+---------+-------+\\n| Field           | Type         | Null | Key  | Default | Extra |\\n+-----------------+--------------+------+------+---------+-------+\\n| domain          | varchar(255) | YES  |      | NULL    |       |\\n| datetime        | varchar(63)  | YES  |      | NULL    |       |\\n| remote_addr     | varchar(63)  | YES  |      | NULL    |       |\\n| http_ver        | varchar(15)  | YES  |      | NULL    |       |\\n| body_bytes_send | int(11)      | YES  |      | NULL    |       |\\n| status          | int(11)      | YES  |      | NULL    |       |\\n| request_time    | float        | YES  |      | NULL    |       |\\n| url             | text         | YES  |      | NULL    |       |\\n+-----------------+--------------+------+------+---------+-------+\\n```\\n\\n**Output**\uff08\u5b58\u50a8\u7ed3\u679c\u6570\u636e\u7684\u8868\uff09\\n\\n```\\nCREATE TABLE access_collect (\\n    date VARCHAR(23),\\n    domain VARCHAR(63),\\n    status INT,\\n    hit INT\\n)\\n```\\n\\n```\\n+--------+-------------+------+------+---------+-------+\\n| Field  | Type        | Null | Key  | Default | Extra |\\n+--------+-------------+------+------+---------+-------+\\n| date   | varchar(23) | YES  |      | NULL    |       |\\n| domain | varchar(63) | YES  |      | NULL    |       |\\n| status | int(11)     | YES  |      | NULL    |       |\\n| hit    | int(11)     | YES  |      | NULL    |       |\\n+--------+-------------+------+------+---------+-------+\\n```\\n\\n##### 2. \u5b89\u88c5 Seatunnel\\n\\n\u6709\u4e86 TiDB \u8f93\u5165\u548c\u8f93\u51fa\u8868\u4e4b\u540e\uff0c \u6211\u4eec\u9700\u8981\u5b89\u88c5 Seatunnel\uff0c\u5b89\u88c5\u5341\u5206\u7b80\u5355\uff0c\u65e0\u9700\u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\\n1. \u51c6\u5907 Spark\u73af\u5883\\n2. \u5b89\u88c5 Seatunnel\\n3. \u914d\u7f6e Seatunnel\\n\\n\u4ee5\u4e0b\u662f\u7b80\u6613\u6b65\u9aa4\uff0c\u5177\u4f53\u5b89\u88c5\u53ef\u4ee5\u53c2\u7167 [Quick Start](/docs/quick-start)\\n\\n```\\n# \u4e0b\u8f7d\u5b89\u88c5Spark\\ncd /usr/local\\nwget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz\\nwget\\n# \u4e0b\u8f7d\u5b89\u88c5seatunnel\\nhttps://github.com/InterestingLab/seatunnel/releases/download/v1.2.0/seatunnel-1.2.0.zip\\nunzip seatunnel-1.2.0.zip\\ncd seatunnel-1.2.0\\n\\nvim config/seatunnel-env.sh\\n# \u6307\u5b9aSpark\u5b89\u88c5\u8def\u5f84\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.1.0-bin-hadoop2.7}\\n```\\n\\n\\n### \u5b9e\u73b0 Seatunnel \u5904\u7406\u6d41\u7a0b\\n\\n\u6211\u4eec\u4ec5\u9700\u8981\u7f16\u5199\u4e00\u4e2a Seatunnel \u914d\u7f6e\u6587\u4ef6\u5373\u53ef\u5b8c\u6210\u6570\u636e\u7684\u8bfb\u53d6\u3001\u5904\u7406\u3001\u5199\u5165\u3002\\n\\nSeatunnel \u914d\u7f6e\u6587\u4ef6\u7531\u56db\u4e2a\u90e8\u5206\u7ec4\u6210\uff0c\u5206\u522b\u662f `Spark`\u3001`Input`\u3001`Filter` \u548c `Output`\u3002`Input` \u90e8\u5206\u7528\u4e8e\u6307\u5b9a\u6570\u636e\u7684\u8f93\u5165\u6e90\uff0c`Filter` \u90e8\u5206\u7528\u4e8e\u5b9a\u4e49\u5404\u79cd\u5404\u6837\u7684\u6570\u636e\u5904\u7406\u3001\u805a\u5408\uff0c`Output` \u90e8\u5206\u8d1f\u8d23\u5c06\u5904\u7406\u4e4b\u540e\u7684\u6570\u636e\u5199\u5165\u6307\u5b9a\u7684\u6570\u636e\u5e93\u6216\u8005\u6d88\u606f\u961f\u5217\u3002\\n\\n\u6574\u4e2a\u5904\u7406\u6d41\u7a0b\u4e3a `Input` -> `Filter` -> `Output`\uff0c\u6574\u4e2a\u6d41\u7a0b\u7ec4\u6210\u4e86 Seatunnel \u7684 \u5904\u7406\u6d41\u7a0b\uff08Pipeline\uff09\u3002\\n\\n> \u4ee5\u4e0b\u662f\u4e00\u4e2a\u5177\u4f53\u914d\u7f6e\uff0c\u6b64\u914d\u7f6e\u6765\u6e90\u4e8e\u7ebf\u4e0a\u5b9e\u9645\u5e94\u7528\uff0c\u4f46\u662f\u4e3a\u4e86\u6f14\u793a\u6709\u6240\u7b80\u5316\u3002\\n\\n\\n##### Input (TiDB)\\n\\n\u8fd9\u91cc\u90e8\u5206\u914d\u7f6e\u5b9a\u4e49\u8f93\u5165\u6e90\uff0c\u5982\u4e0b\u662f\u4ece TiDB \u4e00\u5f20\u8868\u4e2d\u8bfb\u53d6\u6570\u636e\u3002\\n\\n    input {\\n        tidb {\\n            database = \\"nginx\\"\\n            pre_sql = \\"select * from nginx.access_log\\"\\n            table_name = \\"spark_nginx_input\\"\\n        }\\n    }\\n\\n##### Filter\\n\\n\u5728Filter\u90e8\u5206\uff0c\u8fd9\u91cc\u6211\u4eec\u914d\u7f6e\u4e00\u7cfb\u5217\u7684\u8f6c\u5316, \u5927\u90e8\u5206\u6570\u636e\u5206\u6790\u7684\u9700\u6c42\uff0c\u90fd\u662f\u5728Filter\u5b8c\u6210\u7684\u3002Seatunnel \u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u63d2\u4ef6\uff0c\u8db3\u4ee5\u6ee1\u8db3\u5404\u79cd\u6570\u636e\u5206\u6790\u9700\u6c42\u3002\u8fd9\u91cc\u6211\u4eec\u901a\u8fc7 SQL \u63d2\u4ef6\u5b8c\u6210\u6570\u636e\u7684\u805a\u5408\u64cd\u4f5c\u3002\\n\\n    filter {\\n        sql {\\n            table_name = \\"spark_nginx_log\\"\\n            sql = \\"select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=\'2019-01-20\' group by domain, status, substring(datetime, 1, 10)\\"\\n        }\\n    }\\n\\n\\n##### Output (TiDB)\\n\\n\u6700\u540e\uff0c \u6211\u4eec\u5c06\u5904\u7406\u540e\u7684\u7ed3\u679c\u5199\u5165TiDB\u53e6\u5916\u4e00\u5f20\u8868\u4e2d\u3002TiDB Output\u662f\u901a\u8fc7JDBC\u5b9e\u73b0\u7684\\n\\n    output {\\n        tidb {\\n            url = \\"jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&characterEncoding=utf8\\"\\n            table = \\"access_collect\\"\\n            user = \\"username\\"\\n            password = \\"password\\"\\n            save_mode = \\"append\\"\\n        }\\n    }\\n\\n##### Spark\\n\\n\u8fd9\u4e00\u90e8\u5206\u662f Spark \u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u914d\u7f6e Spark \u6267\u884c\u65f6\u6240\u9700\u7684\u8d44\u6e90\u5927\u5c0f\u4ee5\u53ca\u5176\u4ed6 Spark \u914d\u7f6e\u3002\\n\\n\u6211\u4eec\u7684 TiDB Input \u63d2\u4ef6\u662f\u57fa\u4e8e TiSpark \u5b9e\u73b0\u7684\uff0c\u800c TiSpark \u4f9d\u8d56\u4e8e TiKV \u96c6\u7fa4\u548c Placement Driver (PD)\u3002\u56e0\u6b64\u6211\u4eec\u9700\u8981\u6307\u5b9a PD \u8282\u70b9\u4fe1\u606f\u4ee5\u53ca TiSpark \u76f8\u5173\u914d\u7f6e`spark.tispark.pd.addresses`\u548c`spark.sql.extensions`\u3002\\n\\n\\n    spark {\\n      spark.app.name = \\"seatunnel-tidb\\"\\n      spark.executor.instances = 2\\n      spark.executor.cores = 1\\n      spark.executor.memory = \\"1g\\"\\n      # Set for TiSpark\\n      spark.tispark.pd.addresses = \\"localhost:2379\\"\\n      spark.sql.extensions = \\"org.apache.spark.sql.TiExtensions\\"\\n    }\\n\\n\\n#### \u8fd0\u884c Seatunnel\\n\\n\u6211\u4eec\u5c06\u4e0a\u8ff0\u56db\u90e8\u5206\u914d\u7f6e\u7ec4\u5408\u6210\u6211\u4eec\u6700\u7ec8\u7684\u914d\u7f6e\u6587\u4ef6 `conf/tidb.conf`\\n\\n```\\nspark {\\n    spark.app.name = \\"seatunnel-tidb\\"\\n    spark.executor.instances = 2\\n    spark.executor.cores = 1\\n    spark.executor.memory = \\"1g\\"\\n    # Set for TiSpark\\n    spark.tispark.pd.addresses = \\"localhost:2379\\"\\n    spark.sql.extensions = \\"org.apache.spark.sql.TiExtensions\\"\\n}\\ninput {\\n    tidb {\\n        database = \\"nginx\\"\\n        pre_sql = \\"select * from nginx.access_log\\"\\n        table_name = \\"spark_table\\"\\n    }\\n}\\nfilter {\\n    sql {\\n        table_name = \\"spark_nginx_log\\"\\n        sql = \\"select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)=\'2019-01-20\' group by domain, status, substring(datetime, 1, 10)\\"\\n    }\\n}\\noutput {\\n    tidb {\\n        url = \\"jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&characterEncoding=utf8\\"\\n        table = \\"access_collect\\"\\n        user = \\"username\\"\\n        password = \\"password\\"\\n        save_mode = \\"append\\"\\n    }\\n}\\n```\\n\\n\u6267\u884c\u547d\u4ee4\uff0c\u6307\u5b9a\u914d\u7f6e\u6587\u4ef6\uff0c\u8fd0\u884c Seatunnel \uff0c\u5373\u53ef\u5b9e\u73b0\u6211\u4eec\u7684\u6570\u636e\u5904\u7406\u903b\u8f91\u3002\\n\\n* Local\\n\\n> ./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master \'local[2]\'\\n\\n* yarn-client\\n\\n> ./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master yarn\\n\\n* yarn-cluster\\n\\n> ./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode cluster -master yarn\\n\\n\u5982\u679c\u662f\u672c\u673a\u6d4b\u8bd5\u9a8c\u8bc1\u903b\u8f91\uff0c\u7528\u672c\u5730\u6a21\u5f0f\uff08Local\uff09\u5c31\u53ef\u4ee5\u4e86\uff0c\u4e00\u822c\u751f\u4ea7\u73af\u5883\u4e0b\uff0c\u90fd\u662f\u4f7f\u7528`yarn-client`\u6216\u8005`yarn-cluster`\u6a21\u5f0f\u3002\\n\\n#### \u68c0\u67e5\u7ed3\u679c\\n\\n```\\nmysql> select * from access_collect;\\n+------------+--------+--------+------+\\n| date       | domain | status | hit  |\\n+------------+--------+--------+------+\\n| 2019-01-20 | b.com  |    200 |   63 |\\n| 2019-01-20 | a.com  |    200 |   85 |\\n+------------+--------+--------+------+\\n2 rows in set (0.21 sec)\\n```\\n\\n\\n\\n## \u603b\u7ed3\\n\\n\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u5982\u4f55\u4f7f\u7528 Seatunnel \u4ece TiDB \u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u505a\u7b80\u5355\u7684\u6570\u636e\u5904\u7406\u4e4b\u540e\u5199\u5165 TiDB \u53e6\u5916\u4e00\u4e2a\u8868\u4e2d\u3002\u4ec5\u901a\u8fc7\u4e00\u4e2a\u914d\u7f6e\u6587\u4ef6\u4fbf\u53ef\u5feb\u901f\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\uff0c\u65e0\u9700\u7f16\u5199\u4efb\u4f55\u4ee3\u7801\u3002\\n\\n\u9664\u4e86\u652f\u6301 TiDB \u6570\u636e\u6e90\u4e4b\u5916\uff0cSeatunnel \u540c\u6837\u652f\u6301Elasticsearch, Kafka, Kudu, ClickHouse\u7b49\u6570\u636e\u6e90\u3002\\n\\n**\u4e8e\u6b64\u540c\u65f6\uff0c\u6211\u4eec\u6b63\u5728\u7814\u53d1\u4e00\u4e2a\u91cd\u8981\u529f\u80fd\uff0c\u5c31\u662f\u5728 Seatunnel \u4e2d\uff0c\u5229\u7528 TiDB \u7684\u4e8b\u52a1\u7279\u6027\uff0c\u5b9e\u73b0\u4ece Kafka \u5230 TiDB \u6d41\u5f0f\u6570\u636e\u5904\u7406\uff0c\u5e76\u4e14\u652f\u6301\u7aef\uff08Kafka\uff09\u5230\u7aef\uff08TiDB\uff09\u7684 Exactly-Once \u6570\u636e\u4e00\u81f4\u6027\u3002**\\n\\n\u5e0c\u671b\u4e86\u89e3 Seatunnel \u548c TiDB\uff0cClickHouse\u3001Elasticsearch\u3001Kafka\u7ed3\u5408\u4f7f\u7528\u7684\u66f4\u591a\u529f\u80fd\u548c\u6848\u4f8b\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fdb\u5165\u5b98\u7f51 [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n## \u8054\u7cfb\u6211\u4eec\\n* \u90ae\u4ef6\u5217\u8868 : **dev@seatunnel.apache.org**. \u53d1\u9001\u4efb\u610f\u5185\u5bb9\u81f3 `dev-subscribe@seatunnel.apache.org`\uff0c \u6309\u7167\u56de\u590d\u8ba2\u9605\u90ae\u4ef6\u5217\u8868\u3002\\n* Slack: \u53d1\u9001 `Request to join SeaTunnel slack` \u90ae\u4ef6\u5230\u90ae\u4ef6\u5217\u8868 (`dev@seatunnel.apache.org`), \u6211\u4eec\u4f1a\u9080\u8bf7\u4f60\u52a0\u5165\uff08\u5728\u6b64\u4e4b\u524d\u8bf7\u786e\u8ba4\u5df2\u7ecf\u6ce8\u518cSlack\uff09.\\n* [bilibili B\u7ad9 \u89c6\u9891](https://space.bilibili.com/1542095008)\\n\\n-- Power by [InterestingLab](https://github.com/InterestingLab)"},{"id":"spark-structured-streaming","metadata":{"permalink":"/zh-CN/blog/spark-structured-streaming","editUrl":"https://github.com/apache/seatunnel-website/edit/main/blog/2021-12-30-spark-structured-streaming.md","source":"@site/i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-spark-structured-streaming.md","title":"\u5982\u4f55\u652f\u6301\u7684 Spark StructuredStreaming","description":"\u524d\u8a00","date":"2021-12-30T00:00:00.000Z","formattedDate":"2021\u5e7412\u670830\u65e5","tags":[{"label":"Spark","permalink":"/zh-CN/blog/tags/spark"},{"label":"StructuredStreaming","permalink":"/zh-CN/blog/tags/structured-streaming"}],"readingTime":10.075,"hasTruncateMarker":false,"authors":[],"frontMatter":{"slug":"spark-structured-streaming","title":"\u5982\u4f55\u652f\u6301\u7684 Spark StructuredStreaming","tags":["Spark","StructuredStreaming"]},"prevItem":{"title":"\u600e\u4e48\u7528 Spark \u5728 TiDB \u4e0a\u505a OLAP \u5206\u6790","permalink":"/zh-CN/blog/spark-execute-tidb"}},"content":"### \u524d\u8a00\\n\\nStructuredStreaming\u662fSpark 2.0\u4ee5\u540e\u65b0\u5f00\u653e\u7684\u4e00\u4e2a\u6a21\u5757\uff0c\u76f8\u6bd4SparkStreaming\uff0c\u5b83\u6709\u4e00\u4e9b\u6bd4\u8f83\u7a81\u51fa\u7684\u4f18\u70b9\uff1a<br/> &emsp;&emsp;\u4e00\u3001\u5b83\u80fd\u505a\u5230\u66f4\u4f4e\u7684\u5ef6\u8fdf;<br/>\\n&emsp;&emsp;\u4e8c\u3001\u53ef\u4ee5\u505a\u5b9e\u65f6\u7684\u805a\u5408\uff0c\u4f8b\u5982\u5b9e\u65f6\u8ba1\u7b97\u6bcf\u5929\u6bcf\u4e2a\u5546\u54c1\u7684\u9500\u552e\u603b\u989d\uff1b<br/>\\n&emsp;&emsp;\u4e09\u3001\u53ef\u4ee5\u505a\u6d41\u4e0e\u6d41\u4e4b\u95f4\u7684\u5173\u8054\uff0c\u4f8b\u5982\u8ba1\u7b97\u5e7f\u544a\u7684\u70b9\u51fb\u7387\uff0c\u9700\u8981\u5c06\u5e7f\u544a\u7684\u66dd\u5149\u8bb0\u5f55\u548c\u70b9\u51fb\u8bb0\u5f55\u5173\u8054\u3002<br/>\\n\u4ee5\u4e0a\u51e0\u70b9\u5982\u679c\u4f7f\u7528SparkStreaming\u6765\u5b9e\u73b0\u53ef\u80fd\u4f1a\u6bd4\u8f83\u9ebb\u70e6\u6216\u8005\u8bf4\u662f\u5f88\u96be\u5b9e\u73b0\uff0c\u4f46\u662f\u4f7f\u7528StructuredStreaming\u5b9e\u73b0\u8d77\u6765\u4f1a\u6bd4\u8f83\u8f7b\u677e\u3002\\n### \u5982\u4f55\u4f7f\u7528StructuredStreaming\\n\u53ef\u80fd\u4f60\u6ca1\u6709\u8be6\u7ec6\u7814\u7a76\u8fc7StructuredStreaming\uff0c\u4f46\u662f\u53d1\u73b0StructuredStreaming\u80fd\u5f88\u597d\u7684\u89e3\u51b3\u4f60\u7684\u9700\u6c42\uff0c\u5982\u4f55\u5feb\u901f\u5229\u7528StructuredStreaming\u6765\u89e3\u51b3\u4f60\u7684\u9700\u6c42\uff1f\u76ee\u524d\u793e\u533a\u6709\u4e00\u6b3e\u5de5\u5177 **Seatunnel**\uff0c\u9879\u76ee\u5730\u5740\uff1a[https://github.com/apache/incubator-seatunnel](https://github.com/apache/incubator-seatunnel) ,\\n\u53ef\u4ee5\u9ad8\u6548\u4f4e\u6210\u672c\u7684\u5e2e\u52a9\u4f60\u5229\u7528StructuredStreaming\u6765\u5b8c\u6210\u4f60\u7684\u9700\u6c42\u3002\\n\\n### Seatunnel\\n\\nSeatunnel \u662f\u4e00\u4e2a\u975e\u5e38\u6613\u7528\uff0c\u9ad8\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6d77\u91cf\u6570\u636e\u7684\u5b9e\u65f6\u6570\u636e\u5904\u7406\u4ea7\u54c1\uff0c\u5b83\u6784\u5efa\u5728Spark\u4e4b\u4e0a\u3002Seatunnel \u62e5\u6709\u7740\u975e\u5e38\u4e30\u5bcc\u7684\u63d2\u4ef6\uff0c\u652f\u6301\u4eceKafka\u3001HDFS\u3001Kudu\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u8fdb\u884c\u5404\u79cd\u5404\u6837\u7684\u6570\u636e\u5904\u7406\uff0c\u5e76\u5c06\u7ed3\u679c\u5199\u5165ClickHouse\u3001Elasticsearch\u6216\u8005Kafka\u4e2d\\n\\n### \u51c6\u5907\u5de5\u4f5c\\n\\n\u9996\u5148\u6211\u4eec\u9700\u8981\u5b89\u88c5 Seatunnel\uff0c\u5b89\u88c5\u5341\u5206\u7b80\u5355\uff0c\u65e0\u9700\u914d\u7f6e\u7cfb\u7edf\u73af\u5883\u53d8\u91cf\\n\\n1. \u51c6\u5907Spark\u73af\u5883\\n2. \u5b89\u88c5 Seatunnel\\n3. \u914d\u7f6e Seatunnel\\n\\n\u4ee5\u4e0b\u662f\u7b80\u6613\u6b65\u9aa4\uff0c\u5177\u4f53\u5b89\u88c5\u53ef\u4ee5\u53c2\u7167 [Quick Start](/docs/quick-start)\\n\\n```\\ncd /usr/local\\nwget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\ntar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz\\nwget https://github.com/InterestingLab/seatunnel/releases/download/v1.3.0/seatunnel-1.3.0.zip\\nunzip seatunnel-1.3.0.zip\\ncd seatunnel-1.3.0\\n\\nvim config/seatunnel-env.sh\\n# \u6307\u5b9aSpark\u5b89\u88c5\u8def\u5f84\\nSPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}\\n```\\n\\n### Seatunnel Pipeline\\n\\n\u6211\u4eec\u4ec5\u9700\u8981\u7f16\u5199\u4e00\u4e2a Seatunnel Pipeline\u7684\u914d\u7f6e\u6587\u4ef6\u5373\u53ef\u5b8c\u6210\u6570\u636e\u7684\u5bfc\u5165\u3002\\n\\n\u914d\u7f6e\u6587\u4ef6\u5305\u62ec\u56db\u4e2a\u90e8\u5206\uff0c\u5206\u522b\u662fSpark\u3001Input\u3001filter\u548cOutput\u3002\\n\\n#### Spark\\n\\n\u8fd9\u4e00\u90e8\u5206\u662fSpark\u7684\u76f8\u5173\u914d\u7f6e\uff0c\u4e3b\u8981\u914d\u7f6eSpark\u6267\u884c\u65f6\u6240\u9700\u7684\u8d44\u6e90\u5927\u5c0f\u3002\\n\\n```\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n```\\n\\n#### Input\\n\\n\u4e0b\u9762\u662f\u4e00\u4e2a\u4ecekafka\u8bfb\u53d6\u6570\u636e\u7684\u4f8b\u5b50\\n\\n```\\nkafkaStream {\\n    topics = \\"seatunnel\\"\\n    consumer.bootstrap.servers = \\"localhost:9092\\"\\n    schema = \\"{\\\\\\"name\\\\\\":\\\\\\"string\\\\\\",\\\\\\"age\\\\\\":\\\\\\"integer\\\\\\",\\\\\\"addrs\\\\\\":{\\\\\\"country\\\\\\":\\\\\\"string\\\\\\",\\\\\\"city\\\\\\":\\\\\\"string\\\\\\"}}\\"\\n}\\n```\\n\\n\u901a\u8fc7\u4e0a\u9762\u7684\u914d\u7f6e\u5c31\u53ef\u4ee5\u8bfb\u53d6kafka\u91cc\u7684\u6570\u636e\u4e86 \uff0ctopics\u662f\u8981\u8ba2\u9605\u7684kafka\u7684topic\uff0c\u540c\u65f6\u8ba2\u9605\u591a\u4e2atopic\u53ef\u4ee5\u4ee5\u9017\u53f7\u9694\u5f00\uff0cconsumer.bootstrap.servers\u5c31\u662fKafka\u7684\u670d\u52a1\u5668\u5217\u8868\uff0cschema\u662f\u53ef\u9009\u9879\uff0c\u56e0\u4e3aStructuredStreaming\u4ecekafka\u8bfb\u53d6\u5230\u7684\u503c(\u5b98\u65b9\u56fa\u5b9a\u5b57\u6bb5value)\u662fbinary\u7c7b\u578b\u7684\uff0c\u8be6\u89c1http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\\n\u4f46\u662f\u5982\u679c\u4f60\u786e\u5b9a\u4f60kafka\u91cc\u7684\u6570\u636e\u662fjson\u5b57\u7b26\u4e32\u7684\u8bdd\uff0c\u4f60\u53ef\u4ee5\u6307\u5b9aschema\uff0cinput\u63d2\u4ef6\u5c06\u6309\u7167\u4f60\u6307\u5b9a\u7684schema\u89e3\u6790\\n\\n#### Filter\\n\\n\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684filter\u4f8b\u5b50\\n\\n```\\nfilter{\\n    sql{\\n        table_name = \\"student\\"\\n        sql = \\"select name,age from student\\"\\n    }\\n}\\n```\\n`table_name`\u662f\u6ce8\u518c\u6210\u7684\u4e34\u65f6\u8868\u540d\uff0c\u4ee5\u4fbf\u4e8e\u5728\u4e0b\u9762\u7684sql\u4f7f\u7528\\n\\n#### Output\\n\\n\u5904\u7406\u597d\u7684\u6570\u636e\u5f80\u5916\u8f93\u51fa\uff0c\u5047\u8bbe\u6211\u4eec\u7684\u8f93\u51fa\u4e5f\u662fkafka\\n\\n```\\noutput{\\n    kafka {\\n        topic = \\"seatunnel\\"\\n        producer.bootstrap.servers = \\"localhost:9092\\"\\n        streaming_output_mode = \\"update\\"\\n        checkpointLocation = \\"/your/path\\"\\n    }\\n}\\n```\\n\\n`topic` \u662f\u4f60\u8981\u8f93\u51fa\u7684topic\uff0c` producer.bootstrap.servers`\u662fkafka\u96c6\u7fa4\u5217\u8868\uff0c`streaming_output_mode`\u662fStructuredStreaming\u7684\u4e00\u4e2a\u8f93\u51fa\u6a21\u5f0f\u53c2\u6570\uff0c\u6709\u4e09\u79cd\u7c7b\u578b`append|update|complete`\uff0c\u5177\u4f53\u4f7f\u7528\u53c2\u89c1\u6587\u6863http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\\n\\n`checkpointLocation`\u662fStructuredStreaming\u7684checkpoint\u8def\u5f84\uff0c\u5982\u679c\u914d\u7f6e\u4e86\u7684\u8bdd\uff0c\u8fd9\u4e2a\u76ee\u5f55\u4f1a\u5b58\u50a8\u7a0b\u5e8f\u7684\u8fd0\u884c\u4fe1\u606f\uff0c\u6bd4\u5982\u7a0b\u5e8f\u9000\u51fa\u518d\u542f\u52a8\u7684\u8bdd\u4f1a\u63a5\u7740\u4e0a\u6b21\u7684offset\u8fdb\u884c\u6d88\u8d39\u3002\\n\\n### \u573a\u666f\u5206\u6790\\n\\n\u4ee5\u4e0a\u5c31\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4f8b\u5b50\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c31\u6765\u4ecb\u7ecd\u7684\u7a0d\u5fae\u590d\u6742\u4e00\u4e9b\u7684\u4e1a\u52a1\u573a\u666f\\n\\n#### \u573a\u666f\u4e00\uff1a\u5b9e\u65f6\u805a\u5408\u573a\u666f\\n\\n\u5047\u8bbe\u73b0\u5728\u6709\u4e00\u4e2a\u5546\u57ce\uff0c\u4e0a\u9762\u670910\u79cd\u5546\u54c1\uff0c\u73b0\u5728\u9700\u8981\u5b9e\u65f6\u6c42\u6bcf\u5929\u6bcf\u79cd\u5546\u54c1\u7684\u9500\u552e\u989d\uff0c\u751a\u81f3\u662f\u6c42\u6bcf\u79cd\u5546\u54c1\u7684\u8d2d\u4e70\u4eba\u6570\uff08\u4e0d\u8981\u6c42\u5341\u5206\u7cbe\u786e\uff09\u3002\\n\u8fd9\u4e48\u505a\u7684\u5de8\u5927\u7684\u4f18\u52bf\u5c31\u662f\u6d77\u91cf\u6570\u636e\u53ef\u4ee5\u5728\u5b9e\u65f6\u5904\u7406\u7684\u65f6\u5019\uff0c\u5b8c\u6210\u805a\u5408\uff0c\u518d\u4e5f\u4e0d\u9700\u8981\u5148\u5c06\u6570\u636e\u5199\u5165\u6570\u636e\u4ed3\u5e93\uff0c\u518d\u8dd1\u79bb\u7ebf\u7684\u5b9a\u65f6\u4efb\u52a1\u8fdb\u884c\u805a\u5408\uff0c\\n\u64cd\u4f5c\u8d77\u6765\u8fd8\u662f\u5f88\u65b9\u4fbf\u7684\u3002\\n\\nkafka\u7684\u6570\u636e\u5982\u4e0b\\n\\n```\\n{\\"good_id\\":\\"abc\\",\\"price\\":300,\\"user_id\\":123456,\\"time\\":1553216320}\\n```\\n\\n\u90a3\u6211\u4eec\u8be5\u600e\u4e48\u5229\u7528 Seatunnel \u6765\u5b8c\u6210\u8fd9\u4e2a\u9700\u6c42\u5462\uff0c\u5f53\u7136\u8fd8\u662f\u53ea\u9700\u8981\u914d\u7f6e\u5c31\u597d\u4e86\u3002\\n\\n```\\n#spark\u91cc\u7684\u914d\u7f6e\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u914d\u7f6e\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n\\n#\u914d\u7f6einput\\ninput {\\n    kafkaStream {\\n        topics = \\"good_topic\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        schema = \\"{\\\\\\"good_id\\\\\\":\\\\\\"string\\\\\\",\\\\\\"price\\\\\\":\\\\\\"integer\\\\\\",\\\\\\"user_id\\\\\\":\\\\\\"Long\\\\\\",\\\\\\"time\\\\\\":\\\\\\"Long\\\\\\"}\\"\\n    }\\n}\\n\\n#\u914d\u7f6efilter    \\nfilter {\\n    \\n    #\u5728\u7a0b\u5e8f\u505a\u805a\u5408\u7684\u65f6\u5019\uff0c\u5185\u90e8\u4f1a\u53bb\u5b58\u50a8\u7a0b\u5e8f\u4ece\u542f\u52a8\u5f00\u59cb\u7684\u805a\u5408\u72b6\u6001\uff0c\u4e45\u800c\u4e45\u4e4b\u4f1a\u5bfc\u81f4OOM,\u5982\u679c\u8bbe\u7f6e\u4e86watermark\uff0c\u7a0b\u5e8f\u81ea\u52a8\u7684\u4f1a\u53bb\u6e05\u7406watermark\u4e4b\u5916\u7684\u72b6\u6001\\n    #\u8fd9\u91cc\u8868\u793a\u4f7f\u7528ts\u5b57\u6bb5\u8bbe\u7f6ewatermark\uff0c\u754c\u9650\u4e3a1\u5929\\n\\n    Watermark {\\n        time_field = \\"time\\"\\n        time_type = \\"UNIX\\"              #UNIX\u8868\u793a\u65f6\u95f4\u5b57\u6bb5\u4e3a10\u4e3a\u7684\u65f6\u95f4\u6233\uff0c\u8fd8\u6709\u5176\u4ed6\u7684\u7c7b\u578b\u8be6\u7ec6\u53ef\u4ee5\u67e5\u770b\u63d2\u4ef6\u6587\u6863\\n        time_pattern = \\"yyyy-MM-dd\\"     #\u8fd9\u91cc\u4e4b\u6240\u4ee5\u8981\u628ats\u5bf9\u5176\u5230\u5929\u662f\u56e0\u4e3a\u6c42\u6bcf\u5929\u7684\u9500\u552e\u989d\uff0c\u5982\u679c\u662f\u6c42\u6bcf\u5c0f\u65f6\u7684\u9500\u552e\u989d\u53ef\u4ee5\u5bf9\u5176\u5230\u5c0f\u65f6`yyyy-MM-dd HH`\\n        delay_threshold = \\"1 day\\"\\n        watermark_field = \\"ts\\"          #\u8bbe\u7f6ewatermark\u4e4b\u540e\u4f1a\u65b0\u589e\u4e00\u4e2a\u5b57\u6bb5\uff0c`ts`\u5c31\u662f\u8fd9\u4e2a\u5b57\u6bb5\u7684\u540d\u5b57\\n    }\\n    \\n    #\u4e4b\u6240\u4ee5\u8981group by ts\u662f\u8981\u8ba9watermark\u751f\u6548\uff0capprox_count_distinct\u662f\u4e00\u4e2a\u4f30\u503c\uff0c\u5e76\u4e0d\u662f\u7cbe\u786e\u7684count_distinct\\n    sql {\\n        table_name = \\"good_table_2\\"\\n        sql = \\"select good_id,sum(price) total,\\tapprox_count_distinct(user_id) person from good_table_2 group by ts,good_id\\"\\n    }\\n}\\n\\n#\u63a5\u4e0b\u6765\u6211\u4eec\u9009\u62e9\u5c06\u7ed3\u679c\u5b9e\u65f6\u8f93\u51fa\u5230Kafka\\noutput{\\n    kafka {\\n        topic = \\"seatunnel\\"\\n        producer.bootstrap.servers = \\"localhost:9092\\"\\n        streaming_output_mode = \\"update\\"\\n        checkpointLocation = \\"/your/path\\"\\n    }\\n}\\n\\n```\\n\u5982\u4e0a\u914d\u7f6e\u5b8c\u6210\uff0c\u542f\u52a8 Seatunnel\uff0c\u5c31\u53ef\u4ee5\u83b7\u53d6\u4f60\u60f3\u8981\u7684\u7ed3\u679c\u4e86\u3002\\n\\n#### \u573a\u666f\u4e8c\uff1a\u591a\u4e2a\u6d41\u5173\u8054\u573a\u666f\\n\\n\u5047\u8bbe\u4f60\u5728\u67d0\u4e2a\u5e73\u53f0\u6295\u653e\u4e86\u5e7f\u544a\uff0c\u73b0\u5728\u8981\u5b9e\u65f6\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u5e7f\u544a\u7684CTR(\u70b9\u51fb\u7387)\uff0c\u6570\u636e\u5206\u522b\u6765\u81ea\u4e24\u4e2atopic\uff0c\u4e00\u4e2a\u662f\u5e7f\u544a\u66dd\u5149\u65e5\u5fd7\uff0c\u4e00\u4e2a\u662f\u5e7f\u544a\u70b9\u51fb\u65e5\u5fd7,\\n\u6b64\u65f6\u6211\u4eec\u5c31\u9700\u8981\u628a\u4e24\u4e2a\u6d41\u6570\u636e\u5173\u8054\u5230\u4e00\u8d77\u505a\u8ba1\u7b97\uff0c\u800c Seatunnel \u6700\u8fd1\u4e5f\u652f\u6301\u4e86\u6b64\u529f\u80fd\uff0c\u8ba9\u6211\u4eec\u4e00\u8d77\u770b\u4e00\u4e0b\u8be5\u600e\u4e48\u505a\uff1a\\n\\n\\n\u70b9\u51fbtopic\u6570\u636e\u683c\u5f0f\\n\\n```\\n{\\"ad_id\\":\\"abc\\",\\"click_time\\":1553216320,\\"user_id\\":12345}\\n\\n```\\n\\n\u66dd\u5149topic\u6570\u636e\u683c\u5f0f\\n\\n```\\n{\\"ad_id\\":\\"abc\\",\\"show_time\\":1553216220,\\"user_id\\":12345}\\n\\n```\\n\\n```\\n#spark\u91cc\u7684\u914d\u7f6e\u6839\u636e\u4e1a\u52a1\u9700\u6c42\u914d\u7f6e\\nspark {\\n  spark.app.name = \\"seatunnel\\"\\n  spark.executor.instances = 2\\n  spark.executor.cores = 1\\n  spark.executor.memory = \\"1g\\"\\n}\\n\\n#\u914d\u7f6einput\\ninput {\\n    \\n    kafkaStream {\\n        topics = \\"click_topic\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        schema = \\"{\\\\\\"ad_id\\\\\\":\\\\\\"string\\\\\\",\\\\\\"user_id\\\\\\":\\\\\\"Long\\\\\\",\\\\\\"click_time\\\\\\":\\\\\\"Long\\\\\\"}\\"\\n        table_name = \\"click_table\\"\\n    }\\n    \\n    kafkaStream {\\n        topics = \\"show_topic\\"\\n        consumer.bootstrap.servers = \\"localhost:9092\\"\\n        schema = \\"{\\\\\\"ad_id\\\\\\":\\\\\\"string\\\\\\",\\\\\\"user_id\\\\\\":\\\\\\"Long\\\\\\",\\\\\\"show_time\\\\\\":\\\\\\"Long\\\\\\"}\\"\\n        table_name = \\"show_table\\"\\n    }\\n}\\n\\nfilter {\\n    \\n    #\u5de6\u5173\u8054\u53f3\u8868\u5fc5\u987b\u8bbe\u7f6ewatermark\\n    #\u53f3\u5173\u5de6\u53f3\u8868\u5fc5\u987b\u8bbe\u7f6ewatermark\\n    #http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#inner-joins-with-optional-watermarking\\n    Watermark {\\n              source_table_name = \\"click_table\\" #\u8fd9\u91cc\u53ef\u4ee5\u6307\u5b9a\u4e3a\u67d0\u4e2a\u4e34\u65f6\u8868\u6dfb\u52a0watermark\uff0c\u4e0d\u6307\u5b9a\u7684\u8bdd\u5c31\u662f\u4e3ainput\u4e2d\u7684\u7b2c\u4e00\u4e2a\\n              time_field = \\"time\\"\\n              time_type = \\"UNIX\\"               \\n              delay_threshold = \\"3 hours\\"\\n              watermark_field = \\"ts\\" \\n              result_table_name = \\"click_table_watermark\\" #\u6dfb\u52a0\u5b8cwatermark\u4e4b\u540e\u53ef\u4ee5\u6ce8\u518c\u6210\u4e34\u65f6\u8868\uff0c\u65b9\u4fbf\u540e\u7eed\u5728sql\u4e2d\u4f7f\u7528\\n    }\\n    \\n    Watermark {\\n                source_table_name = \\"show_table\\" \\n                time_field = \\"time\\"\\n                time_type = \\"UNIX\\"               \\n                delay_threshold = \\"2 hours\\"\\n                watermark_field = \\"ts\\" \\n                result_table_name = \\"show_table_watermark\\" \\n     }\\n    \\n    \\n    sql {\\n        table_name = \\"show_table_watermark\\"\\n        sql = \\"select a.ad_id,count(b.user_id)/count(a.user_id) ctr from show_table_watermark as a left join click_table_watermark as b on a.ad_id = b.ad_id and a.user_id = b.user_id \\"\\n    }\\n    \\n}\\n\\n#\u63a5\u4e0b\u6765\u6211\u4eec\u9009\u62e9\u5c06\u7ed3\u679c\u5b9e\u65f6\u8f93\u51fa\u5230Kafka\\noutput {\\n    kafka {\\n        topic = \\"seatunnel\\"\\n        producer.bootstrap.servers = \\"localhost:9092\\"\\n        streaming_output_mode = \\"append\\" #\u6d41\u5173\u8054\u53ea\u652f\u6301append\u6a21\u5f0f\\n        checkpointLocation = \\"/your/path\\"\\n    }\\n}\\n```\\n\\n\u901a\u8fc7\u914d\u7f6e\uff0c\u5230\u8fd9\u91cc\u6d41\u5173\u8054\u7684\u6848\u4f8b\u4e5f\u5b8c\u6210\u4e86\u3002\\n\\n### \u7ed3\u8bed\\n\u901a\u8fc7\u914d\u7f6e\u80fd\u5f88\u5feb\u7684\u5229\u7528StructuredStreaming\u505a\u5b9e\u65f6\u6570\u636e\u5904\u7406\uff0c\u4f46\u662f\u8fd8\u662f\u9700\u8981\u5bf9StructuredStreaming\u7684\u4e00\u4e9b\u6982\u5ff5\u4e86\u89e3\uff0c\u6bd4\u5982\u5176\u4e2d\u7684watermark\u673a\u5236\uff0c\u8fd8\u6709\u7a0b\u5e8f\u7684\u8f93\u51fa\u6a21\u5f0f\u3002\\n\\n\u6700\u540e\uff0cSeatunnel \u5f53\u7136\u8fd8\u652f\u6301spark streaming\u548cspark \u6279\u5904\u7406\u3002\\n\u5982\u679c\u4f60\u5bf9\u8fd9\u4e24\u4e2a\u4e5f\u611f\u5174\u8da3\u7684\u8bdd\uff0c\u53ef\u4ee5\u9605\u8bfb\u6211\u4eec\u4ee5\u524d\u53d1\u5e03\u7684\u6587\u7ae0\u300a[\u5982\u4f55\u5feb\u901f\u5730\u5c06Hive\u4e2d\u7684\u6570\u636e\u5bfc\u5165ClickHouse](i18n/zh-CN/docusaurus-plugin-content-blog/current/2021-12-30-hive-to-clickhouse.mdtent-blog/current/2021-12-30-hive-to-clickhouse.md)\u300b\u3001\\n\u300a[\u4f18\u79c0\u7684\u6570\u636e\u5de5\u7a0b\u5e08\uff0c\u600e\u4e48\u7528Spark\u5728TiDB\u4e0a\u505aOLAP\u5206\u6790](i18n/zh-CN/docusaurus-plugin-content-blog/current/2021-12-30-spark-execute-tidb.mdtent-blog/current/2021-12-30-spark-execute-tidb.md)\u300b\u3001\\n\u300a[\u5982\u4f55\u4f7f\u7528Spark\u5feb\u901f\u5c06\u6570\u636e\u5199\u5165Elasticsearch](i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-spark-execute-elasticsearch.md/current/2021-12-30-spark-execute-elasticsearch.md)\u300b\\n\\n\u5e0c\u671b\u4e86\u89e3 Seatunnel \u548c HBase, ClickHouse\u3001Elasticsearch\u3001Kafka\u3001MySQL \u7b49\u6570\u636e\u6e90\u7ed3\u5408\u4f7f\u7528\u7684\u66f4\u591a\u529f\u80fd\u548c\u6848\u4f8b\uff0c\u53ef\u4ee5\u76f4\u63a5\u8fdb\u5165\u5b98\u7f51 [https://seatunnel.apache.org/](https://seatunnel.apache.org/)\\n\\n## \u8054\u7cfb\u6211\u4eec\\n* \u90ae\u4ef6\u5217\u8868 : **dev@seatunnel.apache.org**. \u53d1\u9001\u4efb\u610f\u5185\u5bb9\u81f3 `dev-subscribe@seatunnel.apache.org`\uff0c \u6309\u7167\u56de\u590d\u8ba2\u9605\u90ae\u4ef6\u5217\u8868\u3002\\n* Slack: \u53d1\u9001 `Request to join SeaTunnel slack` \u90ae\u4ef6\u5230\u90ae\u4ef6\u5217\u8868 (`dev@seatunnel.apache.org`), \u6211\u4eec\u4f1a\u9080\u8bf7\u4f60\u52a0\u5165\uff08\u5728\u6b64\u4e4b\u524d\u8bf7\u786e\u8ba4\u5df2\u7ecf\u6ce8\u518cSlack\uff09.\\n* [bilibili B\u7ad9 \u89c6\u9891](https://space.bilibili.com/1542095008)"}]}')}}]);