<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Apache SeaTunnel Blog</title>
        <link>https://seatunnel.apache.org/zh-CN/blog</link>
        <description>Apache SeaTunnel Blog</description>
        <lastBuildDate>Fri, 31 Mar 2023 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>zh-CN</language>
        <item>
            <title><![CDATA[SeaTunnel 2.3.1 is released! The refactored AI Compatible feature allows ChatGPT to automatically generate Connector code]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get</guid>
            <pubDate>Fri, 31 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[SeaTunnel version 2.3.1 was released recently. This is a high-profile release with many important function updates and optimizations.]]></description>
            <content:encoded><![CDATA[<blockquote><p><em>SeaTunnel version 2.3.1 was released recently. This is a high-profile release with many important function updates and optimizations.</em>
<em>At the level of programming user experience, the new version improves the stability of SeaTunnel Zeta and CI/CD; at the level of connectors, the new version implements 7+ new connectors and fixes existing commonly used connectors bugs, and improved security. The community refactored multiple underlying base classes and added an important feature, AI Compatible. With the optimized API, users can use ChatGPT 4.0 to quickly build the SaaS Connector they need.</em></p><h1>Major Feature update</h1></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-seatunnel-zeta">01 SeaTunnel Zeta<a href="#01-seatunnel-zeta" class="hash-link" aria-label="01 SeaTunnel Zeta的直接链接" title="01 SeaTunnel Zeta的直接链接">​</a></h2><p>The first version of the data integration engine-SeaTunnel Zeta is introduced in the SeaTunnel 2.3.0 release and has received feedback from numerous community users. In SeaTunnel version 2.3.1, we have fixed all the bugs reported by users, optimized the use of memory and threads, and greatly improved the stability of Zeta.</p><p>In version 2.3.1, the community also added several new Zeta features, including a dedicated JVM parameter configuration file, client output of job monitoring information, Rest API for Zeta cluster information and job information, etc.</p><p>At the checkpoint level, version 2.3.1 Zeta supports using OSS as checkpoint storage. It also supports savepoint running jobs and resuming jobs from savepoints.</p><p>In addition, version 2.3.1 also adds a set of Zeta’s Rest API, which can be used to obtain the list of jobs running on Zeta, the status information of jobs, and the monitoring indicators of Zeta cluster nodes. For specific usage methods, please refer to&nbsp;<strong>https:/ /seatunnel.apache.org/docs/seatunnel-engine/rest-api/</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-ai-compatible">02 AI Compatible<a href="#02-ai-compatible" class="hash-link" aria-label="02 AI Compatible的直接链接" title="02 AI Compatible的直接链接">​</a></h2><p>In SeaTunnel 2.3.1, the HTTP interface and related APIs are reconstructed, and the SaaS Connector-related API and Connector construction process are simplified according to the existing xGPT level capabilities so that ChatGPT 4.0 can directly generate SaaS Connectors and quickly generate various SaaS Connector interfaces. Under normal circumstances, the results obtained by this method are 95% similar to the code written by open-source contributors (see appendix).</p><p>Of course, because ChatGPT4.0 will be updated in October 2021, it is necessary to provide some latest vectorized documents for the latest SaaS interface adaptation to have the latest interface adaptation. However, this refactored API and code framework allows users to generate Connectors more quickly and contribute to the open-source community, making the SeaTunnel interface more powerful.</p><h1>Connector</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-7-new-connectors">01 7+ new connectors<a href="#01-7-new-connectors" class="hash-link" aria-label="01 7+ new connectors的直接链接" title="01 7+ new connectors的直接链接">​</a></h2><p>While fixing the bugs of known connectors and optimizing the connectors, the community has added 7 new connectors including SAP HANA, Persistiq, TDEngine, SelectDB Cloud, Hbase, FieldMapper Transform, and SimpleSQL Transform.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-reimplement-sql-transform">02 Reimplement SQL Transform<a href="#02-reimplement-sql-transform" class="hash-link" aria-label="02 Reimplement SQL Transform的直接链接" title="02 Reimplement SQL Transform的直接链接">​</a></h2><p>Since the previous SQL Transform connector was defined based on Flink SQL and Spark SQL, SQL Transform cannot adapt to the execution of multiple engines, so we removed the SQL Transform function in version 2.3.0. In version 2.3.1, we reimplemented SQL Transform. SQL Transform is an API that does not depend on a task-specific execution engine and can perfectly run on three different engines: Flink/Spark/Zeta. Special thanks to contributor Ma Chengyuan (GitHub ID: rewerma) for leading and contributing this important Feature.</p><p>For the functions already supported by SQL Transform, please refer to&nbsp;<a href="https://seatunnel.apache.org/docs/2.3.1/transform-v2/sql-functions" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.3.1/transform-v2/sql-functions</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-new-sql-server-cdc">03 New SQL Server CDC<a href="#03-new-sql-server-cdc" class="hash-link" aria-label="03 New SQL Server CDC的直接链接" title="03 New SQL Server CDC的直接链接">​</a></h2><p>At the CDC connector level, the community has newly added a SQL Server CDC connector, and made a lot of optimizations to MySQL CDC, improving the stability of MySQL CDC.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-added-cdc-connector-to-output-debezium-json-format-function">04 Added CDC connector to output debezium-json format function<a href="#04-added-cdc-connector-to-output-debezium-json-format-function" class="hash-link" aria-label="04 Added CDC connector to output debezium-json format function的直接链接" title="04 Added CDC connector to output debezium-json format function的直接链接">​</a></h2><p>In addition, version 2.3.1 also added the function of the CDC connector to output debezium-json format. Users can use MySQL CDC to read binlog and output data in debezium-json format to Kafka, so that users can create new synchronization tasks to read The data in debezium-json format in Kafka is synchronized to the target data source, or you can directly write other programs to read the data in debezium-json format in Kafka to perform some indicator calculations.</p><h1>Safety</h1><p>Before version 2.3.1, users need to configure the database username, password, and other information in plain text in the config file, which may cause some security problems. In version 2.3.1, we added the configuration file encryption function, and users can fill in the encrypted database username, password, and other information in the config file. When the job is running, SeaTunnel will decrypt the content in the config file based on the default encryption and decryption algorithm. At the same time, the encryption function provides SPI, by which users can customize the parameter list of encryption and decryption and the algorithm of encryption and decryption based on their own needs.</p><p>For how to use this function, please refer to&nbsp;<a href="https://seatunnel.apache.org/docs/2.3.1/connector-v2/Config-Encryption-Decryption" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.3.1/connector-v2/Config-Encryption-Decryption</a></p><h1>Third-party engine support</h1><p>SeaTunnel version 2.3.1 supports Spark version 3.3, as well as Flink 1.14.6, Flink 1.15, Flink 1.16, and other versions, basically covering the mainstream versions of Spark and Flink.</p><h1>Client</h1><p>The new version introduces an SPI for job configuration. Previously, only hocon json configuration files were supported. Now SPI is opened to the users to customize the format of job configuration files to meet different business system integration requirements.</p><h1>Optimization</h1><p>SeaTunnel 2.1.3 version has made many important optimizations, including changes in core components, connector components, CI/CD, Zeta(ST-Engine), and E2E components, involving updating new functions, improving existing functions, and optimizing tests and deployment processes. Some notable changes include adding parallelism and column projection interfaces in Core API, introducing MySQL-CDC source factory in Connector-V2 and supporting only-once semantics for JDBC source connectors, improving CI/CD process and stability for E2E In Zeta (ST-Engine), the logic of restarting the job when all nodes are down is added, and the timeout period for writing data is configurable.</p><p>For a detailed list, see the Release Note <!-- -->[Improve]<!-- --> section.</p><h1>Document</h1><p>In addition, the new version also has a series of updates to the documentation, including adding transform v2 documentation and some hints, as well as improving the documentation of various connectors.</p><p>See the Release Note <!-- -->[Docs]<!-- --> section for details.</p><p>Document address:&nbsp;<a href="https://seatunnel.apache.org/versions/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/versions/</a></p><h1>Release Note</h1><p><a href="https://github.com/apache/incubator-seatunnel/blob/2.3.1/release-note.md" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/blob/2.3.1/release-note.md</a></p><ul><li>Project address:&nbsp;<a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></li><li>Download address:&nbsp;<a href="https://seatunnel.apache.org/download" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/download</a></li></ul><h1>Acknowledgement to the contributors</h1><p><img loading="lazy" alt="contributors" src="/zh-CN/assets/images/contributors-d048b07cc9a14c93d2d0ac41bdaae446.png" width="687" height="712" class="img_ev3q"></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Performance Test Report: SeaTunnel Synchronizes data in batches 420% Faster than GLUE!]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md</guid>
            <pubDate>Wed, 29 Mar 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[cover]]></description>
            <content:encoded><![CDATA[<p>cover</p><p>SeaTunnel Zeta has been officially released with the joint efforts of the community. After comparing the performance of SeaTunnel with DataX and Airbyte, we also compared the performance of SeaTunnel with the popular data synchronization tool AWS GLUE.</p><p>The results showed that SeaTunnel batch syncs MySQL data to MySQL 420% faster than GLUE.</p><p>To ensure the accuracy of the test, we took on the test under the same test environment: under the same resource conditions, we tested SeaTunnel and AWS GLUE to synchronize data from MySQL to MySQL in batches and compared the time required for the two tools.</p><p><img loading="lazy" alt="1" src="/zh-CN/assets/images/1-d51ab80474d84a87dd8fbd61ab6f77bf.png" width="441" height="131" class="img_ev3q"></p><p>We created a table in MySQL containing 31 fields, with the primary key selected as an incrementing ID, and all other fields generated randomly, without setting any indexes. The table creation statement is as follows:</p><div class="language-plain codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-plain codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">create table test.type_source_table</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    id                   int auto_increment</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        primary key,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_binary             binary(64)          null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_blob               blob                null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_long_varbinary     mediumblob          null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_longblob           longblob            null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_tinyblob           tinyblob            null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_varbinary          varbinary(100)      null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_smallint           smallint            null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_smallint_unsigned  smallint unsigned   null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_mediumint          mediumint           null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_mediumint_unsigned mediumint unsigned  null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_int                int                 null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_int_unsigned       int unsigned        null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_integer            int                 null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_integer_unsigned   int unsigned        null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_bigint             bigint              null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_bigint_unsigned    bigint unsigned     null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_numeric            decimal             null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_decimal            decimal             null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_float              float               null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_double             double              null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_double_precision   double              null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_longtext           longtext            null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_mediumtext         mediumtext          null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_text               text                null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_tinytext           tinytext            null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_varchar            varchar(100)        null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_date               date                null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_datetime           datetime            null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_time               time                null,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    f_timestamp          timestamp           null</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">);</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>SeaTunnel Task Configuration</h1><p>In SeaTunnel, we split the data according to the ID field and process it in multiple sub-tasks. Here is the configuration file for SeaTunnel:</p><div class="language-plain codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-plain codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">env {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    job.mode = "BATCH"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    checkpoint.interval = 300000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">source {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Jdbc {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        url = "jdbc:mysql://XXX:3306/test"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        driver = "com.mysql.cj.jdbc.Driver"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        user = "root"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = "password"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        connection_check_timeout_sec = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        query = "select id, f_binary, f_blob, f_long_varbinary, f_longblob, f_tinyblob, f_varbinary, f_smallint, f_smallint_unsigned, f_mediumint, f_mediumint_unsigned, f_int, f_int_unsigned, f_integer, f_integer_unsigned, f_bigint, f_bigint_unsigned, f_numeric, f_decimal, f_float, f_double, f_double_precision, f_longtext, f_mediumtext, f_text, f_tinytext, f_varchar, f_date, f_datetime, f_time, f_timestamp from test"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        partition_column = "id"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        partition_num = 40</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        parallelism = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">sink {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Jdbc {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">          url = "jdbc:mysql://XXX:3306/test"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         driver = "com.mysql.cj.jdbc.Driver" </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        user = "root"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = "password"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">         query = "insert into test_1 values (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Under fixed JVM memory of 4G and parallelism of 2, SeaTunnel completed the synchronization in 1965 seconds. Based on this conclusion, we tested the speed of GLUE under the same memory and concurrency settings.</p><h1>GLUE Task Configuration</h1><p>We created a MySQL-to-MySQL job as follows:</p><p><img loading="lazy" alt="2" src="/zh-CN/assets/images/2-f1fd058ea26a46a8150a703b6905084a.png" width="1280" height="492" class="img_ev3q"></p><p>Configuration source connect with the target:</p><p><img loading="lazy" alt="3" src="/zh-CN/assets/images/3-37d8f5d74bdbe297c063376a9d54c8ef.png" width="720" height="931" class="img_ev3q"></p><p>Job configuration:</p><p><img loading="lazy" alt="4" src="/zh-CN/assets/images/4-7302311b5a6ac79323812042a38697d2.png" width="1102" height="720" class="img_ev3q"></p><p><img loading="lazy" alt="5" src="/zh-CN/assets/images/5-349cb5fddaeb1867b9b48922aec68a8e.png" width="1280" height="155" class="img_ev3q"></p><p>Adjust the memory: job parameters configuration</p><p><img loading="lazy" alt="6" src="/zh-CN/assets/images/6-1-4434213ed408f3b916af6bca195f442c.png" width="1280" height="231" class="img_ev3q"></p><p>— conf spark.yarn.executor.memory=4g</p><p>Under this configuration, GLUE took 8191 seconds to complete the synchronization.</p><h1>Conclusion</h1><p>After comparing the best configurations, we conducted a more in-depth comparison for different memory sizes. The following chart shows the comparison results obtained through repeated testing under the same environment.</p><p><img loading="lazy" alt="7" src="/zh-CN/assets/images/7-2d2a97ed15a9f1483a375c7f46b5ee77.png" width="958" height="720" class="img_ev3q"></p><p>The unit is seconds.</p><p><img loading="lazy" alt="8" src="/zh-CN/assets/images/8-67759c33a40c0cc49964474c4501db35.png" width="721" height="171" class="img_ev3q"></p><p>Note: This comparison is based on SeaTunnel: commit ID f57b897, and we welcome to download and test it!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SeaTunnel now supports CDC (Capture Change Data) writing by ClickHouse Connector!]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md</guid>
            <pubDate>Thu, 09 Feb 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[Written by Wang Hailin, Apache SeaTunnel PPMC]]></description>
            <content:encoded><![CDATA[<p>Written by Wang Hailin, Apache SeaTunnel PPMC</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="preface">Preface<a href="#preface" class="hash-link" aria-label="Preface的直接链接" title="Preface的直接链接">​</a></h2><p>Currently, SeaTunnel supports database change data capture (CDC <a href="https://github.com/apache/incubator-seatunnel/issues/3175" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/3175</a>), to transfer data changes to downstream systems in real time. SeaTunnel categorizes the captured data changes into the following 4 types: </p><ul><li>INSERT: Data insertion </li><li>UPDATE_BEFORE: Historical value before data change </li><li>UPDATE_AFTER: New value after data change </li><li>DELETE: Data deletion </li></ul><p>To handle the above data change operations, the Sink Connector needs to support writing behavior. This article will introduce how the ClickHouse Sink Connector supports writing these CDC types of data changes. </p><p>For CDC scenarios, the primary key is a necessary condition, so first, it needs to support the general requirements of INSERT, UPDATE, DELETE, etc. based on the primary key and ensure that the writing order is consistent with the CDC event order. In addition, considering the complexity of the data source in practice, it also needs to support UPSERT writing. Finally, according to the characteristics of ClickHouse itself, corresponding optimizations need to be made, such as UPDATE and DELETE being heavyweight operations in ClickHouse, which should be optimized based on the corresponding table engine's characteristics.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overall-design">Overall design<a href="#overall-design" class="hash-link" aria-label="Overall design的直接链接" title="Overall design的直接链接">​</a></h2><p>The current ClickHouse Sink Connector is based on the JDBC Driver implementation, and a group of JDBC executors can be designed to encapsulate the processing of different types of data, making it convenient to switch or combine implementations based on actual scenarios and encapsulate implementation details. </p><p>JdbcBatchStatementExecutor is the top-level interface of the executor.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public interface JdbcBatchStatementExecutor extends AutoCloseable {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    void prepareStatements(Connection connection) throws SQLException;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    void addToBatch(SeaTunnelRow record) throws SQLException;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    void executeBatch() throws SQLException;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    void closeStatements() throws SQLException;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    default void close() throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        closeStatements();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><code>JdbcBatchStatementExecutor</code> has the following implementation classes: </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">SimpleBatchStatementExecutor // implements simple SQL Batch execution logic </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">InsertOrUpdateBatchStatementExecutor // implements INSERT, UPDATE update, also supports UPSERT mode </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">ReduceBufferedBatchStatementExecutor // memory accumulation, when refreshing to the database, the data change type (INSERT, UPDATE, DELETE) is distributed to the specific execution executor </span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="handling-of-cases-where-the-primary-key-is-not-specified">Handling of cases where the primary key is not specified<a href="#handling-of-cases-where-the-primary-key-is-not-specified" class="hash-link" aria-label="Handling of cases where the primary key is not specified的直接链接" title="Handling of cases where the primary key is not specified的直接链接">​</a></h3><p>Currently, in CDC processing, the primary key is a necessary condition. If the Sink Connector is not specified in the primary key column configuration, it uses the append-only mode to write, calling <code>SimpleBatchStatementExecutor</code> directly.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cdc-data-process">CDC data process<a href="#cdc-data-process" class="hash-link" aria-label="CDC data process的直接链接" title="CDC data process的直接链接">​</a></h3><p>We divide the execution logic of data processing as follows: different data types enter the corresponding Executor and are finally transformed into their respective SQL statements for execution, and Jdbc Batch batching is used during this process.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CDC Event</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">               /         \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              /           \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">             /             \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            /               \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    DELETE Executor   INSERT OR UPDATE Executor</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                            /          \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                           /            \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                          /              \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                         /                \</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                     INSERT Executor    UPDATE Executor</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="maintaining-the-order-of-cdc-data">Maintaining the Order of CDC Data<a href="#maintaining-the-order-of-cdc-data" class="hash-link" aria-label="Maintaining the Order of CDC Data的直接链接" title="Maintaining the Order of CDC Data的直接链接">​</a></h3><p>CDC events are ordered, and writing must be processed in the order in which the events occur, otherwise data inconsistencies may occur. </p><p>In the previous logic, data of different types were distributed to their respective Executors and Jdbc Batch was used for batch submission to improve write performance, but categorizing batching can result in the order of submissions not being consistent with the CDC event order.</p><p>We can add an execution barrier marker, when the processed data row is of the same type as the previous data row, it can be batched, if not, the previous batch is first flushed to the database, ensuring that the data write order is strictly consistent with the CDC event order.</p><p>Example for <code>InsertOrUpdateBatchStatementExecutor</code></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public class InsertOrUpdateBatchStatementExecutor implements JdbcBatchStatementExecutor {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    public void addToBatch(SeaTunnelRow record) throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        boolean currentChangeFlag = hasInsert(record);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if (currentChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            if (preChangeFlag != null &amp;&amp; !preChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                updateStatement.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                updateStatement.clearBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            valueRowConverter.toExternal(record, insertStatement);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            insertStatement.addBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        } else {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            if (preChangeFlag != null &amp;&amp; preChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                insertStatement.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                insertStatement.clearBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            valueRowConverter.toExternal(record, updateStatement);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            updateStatement.addBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        preChangeFlag = currentChangeFlag;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        submitted = false;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    public void executeBatch() throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if (preChangeFlag != null) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            if (preChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                insertStatement.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                insertStatement.clearBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            } else {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                updateStatement.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                updateStatement.clearBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        submitted = true;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Of course, this will significantly slow down the batch processing, so we use <code>ReduceBufferedBatchStatementExecutor</code>to add a memory buffer layer, and when executing batch submissions, we distribute submissions to the database.</p><p>Example for <code>ReduceBufferedBatchStatementExecutor</code></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public class ReduceBufferedBatchStatementExecutor implements JdbcBatchStatementExecutor {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    private final LinkedHashMap&lt;SeaTunnelRow, Pair&lt;Boolean, SeaTunnelRow&gt;&gt; buffer = new LinkedHashMap&lt;&gt;();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    public void addToBatch(SeaTunnelRow record) throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        buffer.put(record, ...);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    public void executeBatch() throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        Boolean preChangeFlag = null;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        Set&lt;Map.Entry&lt;SeaTunnelRow, Pair&lt;Boolean, SeaTunnelRow&gt;&gt;&gt; entrySet = buffer.entrySet();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        for (Map.Entry&lt;SeaTunnelRow, Pair&lt;Boolean, SeaTunnelRow&gt;&gt; entry : entrySet) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            Boolean currentChangeFlag = entry.getValue().getKey();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            if (currentChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                if (preChangeFlag != null &amp;&amp; !preChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                    deleteExecutor.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                insertOrUpdateExecutor.addToBatch(entry.getValue().getValue());</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            } else {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                if (preChangeFlag != null &amp;&amp; preChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                    insertOrUpdateExecutor.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                deleteExecutor.addToBatch(entry.getKey());</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            preChangeFlag = currentChangeFlag;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if (preChangeFlag != null) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            if (preChangeFlag) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                insertOrUpdateExecutor.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            } else {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                deleteExecutor.executeBatch();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        buffer.clear();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="implementing-a-general-upsert-write">Implementing a General UPSERT Write<a href="#implementing-a-general-upsert-write" class="hash-link" aria-label="Implementing a General UPSERT Write的直接链接" title="Implementing a General UPSERT Write的直接链接">​</a></h3><p>In <code>InsertOrUpdateBatchStatementExecutor</code>, you can configure to turn on UPSERT, when processing INSERT or UPDATE data types, it will first use the primary key to query the data row to see if it already exists and then decide to use INSERT or UPDATE SQL for writing. </p><p><em>Note: This configuration is optional and will slow down the write speed, only opens when certain special scenarios are required.</em></p><p>Example for <code>InsertOrUpdateBatchStatementExecutor</code></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">public class InsertOrUpdateBatchStatementExecutor implements JdbcBatchStatementExecutor {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    @Override</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    public void addToBatch(SeaTunnelRow record) throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        boolean currentChangeFlag = hasInsert(record);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">      ...</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    private boolean hasInsert(SeaTunnelRow record) throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        if (upsertMode()) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            return !exist(keyExtractor.apply(record));</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        switch (record.getRowKind()) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            case INSERT:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                return true;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            case UPDATE_AFTER:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                return false;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            default:</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                throw new UnsupportedOperationException();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    private boolean exist(SeaTunnelRow pk) throws SQLException {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        keyRowConverter.toExternal(pk, existStatement);</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        try (ResultSet resultSet = existStatement.executeQuery()) {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">            return resultSet.next();</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimizing-upsert-for-replacingmergetree-engine">Optimizing UPSERT for ReplacingMergeTree Engine<a href="#optimizing-upsert-for-replacingmergetree-engine" class="hash-link" aria-label="Optimizing UPSERT for ReplacingMergeTree Engine的直接链接" title="Optimizing UPSERT for ReplacingMergeTree Engine的直接链接">​</a></h3><p>The <code>ReplacingMergeTree</code> table engine can configure an <code>ORDER BY</code> field, and when executing the INSERT INTO statement, it covers the records with the same ORDER BY field. We can also utilize this feature to implement UPSERT.</p><p>When the user writes to the <code>ReplacingMergeTree</code> table engine and the table's <code>ORDER BY</code> field is the same as the primary key field configured in the Sink Connector, both INSERT/UPDATE_AFTER data types are processed as INSERT to implement UPSERT.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimizing-updates-for-the-mergetree-engine">Optimizing Updates for the MergeTree Engine<a href="#optimizing-updates-for-the-mergetree-engine" class="hash-link" aria-label="Optimizing Updates for the MergeTree Engine的直接链接" title="Optimizing Updates for the MergeTree Engine的直接链接">​</a></h3><p>DELETE and UPDATE are heavyweight operations in ClickHouse, but there is an experimental lightweight deletion (<a href="https://clickhouse.com/docs/en/sql-reference/statements/delete" target="_blank" rel="noopener noreferrer">https://clickhouse.com/docs/en/sql-reference/statements/delete</a>) for <code>MergeTree</code> engine, which performs better than the heavyweight deletion. We allow the user to configure the lightweight deletion.</p><p>When the user writes to the <code>MergeTree</code> table engine and enables the lightweight deletion, we treat both DELETE/UPDATE_BEFORE data types as lightweight deletions, and treat both INSERT/UPDATE_AFTER data types as INSERTs, avoiding the UPDATE operation and using the lightweight deletion.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="related-pr">Related PR<a href="#related-pr" class="hash-link" aria-label="Related PR的直接链接" title="Related PR的直接链接">​</a></h2><ul><li><a href="https://github.com/apache/incubator-seatunnel/pull/3653" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/3653</a> </li></ul><p>Contribution to improving the related functions is welcomed, if you have any questions, please raise an issue on SeaTunnel GitHub (<a href="https://www.github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://www.github.com/apache/incubator-seatunnel</a>), and we will reply as soon as possible.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reference">Reference<a href="#reference" class="hash-link" aria-label="Reference的直接链接" title="Reference的直接链接">​</a></h2><ul><li><a href="https://clickhouse.com/docs/en/sql-reference/statements/delete" target="_blank" rel="noopener noreferrer">https://clickhouse.com/docs/en/sql-reference/statements/delete</a> </li><li><a href="https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree" target="_blank" rel="noopener noreferrer">https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree</a></li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[In the recently released SeaTunnel 2.3.0 official version]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!</guid>
            <pubDate>Tue, 10 Jan 2023 00:00:00 GMT</pubDate>
            <description><![CDATA[In the recently released SeaTunnel 2.3.0 official version, the community self-developed engine SeaTunnel Zeta which has been under preparation for more than a year——is officially released, and it will be used as the default engine of SeaTunnel in the future, providing users with high throughput, low latency, reliable consistent synchronization job operation guarantee.]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="/zh-CN/assets/images/16733443077196-048cdae0ce9892c195e1c6ca1374cfc5.png" width="900" height="383" class="img_ev3q"></p><p>In the recently released SeaTunnel 2.3.0 official version, the community self-developed engine SeaTunnel Zeta which has been under preparation for more than a year——is officially released, and it will be used as the default engine of SeaTunnel in the future, providing users with high throughput, low latency, reliable consistent synchronization job operation guarantee.</p><p>Why does SeaTunnel develop its synchronization engine? What is the positioning of the SeaTunnel Engine? How is it different from traditional computing engines? What is the design idea? What is unique about the architectural design? These questions will be answered in this article.</p><ul><li>Why develop our engine</li><li>SeaTunnel Engine Positioning</li><li>Design ideas</li><li>Architecture design</li><li>Unique advantages and features</li><li>Current basic functions and features</li><li>Future optimization plan</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-why-develop-our-engine">01 Why develop our engine<a href="#01-why-develop-our-engine" class="hash-link" aria-label="01 Why develop our engine的直接链接" title="01 Why develop our engine的直接链接">​</a></h2><p>It was a year ago that the SeaTunnel community publicly stated for the first time that it would develop its engine. The reason why the team decided to develop a self-developed engine was that SeaTunnel's connector can run only on Flink or Spark, and Flink and Spark, as computing engines, have many unsolvable problems when integrating and synchronizing data.</p><p>Refer to:
Why do we self-develop the big data synchronization engine SeaTunnel Zeta?
<a href="https://github.com/apache/incubator-seatunnel/issues/1954" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/1954</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-design-ideas">02 Design ideas<a href="#02-design-ideas" class="hash-link" aria-label="02 Design ideas的直接链接" title="02 Design ideas的直接链接">​</a></h2><p>The general idea of engine design is as follows:</p><ol><li>Simple and easy to use, the new engine minimizes the dependence on third-party services, and can realize cluster management, snapshot storage, and cluster HA functions without relying on big data components such as Zookeeper and HDFS. This is very useful for users who do not have a big data platform or are unwilling to rely on a big data platform for data synchronization.</li><li>More resource-saving, at the CPU level, Zeta Engine internally uses Dynamic Thread Sharing (dynamic thread sharing) technology. In the real-time synchronization scenario, if the number of tables is large but the amount of data in each table is small, Zeta Engine will Synchronous tasks run in shared threads, which can reduce unnecessary thread creation and save system resources. On the read and data write side, the Zeta Engine is designed to minimize the number of JDBC connections. In the CDC scenario, Zeta Engine will try to reuse log reading and parsing resources as much as possible.</li><li>More stable. In this version, Zeta Engine uses Pipeline as the minimum granularity of Checkpoint and fault tolerance for data synchronization tasks. The failure of a task will only affect the tasks that have upstream and downstream relationships with it. Try to avoid task failures that cause the entire Job to fail. or rollback. At the same time, for scenarios where the source data has a storage time limit, Zeta Engine supports enabling data cache to automatically cache the data read from the source, and then the downstream tasks read the cached data and write it to the target. In this scenario, even if the target end fails and data cannot be written, it will not affect the normal reading of the source end, preventing the source end data from being deleted due to expiration.</li><li>Faster, Zeta Engine’s execution plan optimizer will optimize the execution plan to reduce the possible network transmission of data, thereby reducing the loss of overall synchronization performance caused by data serialization and deserialization, and completing faster Data synchronization operations. Of course, it also supports speed limiting, so that sync jobs can be performed at a reasonable speed.</li><li>Data synchronization support for all scenarios. SeaTunnel aims to support full synchronization and incremental synchronization under offline batch synchronization, and support real-time synchronization and CDC.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-architecture-design">03 Architecture design<a href="#03-architecture-design" class="hash-link" aria-label="03 Architecture design的直接链接" title="03 Architecture design的直接链接">​</a></h2><p>SeaTunnel Engine is mainly composed of a set of APIs for data synchronization processing and a core computing engine. Here we mainly introduce the architecture design of the SeaTunnel Engine core engine.
<img loading="lazy" src="/zh-CN/assets/images/16733443263288-1764bbf249c027947d7c6d39cbf41202.png" width="2342" height="1182" class="img_ev3q">
picture</p><p>SeaTunnel Engine consists of three main services: <strong>CoordinatorService, TaskExecutionService, and SlotService.</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="coordinator-service">Coordinator Service<a href="#coordinator-service" class="hash-link" aria-label="Coordinator Service的直接链接" title="Coordinator Service的直接链接">​</a></h3><p>CoordinatorService is the Master service of the cluster, which provides the generation process of each job from LogicalDag to ExecutionDag, and then to PhysicalDag, and finally creates the JobMaster of the job for scheduling execution and status monitoring of the job. CoordinatorService is mainly composed of 4 large functional modules:</p><ol><li>JobMaster is responsible for the generation process from LogicalDag to ExecutionDag to PhysicalDag of a single job, and is scheduled to run by PipelineBaseScheduler.</li><li>CheckpointCoordinator, responsible for the Checkpoint process control of the job.</li><li>ResourceManager is responsible for the application and management of job resources. It currently supports Standalone mode and will support On Yarn and On K8s in the future.</li><li>Metrics Service, responsible for the statistics and summary of job monitoring information.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="taskexecutionservice">TaskExecutionService<a href="#taskexecutionservice" class="hash-link" aria-label="TaskExecutionService的直接链接" title="TaskExecutionService的直接链接">​</a></h3><p>TaskExecutionService is the Worker service of the cluster, which provides the real runtime environment of each Task in the job. TaskExecutionService uses Dynamic Thread Sharing technology to reduce CPU usage.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="slotservice">SlotService<a href="#slotservice" class="hash-link" aria-label="SlotService的直接链接" title="SlotService的直接链接">​</a></h3><p>SlotService runs on each node of the cluster and is mainly responsible for the division, application, and recycling of resources on the node.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-unique-advantages-and-features">04 Unique advantages and features<a href="#04-unique-advantages-and-features" class="hash-link" aria-label="04 Unique advantages and features的直接链接" title="04 Unique advantages and features的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="autonomous-cluster">Autonomous cluster<a href="#autonomous-cluster" class="hash-link" aria-label="Autonomous cluster的直接链接" title="Autonomous cluster的直接链接">​</a></h3><p>SeaTunnel Engine has realized autonomous clustering (no centralization). To achieve cluster autonomy and job fault tolerance without relying on third-party service components (such as Zookeeper), SeaTunnel Engine uses Hazelcast as the underlying dependency. Hazelcast provides a distributed memory network, allowing users to operate a distributed collection like a normal Java collection locally. SeaTunnel saves the status information of the job in the memory grid of Hazelcast. When the Master node switches, it can Job state recovery based on data in the Hazelcast in-memory grid. At the same time, we have also implemented the persistence of Hazelcast memory grid data, and persisted the job status information to the storage (database of JDBC protocol, HDFS, cloud storage) in the form of WAL. In this way, even if the entire cluster hangs and restarts, the runtime information of the job can be repaired.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-cache">Data cache<a href="#data-cache" class="hash-link" aria-label="Data cache的直接链接" title="Data cache的直接链接">​</a></h3><p>SeaTunnel Engine is different from the traditional Spark/Flink computing engine, it is an engine specially used for data synchronization. The SeaTunnel engine naturally supports data cache. When multiple synchronous jobs in the cluster share a data source, the SeaTunnel engine will automatically enable the data cache. The source of a job will read the data and write it into the cache, and all other jobs will no longer read data from the data source but are automatically optimized to read data from the Cache. The advantage of this is that it can reduce the reading pressure of the data source and reduce the impact of data synchronization on the data source.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="speed-control">Speed control<a href="#speed-control" class="hash-link" aria-label="Speed control的直接链接" title="Speed control的直接链接">​</a></h3><p>SeaTunnel Engine supports the speed limit during data synchronization, which is very useful when reading data sources with high concurrency. A reasonable speed limit can not only ensure that the data is synchronized on time, but also minimize the pressure on the data source.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shared-connection-pool-to-reduce-database-pressure">Shared connection pool to reduce database pressure<a href="#shared-connection-pool-to-reduce-database-pressure" class="hash-link" aria-label="Shared connection pool to reduce database pressure的直接链接" title="Shared connection pool to reduce database pressure的直接链接">​</a></h3><p>At present, the underlying operating tools and data synchronization tools provided by computing engines such as Spark/Flink cannot solve the problem that each table needs a JDBC connection when the entire database is synchronized. Database connections are resources for the database. Too many database connections will put great pressure on the database, resulting in a decrease in the stability of database read and write delays. This is a very serious accident for business databases. To solve this problem, SeaTunnel Engine uses a shared connection pool to ensure that multiple tables can share JDBC connections, thereby reducing the use of database connections.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="breakpoint-resume-incrementalfull-volume">Breakpoint resume (incremental/full volume)<a href="#breakpoint-resume-incrementalfull-volume" class="hash-link" aria-label="Breakpoint resume (incremental/full volume)的直接链接" title="Breakpoint resume (incremental/full volume)的直接链接">​</a></h3><p>SeaTunnel Engine supports resumed uploads under offline synchronization. When the amount of data is large, a data synchronization job often needs to run for tens of minutes or several hours. If the middle job hangs up and reruns, it means wasting time. SeaTunnel Engine will continue to save the state (checkpoint) during the offline synchronization process. When the job hangs up and reruns, it will continue to run from the last checkpoint, which effectively solves the data that may be caused by hardware problems such as node downtime. Delay.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-schema-revolution-route">The Schema revolution route<a href="#the-schema-revolution-route" class="hash-link" aria-label="The Schema revolution route的直接链接" title="The Schema revolution route的直接链接">​</a></h3><p>Schema evolution is a feature that allows users to easily change a table's current schema to accommodate data that changes over time. Most commonly, it is used when performing an append or overwrite operation, to automatically adjust the schema to include one or more new columns.</p><p>This capability is required in real-time data warehouse scenarios. Currently, the Flink and Spark engines do not support this feature.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-grained-fault-tolerant-design">Fine-grained fault-tolerant design<a href="#fine-grained-fault-tolerant-design" class="hash-link" aria-label="Fine-grained fault-tolerant design的直接链接" title="Fine-grained fault-tolerant design的直接链接">​</a></h3><p>Flink's design is fault tolerance and rollback at the entire job level. If a task fails, the entire job will be rolled back and restarted. The design of SeaTunnel Engine takes into account that in the data synchronization scenario, in many q cases, the failure of a task should only need to focus on fault tolerance for tasks that have upstream and downstream relationships with it. Based on this design principle, SeaTunnel Engine will first generate a logical DAG according to the user-configured job configuration file, then optimize the logical DAG, and finally generate a pipeline (a connected subgraph in a job DAG) to call and execute jobs at the granularity. fault tolerance.</p><p>A typical usage scenario is:</p><p>Use the CDC connector to read data from MySQL's binlog and write it to another MySQL. If you use Flink or Spark engine, once the target MySQL cannot write, it will cause the task of CDC to read the binlog to be terminated. If MySQL is set If the expiration time of the log is set, the problem of the target MySQL is solved, but the log of the source MySQL is cleared, which leads to data loss and other problems.</p><p>SeaTunnel Engine will automatically optimize this synchronization task, automatically add the source to the target Cache, and then further optimize this job into two Pipelines, pipeline#1 is responsible for reading data from the CDC and writing it to the SeaTunnel Cache, and pipeline#2 is responsible for reading data from the SeaTunnel Cache Cache reads data and writes to target MySQL. If there is a problem with the target MySQL and cannot be written, the pipeline#2 of this synchronization job will be terminated, and the pipeline#1 will still run normally. This design fundamentally solves the above problems and is more in line with the processing logic of the data synchronization engine.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamically-share-threads-to-reduce-resource-usage">Dynamically share threads to reduce resource usage<a href="#dynamically-share-threads-to-reduce-resource-usage" class="hash-link" aria-label="Dynamically share threads to reduce resource usage的直接链接" title="Dynamically share threads to reduce resource usage的直接链接">​</a></h3><p>SeaTunnel Engine's Task design uses shared thread technology. Different from Flink/Spark, SeaTunnel Engine does not simply allow a Task to occupy a thread, but through a dynamic perception method - Dynamic Thread Sharing (Dynamic Thread Sharing) To judge whether a Task should share a thread with other Tasks or should monopolize a thread.</p><p>Compared with single-threaded serial computing, multi-threaded parallel computing has better performance advantages, but if each Task uses an independent thread to run, when there are many tables for data synchronization and the number of Tasks is large, it will be in the Worker node Start very many threads on it. When the number of CPU cores is fixed, the more threads, the better. When the number of threads is too large, the CPU needs to spend a lot of time on thread context switching, which will affect computing performance.</p><p>Flink/Spark usually limits the maximum number of tasks running on each node. In this way, it can avoid starting too many threads. To run more tasks on one node, SeaTunnel Engine can share thread technology. Let those tasks with a small amount of data share threads, and tasks with a large amount of data exclusively use threads. This method makes it possible for SeaTunnel Engine to run hundreds or thousands of table synchronization tasks on one node, with less resource occupation. Complete the synchronization of more tables.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-basic-functions-and-features">05 Basic functions and features<a href="#05-basic-functions-and-features" class="hash-link" aria-label="05 Basic functions and features的直接链接" title="05 Basic functions and features的直接链接">​</a></h2><p>2.3.0 is the first official version of SeaTunnel Engine, which implements some basic functions. For the detailed design, please refer to: <a href="https://github.com/apache/incubator-seatunnel/issues/2272" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/2272</a></p><p><strong>[ Cluster Management ]</strong></p><ul><li>Support stand-alone operation</li><li>Support cluster operation</li><li>Autonomous cluster (no centralization), no need to specify a Master node for the SeaTunnel Engine cluster, SeaTunnel Engine elects the Master node by itself during operation and automatically selects a new Master node after the Master node hangs up.</li><li>Automatic discovery of cluster nodes, the nodes with the same cluster_name will automatically form a cluster.</li></ul><p><strong>[ Core function ]</strong></p><ul><li>Supports running jobs in Local mode. The cluster is automatically destroyed after the job runs.</li><li>It supports running jobs in Cluster mode (single machine or cluster) and submitting jobs to the SeaTunnel Engine service through SeaTunnel Client. After the job is completed, the service continues to run and waits for the next job submission.</li><li>Support offline batch synchronization.</li><li>Support real-time synchronization.</li><li>Batch and flow integration, all SeaTunnel V2 version connectors can run in SeaTunnel Engine.</li><li>Supports distributed snapshot algorithm cooperates with SeaTunnel V2 connector to support two-phase commit, and ensures data exactly-once.</li><li>Supports job invocation at the Pipeline level to ensure that it can be started even when resources are limited.</li><li>Supports job fault tolerance at the Pipeline level. The failure of a Task only affects the Pipeline it is in, and only the Task under the Pipeline needs to be rolled back.</li><li>Supports dynamic thread sharing to achieve real-time synchronization of a large number of small data sets.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="06-future-optimization-plan">06 Future optimization plan<a href="#06-future-optimization-plan" class="hash-link" aria-label="06 Future optimization plan的直接链接" title="06 Future optimization plan的直接链接">​</a></h2><ul><li>Support Cache mode, and first support Kafka as Cache</li><li>Support JobHistory, support the persistence of JobHistory.</li><li>Support indicator (Reader Rows, QPS, Reader Bytes) monitoring and indicator query</li><li>Support dynamic modification of the execution plan.</li><li>Support CDC.</li><li>Support whole database synchronization</li><li>Support multi-table synchronization</li><li>Support for Schema Revolution</li></ul>]]></content:encoded>
            <category>Meetup</category>
        </item>
        <item>
            <title><![CDATA[SeaTunnel supports IoTDB to implement IoT data synchronization]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection</guid>
            <pubDate>Sat, 10 Dec 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache IoTDB (Internet of Things Database) is a software system that integrates the collection, storage, management, and analysis of time series data of the Internet of Things, which can meet the needs of massive data storage, high-speed data reading, and complex data analysis in the field of Industrial Internet of Things. Currently, SeaTunnel already supports IoTDB Connector, realizing the connection of data synchronization scenarios in the IoT field.]]></description>
            <category>Meetup</category>
        </item>
        <item>
            <title><![CDATA[SeaTunnel engine, designed for tens-of-billions data integration]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/Apache SeaTunnel Committer | Zongwen Li</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/Apache SeaTunnel Committer | Zongwen Li</guid>
            <pubDate>Fri, 09 Dec 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache SeaTunnel Committer | Zongwen Li]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="/zh-CN/assets/images/16714309876928-859e41ec15f205a23a1b25d8a2b80046.jpg" width="720" height="306" class="img_ev3q">
Apache SeaTunnel Committer | Zongwen Li</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-to-apache-seatunnel">Introduction to Apache SeaTunnel<a href="#introduction-to-apache-seatunnel" class="hash-link" aria-label="Introduction to Apache SeaTunnel的直接链接" title="Introduction to Apache SeaTunnel的直接链接">​</a></h2><p>Apache SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data.</p><p>Apache SeaTunnel will try its best to solve the problems that may be encountered in the process of mass data synchronization, such as data loss and duplication, task accumulation and delay, low throughput, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="milestones-of-seatunnel">Milestones of SeaTunnel<a href="#milestones-of-seatunnel" class="hash-link" aria-label="Milestones of SeaTunnel的直接链接" title="Milestones of SeaTunnel的直接链接">​</a></h2><p>SeaTunnel, formerly known as Waterdrop, was open-sourced on GitHub in 2017.</p><p>In October 2021, the Waterdrop community joined the Apache incubator and changed its name to SeaTunnel.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-growth">SeaTunnel Growth<a href="#seatunnel-growth" class="hash-link" aria-label="SeaTunnel Growth的直接链接" title="SeaTunnel Growth的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714310892722-67ccd10c8545d757fc1dab589404be3d.jpg" width="720" height="469" class="img_ev3q">
<img loading="lazy" src="/zh-CN/assets/images/16714310916195-8701b0e12c30eae7063696ec13464375.jpg" width="720" height="378" class="img_ev3q">
<img loading="lazy" src="/zh-CN/assets/images/16714310939883-8df5afa031f552b198757dc4dbe0bf70.jpg" width="720" height="392" class="img_ev3q">
When SeaTunnel entered the Apache incubator, the SeaTunnel community ushered in rapid growth.</p><p>As of now, the SeaTunnel community has a total of 151 contributors, 4314 Stars, and 804 forks.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pain-points-of-existing-engines">Pain points of Existing engines<a href="#pain-points-of-existing-engines" class="hash-link" aria-label="Pain points of Existing engines的直接链接" title="Pain points of Existing engines的直接链接">​</a></h2><p>There are many pain points faced by the existing computing engines in the field of data integration, and we will talk about this first. The pain points usually lie in three directions:</p><ul><li>The fault tolerance ability of the engine;</li><li>Difficulty in configuration, operation, and maintenance of engine jobs;</li><li>The resource usage of the engine.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="fault-tolerance">fault tolerance<a href="#fault-tolerance" class="hash-link" aria-label="fault tolerance的直接链接" title="fault tolerance的直接链接">​</a></h2><p>Global Failover
<img loading="lazy" alt="Global-failover" src="/zh-CN/assets/images/16714311670656-14b472dd8abdeff2639c1701ed9997f9.jpg" width="720" height="176" class="img_ev3q">
For distributed streaming processing systems, high throughput and low latency are often the most important requirements. At the same time, fault tolerance is also very important in distributed systems. For scenarios that require high correctness, the implementation of exactly once is often very important.</p><p>In a distributed streaming processing system, since the computing power, network, load, etc. of each node are different, the state of each node cannot be directly merged to obtain a true global state. To obtain consistent results, the distributed processing system needs to be resilient to node failure, that is, it can recover to consistent results when it fails.</p><p>Although it is claimed in their official blog that Spark’s Structured Streaming uses the Chandy-Lamport algorithm for Failover processing, it does not disclose more details.</p><p>Flink implemented Checkpoint as a fault-tolerant mechanism based on the above algorithm and published related papers: Lightweight Asynchronous Snapshots for Distributed Dataflows</p><p>In the current industrial implementation, when a job fails, all nodes of the job DAG need to failover, and the whole process will last for a long time, which will cause a lot of upstream data to accumulate.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="loss-of-data">Loss of Data<a href="#loss-of-data" class="hash-link" aria-label="Loss of Data的直接链接" title="Loss of Data的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714312426416-f778eaa67ecf527449353404cf0bf421.jpg" width="720" height="175" class="img_ev3q">
The previous problem will cause a long-time recovery, and the business service may accept a certain degree of data delay.</p><p>In a worse case, a single sink node cannot be recovered for a long time, and the source data has a limited storage time, such as MySQL and Oracle log data, which will lead to data loss.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="configuration-is-cumbersome">Configuration is cumbersome<a href="#configuration-is-cumbersome" class="hash-link" aria-label="Configuration is cumbersome的直接链接" title="Configuration is cumbersome的直接链接">​</a></h2><p>Single table Configuration</p><p><img loading="lazy" src="/zh-CN/assets/images/16714312637015-abff06af8bf814cbb39e2fddfd5b0271.jpg" width="720" height="360" class="img_ev3q">
The previous examples are cases regarding a small number of tables, but in real business service development, we usually need to synchronize thousands of tables, which may be divided into databases and tables at the same time;</p><p>The status quo is that we need to configure each table, a large number of table synchronization takes a lot of time for users, and it is prone to problems such as field mapping errors, which are difficult to maintain.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="not-supporting-schema-evolution">Not supporting Schema Evolution<a href="#not-supporting-schema-evolution" class="hash-link" aria-label="Not supporting Schema Evolution的直接链接" title="Not supporting Schema Evolution的直接链接">​</a></h2><p><img loading="lazy" alt="Not-supports-DDL" src="/zh-CN/assets/images/16714312769761-80425f8f27f3efc3ab62487814dad59e.jpg" width="720" height="360" class="img_ev3q">
Besides, according to the research report of Fivetran, 60% of the company’s schema will change every month, and 30% will change every week.</p><p>However, none of the existing engines supports Schema Evolution. After changing the Schema each time, the user needs to reconfigure the entire link, which makes the maintenance of the job very cumbersome.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-high-volume-of-resource-usage">The high volume of resource usage<a href="#the-high-volume-of-resource-usage" class="hash-link" aria-label="The high volume of resource usage的直接链接" title="The high volume of resource usage的直接链接">​</a></h2><p>The database link takes up too much</p><p><img loading="lazy" src="/zh-CN/assets/images/16714313100541-98837b408ddcd3e5d7114421e893de80.jpg" width="720" height="480" class="img_ev3q">
If our Source or Sink is of JDBC type, since the existing engine only supports one or more links per table, when there are many tables to be synchronized, more link resources will be occupied, which will bring a great burden to the database server.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="operator-pressure-is-uncontrollable">Operator pressure is uncontrollable<a href="#operator-pressure-is-uncontrollable" class="hash-link" aria-label="Operator pressure is uncontrollable的直接链接" title="Operator pressure is uncontrollable的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714313301435-5d797838b956772428477a7a26a2591c.jpg" width="720" height="333" class="img_ev3q">
In the existing engine, a buffer and other control operators are used to control the pressure, that is, the back pressure mechanism; since the back pressure is transmitted level by level, there will be pressure delay, and at the same time, the processing of data will not be smooth enough, increasing the GC time, fault-tolerant completion time, etc.</p><p>Another case is that neither the source nor the sink has reached the maximum pressure, but the user still needs to control the synchronization rate to prevent too much impact on the source database or the target database, which cannot be controlled through the back pressure mechanism.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-goals-of-apache-seatunnel-engine">Architecture goals of Apache SeaTunnel Engine<a href="#architecture-goals-of-apache-seatunnel-engine" class="hash-link" aria-label="Architecture goals of Apache SeaTunnel Engine的直接链接" title="Architecture goals of Apache SeaTunnel Engine的直接链接">​</a></h2><p>To solve these severe issues faced by computing engines, we self-developed our engine expertise in big data integration.</p><p>Firstly, let’s get through what goals this engine wants to achieve.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pipeline-failover">Pipeline Failover<a href="#pipeline-failover" class="hash-link" aria-label="Pipeline Failover的直接链接" title="Pipeline Failover的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714313559400-d6029daeb8672d49a5bfb8718c518e3f.jpg" width="720" height="176" class="img_ev3q">
In the data integration case, there is a possibility that a job can synchronize hundreds of sheets, and the failure of one node or one table will lead to the failure of all tables, which is too costly.</p><p>We expect that unrelated Job Tasks will not affect each other during fault tolerance, so we call a vertex collection with upstream and downstream relationships a Pipeline, and a Job can consist of one or more pipelines.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="regional-failover">Regional Failover<a href="#regional-failover" class="hash-link" aria-label="Regional Failover的直接链接" title="Regional Failover的直接链接">​</a></h2><p>Now if there is an exception in the pipeline, we still need to failover all the vertex in the pipeline; but can we restore only part of the vertex?
<img loading="lazy" src="/zh-CN/assets/images/16714313919617-100427ea7efcebd349ce998e13a8c0a0.jpg" width="720" height="176" class="img_ev3q">
For example, if the Source fails, the Sink does not need to restart. In the case of a single Source and multiple Sinks, if a single Sink fails, only the Sink and Source that failed will be restored; that is, only the node that failed and its upstream nodes will be restored.</p><p>Obviously, the stateless vertex does not need to be restarted, and since SeaTunnel is a data integration framework, we do not have aggregation state vertexes such as Agg and Count, so we only need to consider Sink;</p><ul><li>Sink does not support idempotence &amp; 2PC; no restart and restart will result in the same data duplication, which can only be solved by Sink without restarting;</li><li>Sink supports idempotence, but does not support 2PC: because it is idempotent writing, it does not matter whether the source reads data inconsistently every time, and it does not need to be restarted;</li><li>Sink supports 2PC:</li><li>If the Source supports data consistency, if an abort is not executed, the processed old data will be automatically ignored through the channel data ID, and at the same time, it will face the problem that the transaction session time may time out;</li><li>If the Source does not support data consistency, perform abort on the Sink to discard the last data, which has the same effect as restarting but does not require initialization operations such as re-establishing links;</li><li>That is, the simplest implementation is to execute abort.
We use the pipeline as the minimum granularity for fault-tolerant management, and use the Chandy-Lamport algorithm to realize fault-tolerant distributed jobs.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-cache">Data Cache<a href="#data-cache" class="hash-link" aria-label="Data Cache的直接链接" title="Data Cache的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714314318184-95b8b08ab40b5d0da4ca1bd2e659f965.jpg" width="720" height="175" class="img_ev3q">
For sink failure, when data cannot be written, a possible solution is to work two jobs at the same time.</p><p>One job reads the database logs using the CDC source connector and then writes the data to Kafka using the Kafka Sink connector. Another job reads data from Kafka using the Kafka source connector and writes data to the destination using the destination sink connector.</p><p>This solution requires users to have a deep understanding of the underlying technology, and both tasks will increase the difficulty of operation and maintenance. Because every job needs JobMaster, it requires more resources.</p><p>Ideally, the user only knows that they will be reading data from the source and writing data to the sink, and at the same time, during this process, the data can be cached in case the sink fails. The sync engine needs to automatically add caching operations to the execution plan and ensure that the source still works in the event of a sink failure. In this process, the engine needs to ensure that the data written to the cache and read from the cache are transactional, to ensure data consistency.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sharding--multi-table-sync">Sharding &amp; Multi-table Sync<a href="#sharding--multi-table-sync" class="hash-link" aria-label="Sharding &amp; Multi-table Sync的直接链接" title="Sharding &amp; Multi-table Sync的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714314489916-cd0b185b6626a8c1598aa3ac569ecdbd.jpg" width="720" height="448" class="img_ev3q"></p><p>For a large number of table synchronization, we expect that a single Source can support reading multiple structural tables, and then use the side stream output to keep consistent with a single table stream.</p><p>The advantage of this is that it can reduce the link occupation of the data source and improve the utilization rate of thread resources.</p><p>At the same time, in SeaTunnel Engine, these multiple tables will be regarded as a pipeline, which will increase the granularity of fault tolerance; there are trade-offs, and the user can choose how many tables a pipeline can pass through.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="schema-evolution">Schema Evolution<a href="#schema-evolution" class="hash-link" aria-label="Schema Evolution的直接链接" title="Schema Evolution的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714314658701-b97c66b4a4eacfe54acf2e5ea6cc001b.jpg" width="720" height="199" class="img_ev3q">
Schema Evolution is a feature that allows users to easily change the current schema of a table to accommodate changing data over time. Most commonly, it is used when performing an append or overwrite operation, to automatically adjust the schema to include one or more new columns.</p><p>This feature is required for real-time data warehouse scenarios. Currently, the Flink and Spark engines do not support this feature.</p><p>In SeaTunnel Engine, we will use the Chandy-Lamport algorithm to send DDL events, make them flow in the DAG graph and change the structure of each operator, and then synchronize them to the Sink.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shared-resource">Shared Resource<a href="#shared-resource" class="hash-link" aria-label="Shared Resource的直接链接" title="Shared Resource的直接链接">​</a></h2><p><img loading="lazy" alt="Shared-resource" src="/zh-CN/assets/images/16714314806989-3f0543f4a2b9412e0d6c48eb1b7aa5c1.jpg" width="720" height="455" class="img_ev3q">
The Multi-table feature can reduce the use of some Source and Sink link resources. At the same time, we have implemented Dynamic Thread Resource Sharing in SeaTunnel Engine, reducing the resource usage of the engine on the server.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="speed-control">Speed Control<a href="#speed-control" class="hash-link" aria-label="Speed Control的直接链接" title="Speed Control的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714315001348-d6e97c70a71c7f4b9220a68751cb1401.jpg" width="720" height="323" class="img_ev3q">
As for the problems that cannot be solved by the back pressure mechanism, we will optimize the Buffer and Checkpoint mechanism:</p><ul><li>Firstly, We try to allow Buffer to control the amount of data in a period;</li><li>Secondly, by the Checkpoint mechanism, the engine can lock the buffer after the Checkpoint reaches the maximum number of parallelism and executes an interval time, prohibiting the writing of Source data, achieving the result of taking the pressure proactively, avoiding issues like back pressure delay or failure to be delivered to Source.
The above is the design goal of SeaTunnel Engine, hoping to help you better solve the problems that bother you in data integration. In the future, we will continue to optimize the experience of using SeaTunnel so that more people are willing to use it.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-future-of-apache-seatunnel">The future of Apache SeaTunnel<a href="#the-future-of-apache-seatunnel" class="hash-link" aria-label="The future of Apache SeaTunnel的直接链接" title="The future of Apache SeaTunnel的直接链接">​</a></h2><p>As an Apache incubator project, the Apache SeaTunnel community is developing rapidly. In the following community planning, we will focus on four directions:</p><p>Support more data integration scenarios (Apache SeaTunnel Engine)
It is used to solve the pain points that existing engines cannot solve, such as the synchronization of the entire database, the synchronization of table structure changes, and the large granularity of task failure;</p><blockquote><p>Guys who are interested in the engine can pay attention to this Umbrella: <a href="https://github.com/apache/incubator-seatunnel/issues/2272" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/2272</a></p></blockquote><p>Expand and improve Connector &amp; Catalog ecology
Support more Connector &amp; Catalog, such as TiDB, Doris, Stripe, etc., and improve existing connectors, improve their usability and performance, etc.;
Support CDC connector for real-time incremental synchronization scenarios.</p><blockquote><p>Guys who are interested in connectors can pay attention to this Umbrella: <a href="https://github.com/apache/incubator-seatunnel/issues/1946" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/1946</a></p></blockquote><p>Support for more versions of the engines
Such as Spark 3.x, Flink 1.14.x, etc.</p><blockquote><p>Guys who are interested in supporting Spark 3.3 can pay attention to this PR: <a href="https://github.com/apache/incubator-seatunnel/pull/2574" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2574</a></p></blockquote><p>Easier to use (Apache SeaTunnel Web)
Provides a web interface to make operations more efficient in the form of DAG/SQL Simple and more intuitive display of Catalog, Connector, Job, etc.;
Access to the scheduling platform to make task management easier</p><blockquote><p>Guys who are interested in Web can pay attention to our Web sub-project: <a href="https://github.com/apache/incubator-seatunnel-web" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel-web</a></p></blockquote>]]></content:encoded>
            <category>Meetup</category>
        </item>
        <item>
            <title><![CDATA[Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/During the joint Apache SeaTunnel &amp; IoTDB Meetup on October 15,</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/During the joint Apache SeaTunnel &amp; IoTDB Meetup on October 15,</guid>
            <pubDate>Thu, 17 Nov 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Bo Bi, data engineer at Mafengwo]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="/zh-CN/assets/images/16714322908857-c6ce9a962f477d13b5480206b583b6d2.jpg" width="720" height="480" class="img_ev3q"></p><p><img loading="lazy" src="/zh-CN/assets/images/16714322944041-351d2ac6aa565d636c97a1ad6b0c136e.jpg" width="360" height="309" class="img_ev3q">
Bo Bi, data engineer at Mafengwo</p><blockquote><p>During the joint Apache SeaTunnel &amp; IoTDB Meetup on October 15, Bo Bi, the data engineer at a leading Chinese travel-social e-commerce platform Mafengwo, introduced the basic principles of SeaTunnel and related enterprise practice thinking, the pain points and optimization thinking in typical scenarios of Mafengwo’s big data development and scheduling platform, and shared his experience of participating in community contributions. We hope to help you understand SeaTunnel and the paths and skills of community building at the same time.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-to-the-technical-principle-of-seatunnel">Introduction to the technical principle of SeaTunnel<a href="#introduction-to-the-technical-principle-of-seatunnel" class="hash-link" aria-label="Introduction to the technical principle of SeaTunnel的直接链接" title="Introduction to the technical principle of SeaTunnel的直接链接">​</a></h2><p>SeaTunnel is a distributed, high-performance data integration platform for the synchronization and transformation of large volumes of data (offline and real-time)</p><p>The diagram above shows the workflow of SeaTunnel, which in simple terms consists of 3 parts: input, transformation, and output; more complex data processing is just a combination of several actions.</p><p>In a synchronization scenario, such as importing Kafka to Elasticsearch, Kafka is the Source of the process and Elasticsearch is the Sink of the process.</p><p>If, during the import process, the field columns do not match the external data columns to be written and some column or type conversion is required, or if you need to join multiple data sources and then do some data widening, field expansion, etc., then you need to add some Transform in the process, corresponding to the middle part of the picture.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714323322988-74b7a47d1a3299efd23c6375d3acaa5e.jpg" width="660" height="781" class="img_ev3q">
This shows that the core of SeaTunnel is the Source, Transform and Sink process definitions.</p><p>In Source we can define the data sources we need to read, in Sink, we can define the data pipeline and eventually write the external storage, and we can transform the data in between, either using SQL or custom functions.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-api-version-v1-architecture-breakdown">SeaTunnel Connector API Version V1 Architecture Breakdown<a href="#seatunnel-connector-api-version-v1-architecture-breakdown" class="hash-link" aria-label="SeaTunnel Connector API Version V1 Architecture Breakdown的直接链接" title="SeaTunnel Connector API Version V1 Architecture Breakdown的直接链接">​</a></h2><p>For a mature component framework, there must be something unique about the design pattern of the API design implementation that makes the framework scalable.</p><p>The SeaTunnel architecture consists of three main parts.</p><p>1、SeaTunnel Basic API.</p><ol><li><p>the implementation of the SeaTunnel base API.</p></li><li><p>SeaTunnel’s plug-in system.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-basic-api">SeaTunnel Basic API<a href="#seatunnel-basic-api" class="hash-link" aria-label="SeaTunnel Basic API的直接链接" title="SeaTunnel Basic API的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714323668557-9c9260e0c54017b5282b1294ccc9c692.jpg" width="720" height="194" class="img_ev3q">
The above diagram shows the definition of the interface, the Plugin interface in SeaTunnel abstracts the various actions of data processing into a Plugin.</p><p>The five parts of the diagram below, Basesource, Basetransfform, Basesink, Runtimeenv, and Execution, all inherit from the Plugin interface.
<img loading="lazy" src="/zh-CN/assets/images/16714323741126-a61ed7b20a44b14b78d39c7ffe42ec41.jpg" width="720" height="229" class="img_ev3q"></p><p>As a process definition plug-in, Source is responsible for reading data, Transform is responsible for transforming, Sink is responsible for writing and Runtimeenv is setting the base environment variables.</p><p>The overall SeaTunnel base API is shown below</p><p><img loading="lazy" src="/zh-CN/assets/images/16714323846302-eabb8409469fa34d9b0ebd2402fca23d.jpg" width="720" height="347" class="img_ev3q">
Execution, the data flow builder used to build the entire data flow based on the first three, is also part of the base API</p><p><img loading="lazy" src="/zh-CN/assets/images/16714323920717-b5bb28d92939847f1f30c61b6895191a.jpg" width="720" height="192" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-base-api-implementation">SeaTunnel Base API Implementation<a href="#seatunnel-base-api-implementation" class="hash-link" aria-label="SeaTunnel Base API Implementation的直接链接" title="SeaTunnel Base API Implementation的直接链接">​</a></h2><p>Based on the previous basic APIs, SeaTunnel has been implemented in separate packages for different computing engines, currently the Spark API abstraction and the Flink API abstraction, which logically completes the process of building the data pipeline.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714323741126-a61ed7b20a44b14b78d39c7ffe42ec41.jpg" width="720" height="229" class="img_ev3q"></p><p>Due to space constraints, we will focus on Spark batch processing. Based on the wrapped implementation of the previous base Api, the first is that Base spark source implements Base source, base Spark transform implements Base transform and Base Spark sink implements Base sink.</p><p>The method definition uses Spark’s Dataset as the carrier of the data, and all data processing is based on the Dataset, including reading, processing and exporting.</p><p>The SparkEnvironment, which internally encapsulates Spark’s Sparksession in an Env, makes it easy for individual plugins to use.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714324136843-44e65c36b7ef55c34b50a9eb0e43c3cc.jpg" width="720" height="350" class="img_ev3q"></p><p>The Spark batch process ends with SparkBatchExecution (the data stream builder), which is the core code snippet used to functionally build our data stream Pipeline, the most basic data stream on the left in the diagram below.</p><p>The user-based definition of each process component is also the configuration of Source Sink, Transform. More complex data flow logic can be implemented, such as multi-source Join, multi-pipeline processing, etc., all of which can be built through Execution.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714324237449-e5a12e608045d5a153853c93eb844852.jpg" width="720" height="405" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v1-api-architecture-summary">SeaTunnel Connector V1 API Architecture Summary<a href="#seatunnel-connector-v1-api-architecture-summary" class="hash-link" aria-label="SeaTunnel Connector V1 API Architecture Summary的直接链接" title="SeaTunnel Connector V1 API Architecture Summary的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714324336701-6e5cadce0d1a1858d7bffbf96c5cae82.jpg" width="720" height="405" class="img_ev3q">
SeaTunnel’s API consists of three main parts.</p><p>The first part is the SeaTunnel base API, which provides the basic abstract interfaces such as Source, Sink, Transform, and Plugin.</p><p>The second part is based on a set of interfaces Transform, Sink, Source, Runtime, and Execution provided by the SeaTunnel base API, which is wrapped and implemented on the Flink and Spark engines respectively, i.e. Spark engine API layer abstraction and Flink engine API layer abstraction.</p><p>Both Flink and Spark engines support stream and batch processing, so there are different ways to use streams/batches under the Flink API abstraction and Spark abstraction APIs, such as Flinkstream and Flinkbatch under the Flink abstraction API, and Sparkbatch and Sparkstreaming under the Spark abstraction API.</p><p>The third part is the plug-in system, based on Spark abstraction and Flink API abstraction, SeaTunnel engine implements rich connectors and processing plug-ins, while developers can also be based on different engine API abstractions, and extensions to achieve their own Plugin.</p><p>SeaTunnel Implementation Principle
Currently, SeaTunnel offers a variety of ways to use Flink, Spark, and FlinkSQL. Due to space limitations, we will introduce the execution principles of the Spark method.</p><p>First, the entry starts the command Start-seatunnel-spark.sh via the shell, which internally calls Sparkstarter’s Class, which parses the parameters passed by the shell script, and also parses the Config file to determine which Connectors are defined in the Config file, such as Fake, Console, etc.
<img loading="lazy" src="/zh-CN/assets/images/16714324454477-200fd76badcfc17bdd291f364c70a191.jpg" width="720" height="405" class="img_ev3q">
Then find the Connector path from the Connector plugin directory and stitch it into the Spark-submit launch command with — jar, so that the found Plugin jar package can be passed to the Spark cluster as a dependency.</p><p>For Connector plugins, all Spark Connectors are packaged in the plugin directory of the distribution (this directory is managed centrally).</p><p>After Spark-submit is executed, the task is submitted to the Spark cluster, and the Main class of the Spark job’s Driver builds the data flow Pipeline through the data flow builder Execution, combined with Souce, Sink, and Transform so that the whole chain is connected.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v2-api-architecture">SeaTunnel Connector V2 API Architecture<a href="#seatunnel-connector-v2-api-architecture" class="hash-link" aria-label="SeaTunnel Connector V2 API Architecture的直接链接" title="SeaTunnel Connector V2 API Architecture的直接链接">​</a></h2><p>In the latest community release of SeaTunnel 2.2.0-beta, the refactoring of the Connectorapi, now known as the SeaTurnelV2 API, has been completed!</p><p>Why do we need to reconfigure?</p><p>As the Container is currently a strongly coupled engine, i.e. Flink and Spark API, if the Flink or Spark engine is upgraded, the Connector will also have to be adjusted, possibly with changes to parameters or interfaces.</p><p>This can lead to multiple implementations for different engines and inconsistent parameters to develop a new Connector. Therefore, the community has designed and implemented the V2 version of the API based on these pain points.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714324726276-11d9d5c6d4e848796fa71329819caa72.jpg" width="720" height="405" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-v2-api-architecture">SeaTunnel V2 API Architecture<a href="#seatunnel-v2-api-architecture" class="hash-link" aria-label="SeaTunnel V2 API Architecture的直接链接" title="SeaTunnel V2 API Architecture的直接链接">​</a></h2><p>SeaTunnel V2 API Architecture</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1table-api">1.Table API<a href="#1table-api" class="hash-link" aria-label="1.Table API的直接链接" title="1.Table API的直接链接">​</a></h3><p>·DataType: defines SeaTunnel’s data structure SeaTunnelRow, which is used to isolate the engine</p><p>·Catalog: used to obtain Table Scheme, Options, etc..</p><p>·Catalog Storage: used to store user-defined Table Schemes etc. for unstructured engines such as Kafka.</p><p>·Table SPI: mainly used to expose the Source and Sink interfaces as an SPI</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-source--sink-api">2. Source &amp; Sink API<a href="#2-source--sink-api" class="hash-link" aria-label="2. Source &amp; Sink API的直接链接" title="2. Source &amp; Sink API的直接链接">​</a></h3><p>Define the Connector’s core programming interface for implementing the Connector</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3engine-api">3.Engine API<a href="#3engine-api" class="hash-link" aria-label="3.Engine API的直接链接" title="3.Engine API的直接链接">​</a></h3><p>·Translation: The translation layer, which translates the Source and Sink APIs implemented by the Connector into a runnable API inside the engine.</p><p>·Execution: Execution logic, used to define the execution logic of Source, Transform, Sink and other operations within the engine.</p><p>The Source &amp; Sink API is the basis for the implementation of the connector and is very important for developers.</p><p>The design of the v2 Source &amp; Sink API is highlighted below</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v2-source-api">SeaTunnel Connector V2 Source API<a href="#seatunnel-connector-v2-source-api" class="hash-link" aria-label="SeaTunnel Connector V2 Source API的直接链接" title="SeaTunnel Connector V2 Source API的直接链接">​</a></h2><p>The current version of SeaTunnel’s API design draws on some of Flink’s design concepts, and the more core classes of the Source API are shown below.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714325444078-fbed659c615c445655896b93a093177f.jpg" width="720" height="405" class="img_ev3q">
<img loading="lazy" src="/zh-CN/assets/images/16714325474972-48cbee3672ad2f32317d48263f204978.jpg" width="720" height="405" class="img_ev3q">
The core Source API interaction flow is shown above. In the case of concurrent reads, the enumerator SourceSplitEnumerator is required to split the task and send the SourceSplit down to the SourceReader, which receives the split and uses it to read the external data source.</p><p>In order to support breakpoints and Eos semantics, it is necessary to preserve and restore the state, for example by preserving the current Reader’s Split consumption state and restoring it after a failure in each Reader through the Checkpoint state and Checkpoint mechanism, so that the data can be read from the place where it failed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v2-sink-api">SeaTunnel Connector V2 Sink API<a href="#seatunnel-connector-v2-sink-api" class="hash-link" aria-label="SeaTunnel Connector V2 Sink API的直接链接" title="SeaTunnel Connector V2 Sink API的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714325600316-f066630b909ec30a7727b7877a4838b0.jpg" width="720" height="405" class="img_ev3q">
The overall Sink API interaction flow is shown in the diagram below. The SeaTunnel sink is currently designed to support distributed transactions, based on a two-stage transaction commit.</p><p>First SinkWriter continuously writes data to an external data source, then when the engine does a checkpoint, it triggers a first-stage commit.</p><p>SinkWriter needs to do a Prepare commit, which is the first stage of the commit.</p><p>The engine will determine if all the Writer's first stage succeeds, and if they all succeed, the engine will combine the Subtask’s Commit info with the Commit method of the Committer to do the actual commit of the transaction and operate the database for the Commit, i.e. the second stage of the commit. This is the second stage of commit.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714325681738-973c53bd86223df4ee98cbb2ecb30eaf.jpg" width="570" height="814" class="img_ev3q">
For the Kafka sink connector implementation, the first stage is to do a pre-commit by calling KafkaProducerSender.prepareCommit().</p><p>The second commit is performed via Producer.commitTransaction();.</p><p>flush(); flushes the data from the Broker’s system cache to disk.</p><p>Finally, it is worth noting!</p><p>Both SinkCommitter and SinkAggregatedCommitter can perform a second stage commit to replace the Committer in the diagram. The difference is that SinkCommitter can only do a partial commit of a single Subtask’s CommitInfo, which may be partially successful and partially unsuccessful, and cannot be handled globally. The difference is that the SinkCommitter can only do partial commits of a single Subtask’s CommitInfo, which may be partially successful and partially unsuccessful.</p><p>SinkAggregatedCommitter is a single parallel, aggregating the CommitInfo of all Subtask, and can do the second stage commit as a whole, either all succeed or all fail, avoiding the problem of inconsistent status due to partial failure of the second stage.</p><p>It is therefore recommended that the SinkAggregatedCommitter be used in preference.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-of-seatunnel-v1-and-v2-api-processing-flows">Comparison of SeaTunnel V1 and V2 API processing flows<a href="#comparison-of-seatunnel-v1-and-v2-api-processing-flows" class="hash-link" aria-label="Comparison of SeaTunnel V1 and V2 API processing flows的直接链接" title="Comparison of SeaTunnel V1 and V2 API processing flows的直接链接">​</a></h2><p>We can look at the changes before and after the V1 V2 upgrade from a data processing perspective, which is more intuitive, Spark batch processing as an example: SeaTunnel V1: The entire data processing process is based on the Spark dataset API, and the Connector and the compute engine are strongly coupled.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714325887598-d27009789ff28e56c8bfcca29bcedfe1.jpg" width="720" height="405" class="img_ev3q">
SeaTunnel V2: Thanks to the work of the engine translator, the Connector API, and the SeaTunnelRow, the data source of the SeaTunnel internal data structures accessed through the Connector, are translated by the translation layer into a runnable Spark API and spark dataset that is recognized inside the engine during data transformation.</p><p>As data is written out, the Spark API and Spark dataset are translated through the translation layer into an executable connector API inside the SeaTunnel connector and a data source of internal SeaTunnel structures that can be used.</p><blockquote><p>Overall, the addition of a translation layer at the API and compute engine layers decouples the Connector API from the engine, and the Connector implementation no longer depends on the compute engine, making the extension and implementation more flexible.</p></blockquote><blockquote><p>In terms of community planning, the V2 API will be the main focus of development, and more features will be supported in V2, while V1 will be stabilized and no longer maintained.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="practice-and-reflections-on-our-off-line-development-scheduling-platform">Practice and reflections on our off-line development scheduling platform<a href="#practice-and-reflections-on-our-off-line-development-scheduling-platform" class="hash-link" aria-label="Practice and reflections on our off-line development scheduling platform的直接链接" title="Practice and reflections on our off-line development scheduling platform的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="practice-and-reflections-on-our-off-line-development-scheduling-platform-1">Practice and reflections on our off-line development scheduling platform<a href="#practice-and-reflections-on-our-off-line-development-scheduling-platform-1" class="hash-link" aria-label="Practice and reflections on our off-line development scheduling platform的直接链接" title="Practice and reflections on our off-line development scheduling platform的直接链接">​</a></h3><p><img loading="lazy" src="/zh-CN/assets/images/16714326227360-bcd55d2c5b7b23ec91a5d1e27c04fb0e.jpg" width="720" height="405" class="img_ev3q">
Hornet’s Nest Big Data Development Platform, which focuses on providing one-stop big data development and scheduling services, helps businesses solve complex problems such as data development management, task scheduling and task monitoring in offline scenarios.</p><p>The offline development and scheduling platform plays the role of the top and the bottom. The top is to provide open interface API and UI to connect with various data application platforms and businesses, and the bottom is to drive various computations and storage, and then run in an orderly manner according to the task dependency and scheduling time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="platform-capabilities">Platform Capabilities<a href="#platform-capabilities" class="hash-link" aria-label="Platform Capabilities的直接链接" title="Platform Capabilities的直接链接">​</a></h2><p><strong>Data development</strong></p><p>Task configuration, quality testing, release live</p><p><strong>·Data synchronisation</strong></p><p>Data access, data processing, data distribution</p><p><strong>·Scheduling capabilities</strong></p><p>Supports timed scheduling, triggered scheduling</p><p><strong>·Operations and Maintenance Centre
</strong>
Job Diagnosis, Task O&amp;M, Instance O&amp;M</p><p><strong>·Management</strong></p><p>Library table management, permission management, API management, script management</p><p>In summary, the core capabilities of the offline development scheduling platform are openness, versatility, and one-stop shopping. Through standardized processes, the entire task development cycle is managed and a one-stop service experience is provided.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-architecture-of-the-platform">The architecture of the platform<a href="#the-architecture-of-the-platform" class="hash-link" aria-label="The architecture of the platform的直接链接" title="The architecture of the platform的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/16714326749427-2957d8414a3175ed8cc46bde32a08565.jpg" width="720" height="405" class="img_ev3q">
The Hornet’s Nest Big Data Development and Scheduling Platform consists of four main modules: the task component layer, the scheduling layer, the service layer, and the monitoring layer.</p><p>The service layer is mainly responsible for job lifecycle management (e.g. job creation, testing, release, offline); Airflow dagphthon file building and generating, task bloodline dependency management, permission management, API (providing data readiness, querying of task execution status).</p><p>The scheduling layer is based on Airflow and is responsible for the scheduling of all offline tasks.</p><p>A task component layer that enables users to develop data through supported components that include tools such as SparkSQL/, HiveSQ, LMR), StarRocks import, etc., directly interfacing with underlying HDFS, MySQL, and other storage systems.</p><p>The monitoring layer is responsible for all aspects of monitoring and alerting on scheduling resources, computing resources, task execution, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-data-sync-capability-scenarios">Open Data Sync Capability Scenarios<a href="#open-data-sync-capability-scenarios" class="hash-link" aria-label="Open Data Sync Capability Scenarios的直接链接" title="Open Data Sync Capability Scenarios的直接链接">​</a></h2><p>Challenges with open capabilities: Need to support multiple business scenarios and meet flexible data pipeline requirements (i.e. extend to support more task components such as hive2clickhourse, clickhourse2mysql, etc.)</p><p>Extending task components based on Airflow: higher maintenance costs for extensions, need to reduce costs and increase efficiency (based on the limited provider's Airflow offers, less applicable in terms of usage requirements, Airflow is a Python technology stack, while our team is mainly based on the Java technology stack, so the technology stack difference brings higher iteration costs)</p><p>Self-developed task components: the high cost of platform integration, long development cycle, high cost of the configuration of task components. (Research or implement task components by yourself, different ways of adapting the parameters of the components in the service layer, no uniform way of parameter configuration)</p><p>We wanted to investigate a data integration tool that, firstly, supported a rich set of components, provided out-of-the-box capabilities, was easy to extend, and offered a uniform configuration of parameters and a uniform way of using them to facilitate platform integration and maintenance.</p><ul><li>Selection of data integration tools
<img loading="lazy" src="/zh-CN/assets/images/16714327002726-6bfd742beb9534e7fdbd917db5f53d51.jpg" width="720" height="405" class="img_ev3q">
To address the pain points mentioned above, we actively explored solutions and conducted a selection analysis of several mainstream data integration products in the industry. As you can see from the comparison above, Datax and SeaTunnel both offer good scalability, and high stability, support rich connector plugins, provide scripted, uniformly configurable usage, and have active communities.</li></ul><p>However, Datax is limited by being distributed and is not well suited to massive data scenarios.</p><p>In contrast, SeaTunnel offers the ability to provide distributed execution, distributed transactions, scalable levels of data handling, and the ability to provide a unified technical solution in data synchronization scenarios.</p><p>In addition to the advantages and features described above and the applicable scenarios, more importantly, the current offline computing resources for big data are unified and managed by yarn, and for the subsequently extended tasks we also wish to execute on Yarn, we finally prefer SeaTunnel for our usage scenarios.</p><p>Further performance testing of SeaTunnel and the development of an open data scheduling platform to integrate SeaTunnel may be carried out at a later stage, and its use will be rolled out gradually.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="outbound-scenario-hive-data-sync-to-starrocks">Outbound scenario: Hive data sync to StarRocks<a href="#outbound-scenario-hive-data-sync-to-starrocks" class="hash-link" aria-label="Outbound scenario: Hive data sync to StarRocks的直接链接" title="Outbound scenario: Hive data sync to StarRocks的直接链接">​</a></h2><p>To briefly introduce the background, the Big Data platform has now completed the unification of the OLAP engine layer, using the StarRocks engine to replace the previous Kylin engine as the main query engine in OLAP scenarios.</p><p>In the data processing process, after the data is modelled in the data warehouse, the upper model needs to be imported into the OLAP engine for query acceleration, so there are a lot of tasks to push data from Hive to StarRocks every day. task (based on a wrapper for the StarRocks Broker Load import method) to a StarRocks-based table.</p><p>The current pain points are twofold.</p><p>·Long data synchronization links: Hive2StarRocks processing links, which require at least two tasks, are relatively redundant.</p><p>·Outbound efficiency: From the perspective of outbound efficiency, many Hive models themselves are processed by Spark SQL, and based on the processing the Spark Dataset in memory can be pushed directly to StarRocks without dropping the disk, improving the model’s regional time.</p><p><img loading="lazy" src="/zh-CN/assets/images/16714327218590-2644dc4ad1179eab81d40fc774d970e9.jpg" width="720" height="405" class="img_ev3q">
StarRocks currently also supports Spark Load, based on the Spark bulk data import method, but our ETL is more complex, needs to support data conversion multi-table Join, data aggregation operations, etc., so temporarily can not meet.</p><p>We know from the SeaTunnel community that there are plans to support the StarRocks Sink Connector, and we are working on that part as well, so we will continue to communicate with the community to build it together later.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-involved-in-community-building">How to get involved in community building<a href="#how-to-get-involved-in-community-building" class="hash-link" aria-label="How to get involved in community building的直接链接" title="How to get involved in community building的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-community-contribution">SeaTunnel Community Contribution<a href="#seatunnel-community-contribution" class="hash-link" aria-label="SeaTunnel Community Contribution的直接链接" title="SeaTunnel Community Contribution的直接链接">​</a></h3><p>As mentioned earlier, the community has completed the refactoring of the V1 to V2 API and needs to implement more connector plug-ins based on the V2 version of the connector API, which I was lucky enough to contribute to.</p><p>I am currently responsible for big data infrastructure work, which many mainstream big data components big data also use, so when the community proposed a connector issue, I was also very interested in it.</p><p>As the platform is also investigating SeaTunnel, learning and being able to contribute pr to the community is a great way to learn about SeaTunnel.</p><p>I remember at first I proposed a less difficult pr to implement the WeChat sink connector, but in the process of contributing I encountered many problems, bad coding style, code style did not take into account the rich output format supported by the extension, etc. Although the process was not so smooth, I was really excited and accomplished when the pr was merged. Although the process was not so smooth, it was very exciting and rewarding when the pr was merged.</p><p>As I became more familiar with the process, I became much more efficient at submitting pr and was confident enough to attempt difficult issues.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-involved-in-community-contributions-quickly">How to get involved in community contributions quickly<a href="#how-to-get-involved-in-community-contributions-quickly" class="hash-link" aria-label="How to get involved in community contributions quickly的直接链接" title="How to get involved in community contributions quickly的直接链接">​</a></h3><ul><li>Good first issue
Good first issue #3018 #2828</li></ul><p>If you are a first-time community contributor, it is advisable to focus on the Good first issue first, as it is basically a relatively simple and newcomer-friendly issue.</p><p>Through Good first issue, you can get familiar with the whole process of participating in the GitHub open source community contribution, for example, first fork the project, then submit the changes, and finally submit the pull request, waiting for the community to review, the community will target to you to put forward some suggestions for improvement, directly will leave a comment below, until when your pr is merged in, this will have completed a comp</p><ul><li>Subscribe to community mailings
Once you’re familiar with the pr contribution process, you can subscribe to community emails to keep up to date with what’s happening in the community, such as what features are currently being worked on and what’s planned for future iterations. If you’re interested in a feature, you can contribute to it in your own situation!</li><li>Familiarity with git use
The main git commands used in development are git clone, git pull, git rebase and git merge. git rebase is recommended in the community development specification and does not generate additional commits compared to git merge.</li><li>Familiarity with GitHub project collaboration process
Open source projects are developed collaboratively by multiple people, and the collaboration method on GitHub is at its core outlined in fork For example, the apache st project, which is under the apache space, is first forked to our own space on GitHub</li></ul><p>Then modify the implementation, mention a pull request, and submit the pull request to be associated with the issue, in the commit, if we change a long time, in the upward commit, then the target branch has a lot of new commits exhausted this time we need to do a pull&amp; merge or rebase.</p><ul><li>Source code compilation project
It is important to be familiar with source compilation, as local source compilation can prove that the code added to a project can be compiled, and can be used as a preliminary check before committing to pr. Source compilation is generally slow and can be speeded up by using mvn -T for multi-threaded parallel compilation.</li><li>Compilation checks
Pre-compilation checks, including Licence header, Code checkstyle, and Document checkstyle, will be checked during Maven compilation, and if they fail, the CI will not be passed. So it is recommended to use some plug-in tools in the idea to improve the efficiency, such as Code checkstyle has a plug-in to automatically check the code specification, Licence header can add code templates in the idea, these have been shared by the community before how to do!</li><li>Add full E2E</li></ul><p>Add full E2E testing and ensure that the E2E is passed before the Pull request.</p><p>Finally, I hope more students will join the SeaTunnel community, where you can not only feel the open-source spirit and culture of Apache but also understand the management process of Apache projects and learn good code design ideas.</p><p>We hope that by working together and growing together, we can build SeaTunnel into a top-notch data integration platform.</p>]]></content:encoded>
            <category>Meetup</category>
        </item>
        <item>
            <title><![CDATA[A tutorial to help you develop a SeaTunnel Connector hand-by-hand while avoiding pitfalls]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/09/20/A-tutorial-to-help-you develop-a-SeaTunnel-Connector-hand-by-hand-while-avoiding -pitfalls</guid>
            <pubDate>Tue, 20 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[SeaTunnel Connector Acess Plan]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="https://miro.medium.com/max/1400/0*4fOZaPYhwL2pdUpK" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-acess-plan">SeaTunnel Connector Acess Plan<a href="#seatunnel-connector-acess-plan" class="hash-link" aria-label="SeaTunnel Connector Acess Plan的直接链接" title="SeaTunnel Connector Acess Plan的直接链接">​</a></h2><p>During the recent live event of the SeaTunnel Connector Access Plan, Beluga open source engineer Wang Hailin shared the “SeaTunnel Connector Access Plan and Development Guide to Avoiding Pit,” and taught everyone how to develop a connector from scratch, including the whole process — from preparation to testing, and final PR.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="speaker">Speaker<a href="#speaker" class="hash-link" aria-label="Speaker的直接链接" title="Speaker的直接链接">​</a></h2><p><img loading="lazy" src="https://miro.medium.com/max/1100/0*LRtFiJkgV5DEWOAa" class="img_ev3q">
<strong>Wang Hailin</strong></p><p>Wailin Hailin is an open-source enthusiast, SkyWalking Committer, DolphinScheduler, and SeaTunnel contributor. His current work focuses on performance monitoring, data processing, and more. He likes to study related technical implementations and participate in community exchanges and contributions.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="this-presentation-is-divided-into-5-parts">This presentation is divided into 5 parts:<a href="#this-presentation-is-divided-into-5-parts" class="hash-link" aria-label="This presentation is divided into 5 parts:的直接链接" title="This presentation is divided into 5 parts:的直接链接">​</a></h2><ol><li>About the connector access incentive program</li><li>Preparation before claiming/developing connector</li><li>Small things in development</li><li>Considerations for writing E2E Tests</li><li>Preparations to submit a PR</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-about-the-connector-access-incentive-plan">1. About the Connector Access Incentive Plan<a href="#1-about-the-connector-access-incentive-plan" class="hash-link" aria-label="1. About the Connector Access Incentive Plan的直接链接" title="1. About the Connector Access Incentive Plan的直接链接">​</a></h2><p>Firstly, let me introduce the SeaTunnel Connector Access Incentive Program, and the steps to develop a connector from start to finish (even for novices). This includes the whole process of preparation for development, testing, and final PR.</p><p>The SeaTunnel community released a new connector API not long ago, which supports running on various engines, including Flink, Spark, and more. This eliminates the need for repeated development of the old version.</p><p>After the new API is released, the old connector needs to be migrated, or the new connector should be supported.</p><p>In order to motivate the community to actively participate in the SeaTunnel Connector Access work and help build SeaTunnel into a more efficient data integration platform, the SeaTunnel community-initiated activities, sponsored by Beluga Open Source.</p><p>The activities have three modes: simple, medium, and hard for the task of accessing the connector. The threshold is low.</p><p>You can see which tasks need to be claimed on the activity issue list, as well as segmentation based on difficulty and priority. You can choose the activity you are comfortable with. You can start contributing based on the difficulty level.
<img loading="lazy" src="https://miro.medium.com/max/1400/0*laqub6yhNDOqPaGc" class="img_ev3q"></p><p>The ecological construction of SeaTunnel can become more complete and advanced only with the help of your contributions. You are welcome to participate actively.</p><p>In order to express our gratitude, our event has set up a link where points can be exchanged for physical prizes. The more points you get, the more prized you can win!</p><p>Presently, we’ve seen many small partners participate in the event and submit their connectors. It’s not too late to join as there is still a significant period of time before the event ends. Based on the difficulty of the activity, the deadline may be relaxed or extended.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-preparations-before-claimingdeveloping-connectors">2. Preparations Before Claiming/Developing Connectors<a href="#2-preparations-before-claimingdeveloping-connectors" class="hash-link" aria-label="2. Preparations Before Claiming/Developing Connectors的直接链接" title="2. Preparations Before Claiming/Developing Connectors的直接链接">​</a></h2><p>So, how do you get involved with this amazing activity?</p><p>By first getting to know the basics of a connector.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-what-is-a-connector">01. What is a connector?<a href="#01-what-is-a-connector" class="hash-link" aria-label="01. What is a connector?的直接链接" title="01. What is a connector?的直接链接">​</a></h3><p><img loading="lazy" src="https://miro.medium.com/max/750/0*IjdxVOKUu649s7vQ" class="img_ev3q">
A connector is composed of Source and SInk (Source + Sink).</p><p>In the above figure, the connectors are connected to various data sources at the upper and lower layers. The source is responsible for reading data from external data sources, while the sink is responsible for writing data to external sources.</p><p>There is also an abstraction layer between the source and the sink.</p><p>Through this abstraction later, the data types of various data sources can be uniformly converted into the data format of SeaTunnelRow. This allows users to arbitrarily assemble various sources and sinks, so as to realize the integration of heterogeneous data sources, and data synchronization between multiple data sources.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-how-to-claim-a-connector">02. How to claim a connector<a href="#02-how-to-claim-a-connector" class="hash-link" aria-label="02. How to claim a connector的直接链接" title="02. How to claim a connector的直接链接">​</a></h3><p>After understanding the basic concepts, the next step is to claim the connector.</p><p>GitHub link: <a href="https://github.com/apache/incubator-seatunnel/issues/1946" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/1946</a></p><p>You can use the above-mentioned GitHub link to see our plans for connecting to the connector. You can make any additions at any time.</p><p>First, find a connector that has not been claimed. To avoid conflicts, search the entire issue to see if anyone has submitted a PR.</p><p>After claiming the connector, we suggest that you create an issue of the corresponding feature, synchronize the problems you encountered in the development, and discuss the design of your solution.</p><p>If you encounter any problems and need help, you can describe them in the issue, and the community can take it up together. Participate in the discussions to help solve the problem. This is also added to the record of the function implementation process, which makes it easy to refer to when maintaining and modifying in the future.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-compile-the-project">03. Compile the project<a href="#03-compile-the-project" class="hash-link" aria-label="03. Compile the project的直接链接" title="03. Compile the project的直接链接">​</a></h3><p>After claiming the connector, it’s time to prepare the development environment.</p><p>First, fork the SeaTunnel project to the local development environment and compile it.</p><p>Here’s the compilation reference documentation: <a href="https://github.com/apache/incubator-seatunnel/blob/dev/docs/en/contribution/setup.md" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/blob/dev/docs/en/contribution/setup.md</a></p><p>Run the testcase in the documentation after the compilation is successful. You might encounter some issues/problems during the first contact compilation process, such as the following compilation errors:</p><p><img loading="lazy" src="https://miro.medium.com/max/1400/0*rGkqQzdfwd6Dp-mR" class="img_ev3q">
<img loading="lazy" src="https://miro.medium.com/max/1400/0*r2X63dr2YBTxZGen" class="img_ev3q"></p><p><strong>The solution to the above exceptions:</strong></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">rm {your_maven_dir}/repository/org/apache/seatunnel</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">./mvnw clean</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">Recompile it</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04-understand-connector-related-code-structure">04. Understand Connector related code structure<a href="#04-understand-connector-related-code-structure" class="hash-link" aria-label="04. Understand Connector related code structure的直接链接" title="04. Understand Connector related code structure的直接链接">​</a></h3><p>The success of project compilation means that the development environment is ready. Next, let’s take a look at the project code structure and API interface structure of the connector.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="engineering-code-structure">Engineering Code structure<a href="#engineering-code-structure" class="hash-link" aria-label="Engineering Code structure的直接链接" title="Engineering Code structure的直接链接">​</a></h4><p>After the project is compiled, there are three parts related to the connector. The first part is the code implementation and dependency management of the new connector module.</p><ul><li>seatunnel-connectors-v2 stores the connector submodule</li><li>seatunnel-connectors-v2-dist manages connectors-v2 maven dependencies</li></ul><p>The second part is the example. When testing locally, you can build a corresponding case on the example to test the connector.</p><ul><li>seatunnel-flink-connector-v2-example example running on Flink</li><li>seatunnel-spark-connector-v2-example example running on Spark</li></ul><p>The third part is the E2E-testcase: adding targeted test cases on the respective running engines of Spark or Flink, and verifying the functional logic of the connector through automated testing.</p><ul><li>seatunnel-flink-connector-v2-e2e testcase running on Flink</li><li>seatunnel-spark-connector-v2-e2e testcase running on Spark</li></ul><p><strong>Code structure (interfaces, base classes)</strong></p><p>The public interfaces and base classes used in the development are fully described in our readme. For example, API function usage scenarios.</p><p>Here’s the link: <a href="https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/README.en.md" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/README.en.md</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="05-see-how-other-people-develop-connectors">05. See how other people develop connectors<a href="#05-see-how-other-people-develop-connectors" class="hash-link" aria-label="05. See how other people develop connectors的直接链接" title="05. See how other people develop connectors的直接链接">​</a></h3><p>After going through the above steps, don’t rush to start the work. Instead, first, check out how others do it.</p><p>We strongly recommend you check out the connector novice development tutorial shared on the community's official account:</p><ul><li>[SeaTunnel Connector Minimalist Development Process]</li><li>[New API Connector Development Analysis]</li><li>[The way of decoupling Apache SeaTunnel (Incubating) and the computing engine — what we’ve done to reconstruct the API]</li></ul><p>In addition, you can refer to the merged Connector code to see the scope of changes, the public interfaces and dependencies used, and the test cases.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-small-issuestasks-during-development">3. Small Issues/Tasks During Development<a href="#3-small-issuestasks-during-development" class="hash-link" aria-label="3. Small Issues/Tasks During Development的直接链接" title="3. Small Issues/Tasks During Development的直接链接">​</a></h2><p>Next, you have to officially enter the connector development process. What problems may be encountered during the development process?</p><p>The connector is divided into the source and sink ends — you can choose either one or both.</p><p><img loading="lazy" src="https://miro.medium.com/max/640/0*QxOnCYLu4AcvKw58" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-source-related-development">01. Source-related development<a href="#01-source-related-development" class="hash-link" aria-label="01. Source-related development的直接链接" title="01. Source-related development的直接链接">​</a></h3><p>The first thing to pay attention to when developing a source is to determine the reading mode of the source: is it streaming or batch? Is support still required?</p><p>Use the Source#getBoundedness interface to mark the modes supported by the source.</p><p>For example, Kafka naturally supports streaming reading, but it can also support batch mode reading by obtaining lastOffset in the source.</p><p>Another question to be aware of: does the source require concurrent reads? If it is single concurrency, after the source is started, a reader will be created to read the data from the data source.</p><p>If you want to achieve multi-concurrency, you need to implement an enumerator interface through which data blocks are allocated to readers, and the readers each read their allocated data blocks.</p><p>For example, the Kafka source uses partition sharding, and the jdbc source uses fields for range query sharding. It should be noted here that if it is a concurrent reading method, the stability of the data block distribution rules must be ensured.</p><p>This is because currently, the connector has a corresponding enumerator on each shard in actual operation, and it is necessary to ensure that the enumerator has data in each shard.</p><p>Thirdly, does the source need to support resumable transfer/state restoration?</p><p>If you want to support this, you need to implement:</p><ul><li><strong>Source#restoreEnumerator</strong>: restore state</li><li><strong>Enumerator#snapshotState</strong>: storage shard allocation</li><li><strong>Reader#snapshotState</strong>: stores the read position</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-sink-related-development">02. Sink-related development<a href="#02-sink-related-development" class="hash-link" aria-label="02. Sink-related development的直接链接" title="02. Sink-related development的直接链接">​</a></h3><p>If the sink is a common sink implementation, use Sink#createWriter to write our data according to the concurrency of the source.</p><p>If you need to support failure recovery, you need to implement:</p><ul><li><strong>Sink#restoreWriter</strong>: restore state</li><li><strong>Writer#snapshotState</strong>: snapshot state</li></ul><p>If you want to support a two-phase commit, you need to implement the following interfaces:</p><ul><li>Sink#createCommitter</li><li>Writer#prepareCommit: pre-commit</li><li>Committer#commit: abort Phase 2 commit</li></ul><p><img loading="lazy" src="https://miro.medium.com/max/640/0*GpYdUR2mTlur8jHQ" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-connector-related">03. Connector related<a href="#03-connector-related" class="hash-link" aria-label="03. Connector related的直接链接" title="03. Connector related的直接链接">​</a></h3><p>Next, let’s take a look at some of the general problems, especially when the first contribution is made with different styles for each environment, there are often various problems. Therefore, it is recommended that you import tools/checkstyle/checkStyle.xml from the project during development, and use a unified coding format.</p><p>Whether it is a source or a sink, it will involve defining the data format. The community is pushing for a unified data format definition.</p><p>To define Schema, please refer to PR: <a href="https://github.com/apache/incubator-seatunnel/pull/2392" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2392</a>
To define the Format, please refer to PR: <a href="https://github.com/apache/incubator-seatunnel/pull/2435" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2435</a></p><p>If you feel that the compilation speed is slow, you can temporarily annotate the old version of the connector-related module in order to speed up both development and debugging.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04-how-to-seek-help">04. How to seek help<a href="#04-how-to-seek-help" class="hash-link" aria-label="04. How to seek help的直接链接" title="04. How to seek help的直接链接">​</a></h3><p>When you encounter problems during development and need help, you can:</p><ul><li>Describe the problem in your Issue and call active contributors</li><li>Discuss on mailing lists and Slack</li><li>Communicate through the WeChat group (if you have not joined, please follow the SeaTunnel official account to join the group, and add a small assistant WeChat seatunnel1)</li><li>There may be a community docking group for docking third-party components (allowing you to do more with less)</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-notes-on-writing-e2e-tests">4. Notes on Writing E2E Tests<a href="#4-notes-on-writing-e2e-tests" class="hash-link" aria-label="4. Notes on Writing E2E Tests的直接链接" title="4. Notes on Writing E2E Tests的直接链接">​</a></h2><p>E2E testing is very important. It is often called the gatekeeper of connector quality.</p><p>This is because, if the connector you wrote is not tested, it could be difficult for the community to judge whether there are problems with the implementation of the static code.</p><p>Therefore, E2E testing is not only functional verification but also a process of checking data logic, which can reduce the pressure on the community to review code and ensure basic functional correctness.</p><p>In E2E testing, these are some of the problems that may be encountered:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-e2e-failed--test-case-network-address-conflict">01. E2E Failed — Test Case Network Address Conflict<a href="#01-e2e-failed--test-case-network-address-conflict" class="hash-link" aria-label="01. E2E Failed — Test Case Network Address Conflict的直接链接" title="01. E2E Failed — Test Case Network Address Conflict的直接链接">​</a></h3><p><strong>Because the E2E network deployment structure has the following characteristics:</strong></p><ul><li>External components that Spark, Flink, and e2e-test case depend on (for example, MySQL), use the container networkAliases(host) as the access address</li><li>e2e-test case on both sides of Spark and Flink may run in parallel under the same host</li><li>External components that e2e-test case depends on, need to map ports to hosts for e2e-test case to access</li></ul><p><strong>Therefore, E2E has to pay attention to:</strong></p><ul><li>The external components e2e-test case depends on the ports mapped to the external networkAliases, and so cannot be the same in the test cases on both sides of Spark and Flink</li><li>e2e-test case uses localhost, the above-mapped port, to access external components</li><li>e2e’s configuration file uses networkAliases(host), the external components that depend on port access in the container</li></ul><p>Here’s the E2E Testcase reference PR: <a href="https://github.com/apache/incubator-seatunnel/pull/2429" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2429</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-e2e-failure--spark-jar-package-conflict">02. E2E failure — Spark jar package conflict<a href="#02-e2e-failure--spark-jar-package-conflict" class="hash-link" aria-label="02. E2E failure — Spark jar package conflict的直接链接" title="02. E2E failure — Spark jar package conflict的直接链接">​</a></h3><p>Spark uses the parent first-class loader by default, which may conflict with the package referenced by the connector. For this, the userClassPathFirst classloader can be configured in the Connector environment.</p><p>However, the current packaging structure of SeaTunnel will cause userClassPathFirst to not work properly, so we created an issue, <a href="https://github.com/apache/incubator-seatunnel/pull/2474" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2474</a>, to track this issue. Everyone is welcome to contribute solutions.</p><p>Currently, this can only be resolved by replacing conflicting packages in the spark jars directory with the documentation.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-e2e-failure--connector-jar-package-conflict">03. E2E failure — Connector jar package conflict<a href="#03-e2e-failure--connector-jar-package-conflict" class="hash-link" aria-label="03. E2E failure — Connector jar package conflict的直接链接" title="03. E2E failure — Connector jar package conflict的直接链接">​</a></h3><p>Both the old and new versions of Connector are dependent on the E2E project and cause conflicts.</p><p>PR <a href="https://github.com/apache/incubator-seatunnel/pull/2414" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2414</a> has resolved this issue.</p><p><strong>Version conflict between Connector-v2:</strong></p><ul><li>Mainly occurs during E2E, because the E2E project depends on all Connectors</li><li>We may plan to provide a separate test project for each Connector (or version) in the future</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04-insufficient-e2e--sink-logic-verification">04. Insufficient E2E — Sink Logic Verification<a href="#04-insufficient-e2e--sink-logic-verification" class="hash-link" aria-label="04. Insufficient E2E — Sink Logic Verification的直接链接" title="04. Insufficient E2E — Sink Logic Verification的直接链接">​</a></h3><p>The FakeSource of the Connector-v2 version can only generate random data of a few fixed columns at present, and the community partners are optimizing it to make it better. <a href="https://github.com/apache/incubator-seatunnel/pull/2406" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2406
</a>
That said, we can temporarily solve this problem by simulating the data of the specified content through Transform sql:
<img loading="lazy" src="https://miro.medium.com/max/1400/0*_uvD-JWrVbABolAq" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="05-insufficient-e2e--source-validation-data">05. Insufficient E2E — Source validation data<a href="#05-insufficient-e2e--source-validation-data" class="hash-link" aria-label="05. Insufficient E2E — Source validation data的直接链接" title="05. Insufficient E2E — Source validation data的直接链接">​</a></h3><p>The Assert Sink can configure column rules, but cannot do row-level value checking. For this problem, you can temporarily use other connector sinks with external storage for query verification data.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="06-e2e-stability-improvement">06. E2E stability improvement<a href="#06-e2e-stability-improvement" class="hash-link" aria-label="06. E2E stability improvement的直接链接" title="06. E2E stability improvement的直接链接">​</a></h3><p>In many cases, when E2E starts, you might use Thread.sleep to wait for resource initialization. Here, sleep will cause fewer initialization failures but more time-wasting issues.</p><p>In addition, due to the instability of resources, network, and other issues, you might be able to run it now but not later.
<img loading="lazy" src="https://miro.medium.com/max/1400/0*iBxwGDaHfXROqtEt" class="img_ev3q">
<img loading="lazy" src="https://miro.medium.com/max/1400/0*c2yFYbeVWPvHV7SY" class="img_ev3q">
To avoid this problem, Thread.sleep can be replaced with Awaitility.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="07-a-method-to-speed-up-e2e">07. A method to speed up E2E<a href="#07-a-method-to-speed-up-e2e" class="hash-link" aria-label="07. A method to speed up E2E的直接链接" title="07. A method to speed up E2E的直接链接">​</a></h3><p>At present, I see that most people run E2E tests separately for both source and sink. If you want to speed up the PR process, it is recommended that you combine both the sink and source into one E2E testcase for verification, and run the testcase only once.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-checks-before-submitting-a-pr">5. Checks Before Submitting a PR<a href="#5-checks-before-submitting-a-pr" class="hash-link" aria-label="5. Checks Before Submitting a PR的直接链接" title="5. Checks Before Submitting a PR的直接链接">​</a></h2><p>After completing the previous steps, please make sure you do some checks before submitting PR — including the following aspects:</p><p>Complete recompile project:</p><ul><li>Codestyle validation, dependency validation</li><li>The successful compilation before does not mean that it can be compiled successfully now</li></ul><p>Running E2E locally succeeds:</p><ul><li>Both Flink and Spark are verified</li></ul><p>Supplement or change the document and review it again before submitting:</p><ul><li>Review for places not covered by tests</li><li>Places that hav been reviewed before and needs to be checked again</li><li>Review for including all files, not just code</li></ul><p>The above operations and steps can greatly save CI resources, speed up PR Merged, and reduce the costs of community reviews.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Code Demo for SeaTunnel Connector Development Process]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/09/19/Code-Demo-for-SeaTunnel-Connector-Development-Process</guid>
            <pubDate>Mon, 19 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[At the Apache SeaTunnel&Apache Doris Joint Meetup held on July 24, Liu Li — senior engineer of WhaleOps and contributor to Apache SeaTunnel — mentioned an easy way to develop a connector in SeaTunnel quickly.]]></description>
            <content:encoded><![CDATA[<p>At the Apache SeaTunnel&amp;Apache Doris Joint Meetup held on July 24, Liu Li — senior engineer of WhaleOps and contributor to Apache SeaTunnel — mentioned an easy way to develop a connector in SeaTunnel quickly.</p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*Rbd5BrSuGiZUQA53DXZrBw.png" class="img_ev3q">
We’ll divide it into four key parts:</p><p>● The definition of a Connector</p><p>● How to access data sources and targets</p><p>● Code to demonstrate how to implement a Connector</p><p>● Sources and targets that are currently supported</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="definition-of-a-connector">Definition of a Connector<a href="#definition-of-a-connector" class="hash-link" aria-label="Definition of a Connector的直接链接" title="Definition of a Connector的直接链接">​</a></h2><p>The Connector consists of Source and Sink and is a concrete implementation of accessing data sources.</p><p>Source: The Source is responsible for reading data from sources such as MySQLSource, DorisSource, HDFSSource, TXTSource, and more.</p><p>Sink: The Sink is responsible for writing read data to the target, including MySQLSink, ClickHouseSink, HudiSink, and more. Data transfer, and more specifically, data synchronization is completed through the cooperation between the Source and Sink.</p><p><img loading="lazy" src="https://miro.medium.com/max/298/1*hsfa9Xtzt7o028XjCpqoOg.png" class="img_ev3q"></p><p>Of course, different sources and sinks can cooperate with each other.</p><p>For example, you can use MySQL Source, and Doris Sink to synchronize data from MySQL to Doris, or even read data from MySQL Source and write to HDFS Sink.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-access-data-sources-and-targets">How to access data sources and targets<a href="#how-to-access-data-sources-and-targets" class="hash-link" aria-label="How to access data sources and targets的直接链接" title="How to access data sources and targets的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-access-source">How to access Source<a href="#how-to-access-source" class="hash-link" aria-label="How to access Source的直接链接" title="How to access Source的直接链接">​</a></h3><p>Firstly, let’s take a look at how we can access the Source. To elaborate, let’s dive in and check out how we can implement a source and the core interfaces that need to be implemented to access the Source.</p><p>The simplest Source is a single concurrent Source. However, if a source does not support state storage and other advanced functions, what interfaces should we implement in these simple single concurrent sources?</p><p>Firstly, we need to use getBoundedness in the Source to identify whether the Source supports real-time or offline, or both.</p><p>createReader creates a Reader whose main function is to read the specific implementation of data. A single concurrent source is really simple as we only need to implement one method, pollNext, through which the read data is sent.</p><p>If concurrent reading is required, what additional interfaces should we implement?
<img loading="lazy" src="https://miro.medium.com/max/393/1*bRxRjyMOGkVqseQkg0ONWg.png" class="img_ev3q"></p><p>For concurrent reading, we’ll introduce a new member, called the Enumerator.</p><p>We implement createEnumerator in Source, and the main function of this member is to create an Enumerator to split the task into segments and then send it to the Reader.</p><p>For example, a task can be divided into 4 splits.</p><p>If it is concurrent twice, it’ll correspond to two Readers. Two of the four splits will be sent to Reader1, and the other two will be sent to Reader2.</p><p>If the number of concurrencies is more — for example, let’s say there are four concurrences, then you have to create four Readers. You have to use the corresponding four splits for concurrent reading for improved efficiency.</p><p>A corresponding interface in the Enumerator called the addSplitsBack sends the splits to the corresponding Reader. Through this method, the ID of the Reader can be specified.</p><p>Similarly, there is an interface called the addSplits in the Reader to receive the splits sent by the Enumerator for data reading.</p><p>In a nutshell, for concurrent reading, we need an Enumerator to implement task splitting and send the splits to the reader. Also, the reader receives the splits and uses them for reading.</p><p>In addition, if we need to support resuming and exactly-once semantics, what additional interfaces should we implement?</p><p>If the goal is to resume the transfer from a breakpoint, we must save the state and restore it. For this, we need to implement a restoreEnumerator in Source.</p><p>The restoreEnumerator method is used to restore an Enumerator through the state and restore the split.</p><p>Correspondingly, we need to implement a snapshotState in this enumerator, which is used to save the state of the current Enumerator and perform failure recovery during checkpoints.</p><p>At the same time, the Reader will also have a snapshotState method to save the split state of the Reader.</p><p>In the event of a failed restart, the Enumerator can be restored through the saved state. After the split is restored, reading can be continued from the place of failure, including fetching and incoming data.</p><p>The exact one-time semantics actually requires the source to support data replays, such as Kafka, Pulsar, and others. In addition, the sink must be submitted in two phases, i.e., the precise one-time semantics can be achieved with the cooperation of these two sources and sinks.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-access-sink">How to access Sink<a href="#how-to-access-sink" class="hash-link" aria-label="How to access Sink的直接链接" title="How to access Sink的直接链接">​</a></h3><p>Now, let’s take a look at how to connect to the Sink. What interfaces does the Sink need to implement?</p><p>Truth be told, Sink is relatively simple. For concurrent sinks, when state storage and two-phase commit are not supported, the Sink is simple.</p><p>To elaborate, the Sink does not distinguish between stream synchronization and batch synchronization as the Sink — and the entire SeaTunnel API system — supports <strong>Unified Stream and Batch Processing.</strong></p><p>Firstly, we need to implement createWriter. A Writer is used for data writing.</p><p>You need to implement a writer method in Writer through which data is written to the target library.</p><p><img loading="lazy" src="https://miro.medium.com/max/414/1*xQ7DRHdBGv-ofjSYdSHAoA.png" class="img_ev3q"></p><p>As shown in the figure above, if two concurrencies are set, the engine will call the createWriter method twice in order to generate two Writers. The engine will feed data to these two writers, which will write the data to the target through the write method.</p><p>For a more advanced setup, for example, we need to support <strong>two-phase commit and state storage</strong>.</p><p>Here, what additional interfaces should we implement?</p><p>First, let’s introduce a new member, the Committer, whose main role is for the second-stage commit.</p><p><img loading="lazy" src="https://miro.medium.com/max/414/1*cvj1i2A-E-1c_bCZneshtg.png" class="img_ev3q"></p><p>Since Sink is stored in state, it is necessary to restore Writer through the state. Hence, restoreWriter should be implemented.</p><p>Also, since we have introduced a new member, the Committer, we should also implement a createCommitter in the sink. We can then use this method to create a Committer for the second-stage commit or rollback.</p><p>In this case, what additional interfaces does Writer need to implement?</p><p>Since it is a two-phase commit, the first-phase commit is done in the Writer through the implementation of the prepareCommit method — which is mainly used for the first-phase commit.</p><p>In addition, state storage and failure recovery is also supported, meaning we need snapshotState to take snapshots at checkpoints. This saves the state for failure recovery scenarios.</p><p>The Committer is the core here. It is mainly used for rollback and commit operations in the second phase.</p><p>For the corresponding process, we need to write data to the database. Here, the engine will trigger the first stage commit during the checkpoint, and then the Writer needs to prepare a commit.</p><p>At the same time, it will return commitInfo to the engine, and the engine will judge whether the first stage commits of all writers are successful.</p><p>If they are indeed successful, the engine will use the commit method to actually commit.</p><p>For MySQL, the first-stage commit just saves a transaction ID and sends it to the commit. The engine determines whether the transaction ID is committed or rolled back.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-implement-the-connector">How to implement the Connector<a href="#how-to-implement-the-connector" class="hash-link" aria-label="How to implement the Connector的直接链接" title="How to implement the Connector的直接链接">​</a></h2><p>We’ve taken a look at Source and Sink; let’s now look at how to access the data source and implement your own Connector.</p><p>Firstly, we need to build a development environment for the Connector.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-necessary-environment">The necessary environment<a href="#the-necessary-environment" class="hash-link" aria-label="The necessary environment的直接链接" title="The necessary environment的直接链接">​</a></h3><ol><li><p>Java 1.8\11, Maven, IntelliJ IDEA</p></li><li><p>Windows users need to additionally download gitbash (<a href="https://gitforwindows.org/" target="_blank" rel="noopener noreferrer">https://gitforwindows.org/</a>)</p></li><li><p>Once you have these, you can download the SeaTunnel source code by cloning the git.</p></li><li><p>Download SeaTunnel source code 1, git clone <a href="https://github.com/apache/incubator-seatunnel.git2" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel.git2</a>, cd incubator-seatunnel</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-engineering-structure">SeaTunnel Engineering Structure<a href="#seatunnel-engineering-structure" class="hash-link" aria-label="SeaTunnel Engineering Structure的直接链接" title="SeaTunnel Engineering Structure的直接链接">​</a></h3><p>We then open it again through the IDE, and see the directory structure as shown in the figure:</p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*utRhNAsYiqQqBFa4Tjewgw.png" class="img_ev3q"></p><p>The directory is divided into several parts:</p><ol><li>Connector — v2</li></ol><p>Specific implementation of the new Connector(Connector — v2) will be placed in this module.</p><ol start="2"><li>connector-v2-dist</li></ol><p>The translation layer of the new connector translates into specific engine implementation — instead of implementing under corresponding engines such as Spark, Flink, and ST-Engine. ST-Engine is the “important, big project” the community is striving to implement. This project is worth the wait.</p><ol start="3"><li>examples</li></ol><p>This package provides a single-machine local operation method, which is convenient for debugging while implementing the Connector.</p><ol start="4"><li>e2e</li></ol><p>The e2e package is for e2e testing of the Connector.</p><p>Next, let’s check out how a Connector can be created (based on the new Connector). Here is the step-by-step process:</p><ol><li><p>Create a new module in the seatunnel-connectors-v2 directory and name it this way: connector-{connector name}.</p></li><li><p>The pom file can refer to the pom file of the existing connector and add the current child model to the parent model’s pom file.</p></li><li><p>Create two new packages corresponding to the packages of Source and Sink, respectively:</p></li></ol><p>a. org.apache.seatunnel.connectors.seatunnel.{connector name}.source</p><p>b. org.apache.seatunnel.connectors.seatunnel.{connector name}.sink</p><p>Take this mysocket example shown in the figure:</p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*K1btD2gNwYxj96OJnPfW2Q.png" class="img_ev3q"></p><p>To do some implementation, develop the connector. During implementation, you can use the example module for local debugging if you need to debug. That said, this module mainly provides the local running environment of Flink and Spark.</p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*qOc3q7okzo7jObHxloc7WQ.png" class="img_ev3q"></p><p>As you can see in the image, there are numerous examples under the “Example” module — including seatunnel-flink-connector-v2-example.</p><p>So how do you use them?</p><p>Let’s take an example. The debugging steps on Flink are as follows (these actions are under the seatunnel-flink-connector-v2-example module:</p><ol><li><p>Add connector dependencies in pom.xml</p></li><li><p>Add the task configuration file under resources/examples</p></li><li><p>Configure the file in the SeaTunnelApiExample main method</p></li><li><p>Run the main method</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="code-demo">Code Demo<a href="#code-demo" class="hash-link" aria-label="Code Demo的直接链接" title="Code Demo的直接链接">​</a></h3><p>This code demonstration is based on DingTalk.</p><p>Here’s a reference（ 19:35s–37:10s）:</p><p><a href="https://weixin.qq.com/sph/A1ri7B" target="_blank" rel="noopener noreferrer">https://weixin.qq.com/sph/A1ri7B</a></p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*ej9ronizPtC09ILWJDlbUg.png" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-connectors-supported-at-this-stage">New Connectors supported at this stage<a href="#new-connectors-supported-at-this-stage" class="hash-link" aria-label="New Connectors supported at this stage的直接链接" title="New Connectors supported at this stage的直接链接">​</a></h3><p>As of July 14, contributions and statistics for the completed connectors are welcome. You are more than welcome to try them out, and raise issues in our community if you find bugs.</p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*RHNJDcbvKmSt2UGGSz3Icg.png" class="img_ev3q"></p><p>The Connector shared below have already been claimed and developed:</p><p><img loading="lazy" src="https://miro.medium.com/max/700/1*RHNJDcbvKmSt2UGGSz3Icg.png" class="img_ev3q"></p><p>Also, we have Connectors in the roadmap — the connectors we want to support in the near future. To foster the process, the SeaTunnel Community initiated SeaTunnel Connector Access Incentive Plan, you are more than welcome to contribute to the project.</p><p>SeaTunnel Connector Access Incentive Plan: <a href="https://github.com/apache/incubator-seatunnel/issues/1946" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/1946</a></p><p>You can claim tasks that haven’t been marked in the comment area, and take a spree home! Here is part of the connectors that need to be accessed as soon as possible:
<img loading="lazy" src="https://miro.medium.com/max/414/1*n-ixPtq066Acx4Ja5qNQqw.png" class="img_ev3q">
In fact, the implementations of Connectors like Feishu, DingTalk, and Facebook messenger are quite simple as the connectors do not need to carry a large amount of data (just a simple Source and Sink). This is in sharp contrast to Hive and other databases that need to consider transaction consistency or concurrency issues.</p><p>We welcome everyone to make contributions and join our Apache SeaTunnel family!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-seatunnel">About SeaTunnel<a href="#about-seatunnel" class="hash-link" aria-label="About SeaTunnel的直接链接" title="About SeaTunnel的直接链接">​</a></h2><p>SeaTunnel (formerly Waterdrop) is an easy-to-use, ultra-high-performance distributed data integration platform that supports the real-time synchronization of massive amounts of data and can synchronize hundreds of billions of data per day stably and efficiently.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="why-do-we-need-seatunnel">Why do we need SeaTunnel?<a href="#why-do-we-need-seatunnel" class="hash-link" aria-label="Why do we need SeaTunnel?的直接链接" title="Why do we need SeaTunnel?的直接链接">​</a></h3><p>SeaTunnel does everything it can to solve the problems you may encounter in synchronizing massive amounts of data.</p><ul><li>Data loss and duplication</li><li>Task buildup and latency</li><li>Low throughput</li><li>Long application-to-production cycle time</li><li>Lack of application status monitoring</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-usage-scenarios">SeaTunnel Usage Scenarios<a href="#seatunnel-usage-scenarios" class="hash-link" aria-label="SeaTunnel Usage Scenarios的直接链接" title="SeaTunnel Usage Scenarios的直接链接">​</a></h3><ul><li>Massive data synchronization</li><li>Massive data integration</li><li>ETL of large volumes of data</li><li>Massive data aggregation</li><li>Multi-source data processing</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="features-of-seatunnel">Features of SeaTunnel<a href="#features-of-seatunnel" class="hash-link" aria-label="Features of SeaTunnel的直接链接" title="Features of SeaTunnel的直接链接">​</a></h3><ul><li>Rich components</li><li>High scalability</li><li>Easy to use</li><li>Mature and stable</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Talk With Overseas contributors | Why do I contribute to SeaTunnel?]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/09/14/Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/09/14/Talk-With-Overseas-contributors-Why-do-I-contribute-to-SeaTunnel</guid>
            <pubDate>Wed, 14 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[As SeaTunnel gets popular around the world, it is attracting more and more contributors from overseas to join the open-source career. Among them, a big data platform engineer at Kakao enterprise corp., Namgung Chan has recently contributed the Neo4j Sink Connector for the SeaTunnel. We have a talk with him to know why SeaTunnel is attractive to him, and how he thinks SeaTunnel should gain popularity in the South Korean market.]]></description>
            <content:encoded><![CDATA[<p>As SeaTunnel gets popular around the world, it is attracting more and more contributors from overseas to join the open-source career. Among them, a big data platform engineer at Kakao enterprise corp., Namgung Chan has recently contributed the Neo4j Sink Connector for the SeaTunnel. We have a talk with him to know why SeaTunnel is attractive to him, and how he thinks SeaTunnel should gain popularity in the South Korean market.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="personal-profile">Personal Profile<a href="#personal-profile" class="hash-link" aria-label="Personal Profile的直接链接" title="Personal Profile的直接链接">​</a></h2><p><img loading="lazy" src="https://miro.medium.com/max/1400/1*sKzXjqu6M_VmoperNBYUGQ.jpeg" class="img_ev3q"></p><p>Namgung Chan, South Korea, Big Data Platform Engineer at Kakao enterprise corp.</p><p>Blog (written in Korean): <a href="https://getchan.github.io/" target="_blank" rel="noopener noreferrer">https://getchan.github.io/</a>
GitHub ID: <a href="https://github.com/getChan" target="_blank" rel="noopener noreferrer">https://github.com/getChan</a>
LinkedIn : <a href="https://www.linkedin.com/in/namgung-chan-6a06441b6/" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/in/namgung-chan-6a06441b6/</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="contributions-to-the-community">Contributions to the community<a href="#contributions-to-the-community" class="hash-link" aria-label="Contributions to the community的直接链接" title="Contributions to the community的直接链接">​</a></h3><p>He writes the Neo4j Sink Connector code for the new SeaTunnel Connector API.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-know-seatunnel-for-the-first-time">How to know SeaTunnel for the first time?<a href="#how-to-know-seatunnel-for-the-first-time" class="hash-link" aria-label="How to know SeaTunnel for the first time?的直接链接" title="How to know SeaTunnel for the first time?的直接链接">​</a></h3><p>It’s the first time Namgung Chan to engage in open source. He wants to learn technical skills by contributing, at the same time experience the open-source culture.</p><p>For him, an open source project which is written by java lang, and made for data engineering, has many issues of ‘help wanted’ or ‘good first issue’ is quite suitable. Then he found SeaTunnel on the Apache Software Foundation project webpage.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-first-impression-of-seatunnel-community">The first impression of SeaTunnel Community<a href="#the-first-impression-of-seatunnel-community" class="hash-link" aria-label="The first impression of SeaTunnel Community的直接链接" title="The first impression of SeaTunnel Community的直接链接">​</a></h3><p>Though it was his first open source experience, he felt it was comfortable and interesting to go to the community. He also felt very welcome, because there are many ‘good first issue, and ‘volunteer wanted’ tagged issues and will get a quick response of code review.</p><p>With gaining knowledge of Neo4j, he grows much more confident in open source contribution.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="research-and-comparison">Research and comparison<a href="#research-and-comparison" class="hash-link" aria-label="Research and comparison的直接链接" title="Research and comparison的直接链接">​</a></h3><p>Before knowing about SeaTunnel, Namgung Chan used Spring Cloud Data Flow for data integration. While after experiencing SeaTunnel, he thinks the latter is more lightweight than SCDF, because in SCDF, every source, processor, and sink component are individual applications, but SeaTunnel is not.</p><p>Though hasn’t used SeaTunnel in his working environment yet, Namgung Chan said he would like to use it positively when he is in need, especially for data integration for various data storage.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="expectations-for-seatunnel">Expectations for SeaTunnel<a href="#expectations-for-seatunnel" class="hash-link" aria-label="Expectations for SeaTunnel的直接链接" title="Expectations for SeaTunnel的直接链接">​</a></h3><p>The most exciting new features or optimizations for Namgung Chan are:</p><p>Data Integration for various data storage.
Strict data validation. monitoring extension
Low computing resource
exactly-once data processing
In the future, Namgung Chan plans to keep contributing from light issues to heavy ones, and we hope he will have a good time here!</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SeaTunnel 2.1.3 released! Introducing in Assert Sink connector and NullRate, Nulltf Transform]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/09/12/SeaTunnel-2.1.3-released</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/09/12/SeaTunnel-2.1.3-released</guid>
            <pubDate>Mon, 12 Sep 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[More than a month after the release of Apache SeaTunnel(Incubating) 2.1.2, we have been collecting user and developer feedback to bring you version 2.1.3. The new version introduces the Assert Sink connector, which is an inurgent need in the community, and two Transforms, NullRate and Nulltf. Some usability problems in the previous version have also been fixed, improving stability and efficiency.]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="https://miro.medium.com/max/1400/1*7jtTFNpvwC6nquA-BLfqGg.png" class="img_ev3q"></p><p>More than a month after the release of Apache SeaTunnel(Incubating) 2.1.2, we have been collecting user and developer feedback to bring you version 2.1.3. The new version introduces the Assert Sink connector, which is an inurgent need in the community, and two Transforms, NullRate and Nulltf. Some usability problems in the previous version have also been fixed, improving stability and efficiency.</p><p>This article will introduce the details of the update of Apache SeaTunnel(Incubating) <strong>version 2.1.3</strong>.</p><ul><li>Release Note: <a href="https://github.com/apache/incubator-seatunnel/blob/2.1.3/release-note.md" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/blob/2.1.3/release-note.md</a></li><li>Download address: <a href="https://seatunnel.apache.org/download" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/download</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-feature-updates">Major feature updates<a href="#major-feature-updates" class="hash-link" aria-label="Major feature updates的直接链接" title="Major feature updates的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="introduces-assert-sink-connector">Introduces Assert Sink connector<a href="#introduces-assert-sink-connector" class="hash-link" aria-label="Introduces Assert Sink connector的直接链接" title="Introduces Assert Sink connector的直接链接">​</a></h3><p>Assert Sink connector is introduced in SeaTunnel version 2.1.3to verify data correctness. Special thanks to Lhyundeadsoul for his contribution.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-two-transforms">Add two Transforms<a href="#add-two-transforms" class="hash-link" aria-label="Add two Transforms的直接链接" title="Add two Transforms的直接链接">​</a></h3><p>In addition, the 2.1.3 version also adds two Transforms, NullRate and Nulltf, which are used to detect data quality and convert null values ​​in the data to generate default values. These two Transforms can effectively improve the availability of data and reduce the frequency of abnormal situations. Special thanks to wsyhj and Interest1-wyt for their contributions.</p><p>At present, SeaTunnel has supported 9 types of Transforms including Common Options, Json, NullRate, Nulltf, Replace, Split, SQL, UDF, and UUID, and the community is welcome to contribute more Transform types.</p><p>For details of Transform, please refer to the official documentation: <a href="https://seatunnel.apache.org/docs/2.1.3/category/transform" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.1.3/category/transform</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="clickhousefile-connector-supports-rsync-data-transfer-method-now">ClickhouseFile connector supports Rsync data transfer method now<a href="#clickhousefile-connector-supports-rsync-data-transfer-method-now" class="hash-link" aria-label="ClickhouseFile connector supports Rsync data transfer method now的直接链接" title="ClickhouseFile connector supports Rsync data transfer method now的直接链接">​</a></h3><p>At the same time, SeaTunnel 2.1.3 version brings Rsync data transfer mode support to ClickhouseFile connector, users can now choose SCP and Rsync data transfer modes. Thanks to Emor-nj for contributing to this feature.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="specific-feature-updates">Specific feature updates:<a href="#specific-feature-updates" class="hash-link" aria-label="Specific feature updates:的直接链接" title="Specific feature updates:的直接链接">​</a></h3><ul><li>Flink Fake data supports BigInteger type <a href="https://github.com/apache/incubator-seatunnel/pull/2118" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2118</a></li><li>Add Flink Assert Sink connector <a href="https://github.com/apache/incubator-seatunnel/pull/2022" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2022</a></li><li>Spark ClickhouseFile connector supports Rsync data file transfer method <a href="https://github.com/apache/incubator-seatunnel/pull/2074" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2074</a></li><li>Add Flink Assert Sink e2e module <a href="https://github.com/apache/incubator-seatunnel/pull/2036" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2036</a></li><li>Add NullRate Transform for detecting data quality <a href="https://github.com/apache/incubator-seatunnel/pull/1978" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/1978</a></li><li>Add Nulltf Transform for setting defaults <a href="https://github.com/apache/incubator-seatunnel/pull/1958" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/1958</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization">Optimization<a href="#optimization" class="hash-link" aria-label="Optimization的直接链接" title="Optimization的直接链接">​</a></h3><ul><li>Refactored Spark TiDB-related parameter information</li><li>Refactor the code to remove redundant code warning information</li><li>Optimize connector jar package loading logic</li><li>Add Plugin Discovery module</li><li>Add documentation for some modules</li><li>Upgrade common-collection from version 4 to 4.4</li><li>Upgrade common-codec version to 1.13</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fix">Bug Fix<a href="#bug-fix" class="hash-link" aria-label="Bug Fix的直接链接" title="Bug Fix的直接链接">​</a></h3><p>In addition, in response to the feedback from users of version 2.1.2, we also fixed some usability issues, such as the inability to use the same components of Source and Sink, and further improved the stability.</p><ul><li>Fixed the problem of Hudi Source loading twice</li><li>Fix the problem that the field TwoPhaseCommit is not recognized after Doris 0.15</li><li>Fixed abnormal data output when accessing Hive using Spark JDBC</li><li>Fix JDBC data loss when partition_column (partition mode) is set</li><li>Fix KafkaTableStream schema JSON parsing error</li><li>Fix Shell script getting APP_DIR path error</li><li>Updated Flink RunMode enumeration to get correct help messages for run modes</li><li>Fix the same source and sink registered connector cache error</li><li>Fix command line parameter -t( — check) conflict with Flink deployment target parameter</li><li>Fix Jackson type conversion error problem</li><li>Fix the problem of failure to run scripts in paths other than SeaTunnel_Home</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="acknowledgment">Acknowledgment<a href="#acknowledgment" class="hash-link" aria-label="Acknowledgment的直接链接" title="Acknowledgment的直接链接">​</a></h3><p>Thanks to all the contributors (GitHub ID, in no particular order,), it is your efforts that fuel the launch of this version, and we look forward to more contributions to the Apache SeaTunnel(Incubating) community!</p><p><code>leo65535, CalvinKirs, mans2singh, ashulin, wanghuan2054, lhyundeadsoul, tobezhou33, Hisoka-X, ic4y, wsyhj, Emor-nj, gleiyu, smallhibiscus, Bingz2, kezhenxu94, youyangkou, immustard, Interest1-wyt, superzhang0929, gaaraG, runwenjun</code></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Upcoming API Connector Development Analysis]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/Upcoming API Connector Development Analysis</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/Upcoming API Connector Development Analysis</guid>
            <pubDate>Thu, 23 Jun 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API.]]></description>
            <content:encoded><![CDATA[<p>After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="priliminary-setup">Priliminary Setup<a href="#priliminary-setup" class="hash-link" aria-label="Priliminary Setup的直接链接" title="Priliminary Setup的直接链接">​</a></h2><ul><li><p>Environment configuration: JDK8 and Scala2.11 are recommended.</p></li><li><p>As before, we need to download the latest code locally through git and import it into the IDE, project address: <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> . At the same time, switch the branch to api-draft, and currently use this branch to develop the new version of the API and the corresponding Connector. The project structure is as follows:</p><p><img loading="lazy" alt="Project Structure" src="/zh-CN/assets/images/0-82b23e9c80c8b70ce10feaccfc96a2a6.png" width="583" height="1014" class="img_ev3q"></p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Prerequisites的直接链接" title="Prerequisites的直接链接">​</a></h2><ul><li><p>At present, in order to distinguish different Connectors, we put the connectors that support</p><ul><li>Flink/Spark under the <code>seatunnel-connectors/seatunnel-connectors-flink(spark)</code> module.</li><li>New version of the Connector is placed under the <code>seatunnel-connectors/seatunnel-connectors-seatunnel</code> module.</li></ul><p>As we can see from the above figure, we have implemented Fake, Console, Kafka Connector, and Clickhouse Connector is also being implemented.</p></li><li><p>At present, the data type we support is SeaTunnelRow, so no matter the type of data generated by the Source or the type of data consumed by the Sink, it should be SeaTunnelRow.</p></li></ul><h1>Development of Connector</h1><p>Taking Fake Connector as an example, let's introduce how to implement a new Connector:</p><ul><li><p>Create a corresponding module with a path under <code>seatunnel-connectors-seatunnel</code>, which is at the same level as other new connectors.</p></li><li><p>Modify the <code>seatunnel-connectors-seatunnel/pom.xml</code> file, add a new module to modules, modify <code>seatunnel-connectors-seatunnel/seatunnel-connector-seatunnel-fake/pom.xml</code>, add seatunnel-api dependencies, and correct parent Quote. The resulting style is as follows:</p><p><img loading="lazy" alt="Style" src="/zh-CN/assets/images/1-27a269d360e9ee05b1dd696eeb0aa8e4.png" width="949" height="568" class="img_ev3q"></p></li><li><p>The next step is to create the corresponding package and related classes, create FakeSource, and need to inherit SeaTunnel Source.</p><ul><li>Note : The Source of SeaTunnel adopts the design of stream and batch integration. The Source of SeaTunnel determines whether current Source is a stream or batch through attribute getBoundedness.</li></ul><p>So you can specify a Source as a stream or batch by dynamic configuration (refer to the default method). The configuration defined by the user in the configuration file can be obtained through the prepare method to realize the customized configuration.</p><p>Then create FakeSourceReader, FakeSource SplitEnumerator, and FakeSourceSplit to inherit the corresponding abstract classes (which can be found in the corresponding classes). As long as we implement the corresponding methods of these classes, then our SeaTunnel Source Connector is basically completed.</p></li><li><p>Next, just follow the existing example to write the corresponding code. The most important one is the FakeSource Reader, which defines how we obtain data from the outside, which is the most critical part of the Source Connector. Every time a piece of data is generated, we need to place it in the collector as shown:</p><p><img loading="lazy" alt="Source" src="/zh-CN/assets/images/2-6e56482ef5f497868040295fe7edff23.png" width="935" height="424" class="img_ev3q"></p></li><li><p>After the code development is complete, we need to configure the configuration file <code>plugin-mapping.properties</code> located under <code>seatunnel-connectors/modules</code>. Adding a seatunnel
<code>.source.FakeSource = seatunnel-connector-fake</code>
means that SeaTunnel can find the jar package corresponding to the project by looking for a Source named FakeSource. This allows the Connector to be used in the normal configuration file.</p></li><li><p>For a detailed description of writing Source and Sink and SeaTunnel API, please refer to the introduction at <code>seatunnel-connectors/seatunnel-connectors-seatunnel/ README.zh.md</code>.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="connector-testing">Connector Testing<a href="#connector-testing" class="hash-link" aria-label="Connector Testing的直接链接" title="Connector Testing的直接链接">​</a></h2><ul><li>For testing, we can find the <code>seatunnel-flink(spark)-new-connector-example</code> module in seatunnel-examples, and test it against different engines to ensure that the performance of the Connector is as consistent as possible. If you find any discrepancies, you can mark them in the document, modify the configuration file under resource, add our Connector to the configuration, and introduce <code>seatunnel-flink(spark)-new-connector-example/pom.xml</code> dependency, you can execute <code>SeaTunnelApiExample</code> to test.</li><li>The default is stream processing mode, and the execution mode is switched to batch mode by modifying <code>job.mode=BATCH</code> in the environment of the configuration file.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="submit-pr">Submit PR<a href="#submit-pr" class="hash-link" aria-label="Submit PR的直接链接" title="Submit PR的直接链接">​</a></h2><p>When our Connector is ready, we can submit PR to github. After reviewing by other partners, our contributed Connector will become part of SeaTunnel!</p>]]></content:encoded>
            <category>Meetup</category>
        </item>
        <item>
            <title><![CDATA[Apache SeaTunnel 与计算引擎的解耦之道，重构API我们做了些什么]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/05/31/engine</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/05/31/engine</guid>
            <pubDate>Tue, 31 May 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[Apache SeaTunnel (Incubating) 与 Apache Inlong (Incubating) 的5月联合Meetup中，第二位分享的嘉宾是来自白鲸开源的高级工程师李宗文。在使用Apache SeaTunnel (Incubating) 的过程中，他发现了 Apache SeaTunnel (Incubating) 存在的四大问题：Connector实现次数多、参数不统一、难以支持多个版本的引擎以及引擎升级难的问题。为了解决以上的难题，李宗文将目标放在将Apache SeaTunnel (Incubating)与计算引擎进行解耦，重构其中Source与Sink API，实现改良了开发体验。]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="/zh-CN/assets/images/0-1b0b7ccac22c107239f10a37321c7719.jpg" width="1920" height="1316" class="img_ev3q"></p><p>Apache SeaTunnel (Incubating) 与 Apache Inlong (Incubating) 的5月联合Meetup中，第二位分享的嘉宾是来自白鲸开源的高级工程师李宗文。在使用Apache SeaTunnel (Incubating) 的过程中，他发现了 Apache SeaTunnel (Incubating) 存在的四大问题：Connector实现次数多、参数不统一、难以支持多个版本的引擎以及引擎升级难的问题。为了解决以上的难题，李宗文将目标放在将Apache SeaTunnel (Incubating)与计算引擎进行解耦，重构其中Source与Sink API，实现改良了开发体验。</p><p>本次演讲主要包含四个部分：</p><ol><li>Apache SeaTunnel (Incubating)<strong>重构的背景和动机</strong></li><li>Apache SeaTunnel (Incubating)<strong>重构的目标</strong></li><li>Apache SeaTunnel (Incubating)<strong>重构整体的设计</strong></li><li>Apache SeaTunnel (Incubating) <strong>Source API的设计</strong></li><li>Apache SeaTunnel (Incubating) <strong>Sink API的设计</strong></li></ol><p><img loading="lazy" src="/zh-CN/assets/images/1-cbc169e3bc121f1008b807103abe9fd8.jpg" width="1440" height="810" class="img_ev3q"></p><p><strong>李宗文</strong></p><p>白鲸开源 高级工程师</p><p>Apache SeaTunnel(Incubating)</p><p>&amp; Flink Contributor, Flink CDC &amp; Debezium Contributor</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-重构的背景与动机"><strong>01</strong> 重构的背景与动机<a href="#01-重构的背景与动机" class="hash-link" aria-label="01-重构的背景与动机的直接链接" title="01-重构的背景与动机的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-apache-seatunnelincubating与引擎耦合">01 Apache SeaTunnel(Incubating)与引擎耦合<a href="#01-apache-seatunnelincubating与引擎耦合" class="hash-link" aria-label="01 Apache SeaTunnel(Incubating)与引擎耦合的直接链接" title="01 Apache SeaTunnel(Incubating)与引擎耦合的直接链接">​</a></h3><p>用过Apache SeaTunnel (Incubating) 的小伙伴或者开发者应该知道，目前Apache SeaTunnel (Incubating) 与引擎完全耦合，完全基于Spark、Flink开发，其中的配置文件参数都基于Flink、Spark引擎。从贡献者和用户的角度出发，我们能发现一些问题。</p><p><strong>从贡献者的角度</strong>：反复实现Connector，没有收获感；潜在贡献者由于引擎版本不一致无法贡献社区；</p><p><strong>从用户的角度</strong>：目前很多公司采用Lambda架构，离线作业使用Spark，实时作业使用Flink， 使用中就会发现SeaTunnel 的Connector可能Spark有，但是Flink没有，以及两个引擎对于同一存储引擎的Connector的参数也不统一，有较高的使用成本，脱离了SeaTunnel简单易用的初衷；还有用户提问说目前支不支持Flink的1.14版本，按照目前SeaTunnel的架构，想要支持Flink的1.14就必须抛弃之前的版本，因此这也会对之前版本的用户造成很大的问题。</p><p>因此，我们不管是做引擎升级或者支持更多的版本的用户都很困难。</p><p>另外Spark和Flink都采用了Chandy-lamport算法实现的Checkpoint容错机制，也在内部进行了DataSet与DataStream的统一，以此为前提我们认为解耦是可行的。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-apache-seatunnelincubating与引擎解耦"><strong>02</strong> Apache SeaTunnel(Incubating)与引擎解耦<a href="#02-apache-seatunnelincubating与引擎解耦" class="hash-link" aria-label="02-apache-seatunnelincubating与引擎解耦的直接链接" title="02-apache-seatunnelincubating与引擎解耦的直接链接">​</a></h2><p>因此为了解决以上提出的问题，我们有了以下的目标：</p><ol><li><strong>Connector只实现一次</strong>：针对参数不统一、Connector多次实现的问题，我们希望实现一个统一的Source 与Sink API;</li><li><strong>支持多个版本的Spark与Flink引擎</strong>：在Source与Sink API上再加入翻译层去支持多个版本与Spark和Flink引擎，解耦后这个代价会小很多。</li><li><strong>明确Source的分片并行逻辑和Sink的提交逻辑</strong>：我们必须提供一个良好的API去支持Connector开发；</li><li><strong>支持实时场景下的数据库整库同步</strong>：这个是目前很多用户提到<strong>需要CDC</strong>支持衍生的需求。我之前参与过Flink CDC社区，当时有许多用户提出在CDC的场景中，如果直接使用Flink CDC的话会导致每一个表都持有一个链接，当遇到需要整库同步需求时，千张表就有千个链接，该情况无论是对于数据库还是DBA都是不能接受的，如果要解决这个问题，最简单的方式就是引入Canal、Debezium等组件，使用其拉取增量数据到Kafka等MQ做中间存储，再使用Flink SQL进行同步，这实际已经违背了Flink CDC最早减少链路的想法，但是Flink CDC的定位只是一个Connector，无法做全链路的需求，所以该proposal在Flink CDC社区中没有被提出，我们借着本次重构，将proposa提交到了SeaTunnel社区中。</li><li><strong>支持元信息的自动发现与存储</strong>：这一部分用户应该有所体验，如Kafka这类存储引擎，没有记录数据结构的功能，但我们在读取数据时又必须是结构化的，导致每次读取一个topic之前，用户都必须定义topic的结构化数据类型，我们希望做到用户只需要完成一次配置，减少重复的操作。</li></ol><p>可能也有同学有疑惑为什么我们不直接使用Apache Beam，Beam的Source分为BOUNDED与UNBOUNDED，也就是需要实现两遍，并且有些Source与Sink的特性也不支持，具体所需的特性在后面会提到；</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-apache-seatunnelincubating重构整体的设计"><strong>03</strong> Apache SeaTunnel(Incubating)重构整体的设计<a href="#03-apache-seatunnelincubating重构整体的设计" class="hash-link" aria-label="03-apache-seatunnelincubating重构整体的设计的直接链接" title="03-apache-seatunnelincubating重构整体的设计的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/1-cbc169e3bc121f1008b807103abe9fd8.jpg" width="1440" height="810" class="img_ev3q"></p><p>Apache SeaTunnel(Incubating) API总体结构的设计如上图；</p><p><strong>Source &amp; Sink API</strong>：数据集成的核心API之一，明确Source的分片并行逻辑和Sink的提交逻辑，用于实现Connector；</p><p><strong>Engine API</strong>：</p><p>Translation: 翻译层，用于将SeaTunnel的Souce与Sink API翻译成引擎内部可以运行的Connector；</p><p><strong>Execution</strong>：执行逻辑，用于定义Source、Transform、Sink等操作在引擎内部的执行逻辑；</p><p><strong>Table API</strong>：</p><p><strong>Table SPI</strong>：主要用于以SPI的方式暴露Source与Sink接口，并明确Connector的必填与可选参数等；</p><p><strong>DataType</strong>：SeaTunnel的数据结构，用于隔离引擎，声明Table Schema等；</p><p><strong>Catalog</strong>：用于获取Table Scheme、Options等；</p><p><strong>Catalog Storage</strong>: 用于存储用户定义Kafka等非结构化引擎的Table Scheme等；</p><p><img loading="lazy" src="/zh-CN/assets/images/2-507471aef3b2e7dc6ee4bc03188bc784.jpg" width="1440" height="810" class="img_ev3q"></p><p><strong>从上图是我们现在设想的执行流程</strong>：</p><ol><li>从配置文件或UI等方式获取任务参数；</li><li>通过参数从Catalog中解析得到Table Schema、Option等信息；</li><li>以SPI方式拉起SeaTunnel的Connector，并注入Table信息等；</li><li>将SeaTunnel的Connector翻译为引擎内部的Connector；</li><li>执行引擎的作业逻辑，图中的多表分发目前只存在CDC整库同步场景下，其他Connector都是单表，不需要分发逻辑；</li></ol><p>从以上可以看出，最难的部分是<strong>如何将Apache SeaTunnel(Incubating) 的Source和Sink翻译成引擎内部的Source和Sink。</strong></p><p>当下许多用户不仅把Apache SeaTunnel (Incubating) 当做一个数据集成方向的工具，也当做数仓方向的工具，会使用很多Spark和Flink的SQL，我们目前希望能够保留这样的SQL能力，让用户实现无缝升级。</p><p><img loading="lazy" src="/zh-CN/assets/images/3-68113e215aa8e91a5d2469ccb37a3c22.jpg" width="1440" height="810" class="img_ev3q"></p><p>根据我们的调研，如上图，是对Source与Sink的理想执行逻辑，由于SeaTunnel以WaterDrop孵化，所以图上的术语偏向Spark；</p><p>理想情况下，在Driver上可以运行Source和Sink的协调器，然后Worker上运行Source的Reader和Sink的Writer。在Source协调器方面，我们希望它能支持几个能力。</p><p><strong>一、是数据的分片逻辑</strong>，可以将分片动态添加到Reader中。</p><p><strong>二、是可以支持Reader的协调</strong>。SourceReader用于读取数据，然后将数据发送到引擎中流转，最终流转到Source Writer中进行数据写入，同时Writer可以支持二阶段事务提交，并由Sink的协调器支持Iceberg等Connector的聚合提交需求；</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-source-api"><strong>04</strong> Source API<a href="#04-source-api" class="hash-link" aria-label="04-source-api的直接链接" title="04-source-api的直接链接">​</a></h2><p>通过我们的调研，发现Source所需要的以下特性：</p><ol><li><strong>统一离线和实时API</strong>：Source只实现一次，同时支持离线和实时；</li><li><strong>能够支持并行读取</strong>：比如Kafka每一个分区都生成一个的读取器，并行的执行；</li><li><strong>支持动态添加分片</strong>：比如Kafka定于一个topic正则，由于业务量的需求，需要新增一个topic，该Source API可以支持我们动态添加到作业中。</li><li><strong>支持协调读取器的工作</strong>：这个目前只发现在CDC这种Connector需要支持。CDC目前都是基于Netfilx的DBlog并行算法去支持，该情况在全量同步和增量同步两个阶段的切换时需要协调读取器的工作。</li><li><strong>支持单个读取器处理多张表</strong>：即由前面提到的支持实时场景下的数据库整库同步需求；</li></ol><p><img loading="lazy" src="/zh-CN/assets/images/4-ac907e3f8e305b9f6585d2171013a973.jpg" width="1440" height="810" class="img_ev3q"></p><p>对应以上需求，我们做出了基础的API，如上图，目前代码以提交到Apache SeaTunnel(Incubating)的社区中api-draft分支，感兴趣的可以查看代码详细了解。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="如何适配spark和flink引擎"><strong>如何适配Spark和Flink引擎</strong><a href="#如何适配spark和flink引擎" class="hash-link" aria-label="如何适配spark和flink引擎的直接链接" title="如何适配spark和flink引擎的直接链接">​</a></h3><p>Flink与Spark都在后面统一了DataSet与DataStream API，即能够支持前两个特性，那么对于剩下的3个特性：</p><ul><li>如何支持动态添加分片？</li><li>如何支持协调读取器？</li><li>如何支持单个读取器处理多张表？</li></ul><p>带着问题，进入目前的设计。</p><p><img loading="lazy" src="/zh-CN/assets/images/5-f743cf882b36bb3c383c66dec4bad95f.jpg" width="1440" height="810" class="img_ev3q"></p><p>我们发现除了<strong>CDC</strong>之外，其他Connector是不需要协调器的，针对不需要协调器的，我们会有一个支持并行的Source，并进行引擎翻译。</p><p>如上图中左边是一个<strong>分片的enumerator</strong>，可以列举source需要哪些分片，有哪些分片，实时进行分片的枚举，随后将每个分片分发到真正的数据读取模块SourceReader中。<strong>对于离线与实时作业的区分使用Boundedness标记</strong>，Connector可以在分片中标记是否有停止的Offset，如Kafka可以支持实时，同时也可以支持离线。ParallelSource可以在引擎设置任意并行度，以支持并行读取。</p><p><img loading="lazy" src="/zh-CN/assets/images/6-ef5d53449db28f4466c14827335d07a6.jpg" width="1440" height="810" class="img_ev3q"></p><p>在需要协调器的场景，如上图，需要在Reader和Enumerator之间进行Event传输，<strong> Enumerator</strong>通过Reader发送的Event进行协调工作。<strong>Coordinated Source</strong>需要在引擎层面保证单并行度，以保证数据的一致性；当然这也不能良好的使用引擎的内存管理机制，但是取舍是必要的；</p><p><strong>对于最后一个问题，我们如何支持单个读取器处理多张表。这会涉及到Table API层</strong>，通过Catalog读取到了所有需要的表后，有些表可能属于一个作业，可以通过一个链接去读取，有些可能需要分开，这个依赖于Source是怎么实现的。基于这是一个特殊需求，我们想要减少普通开发者的难度，在Table API这一层，我们会提供一个SupportMultipleTable接口，用于声明Source支持多表的读取。Source在实现时，要根据多张表实现对应的反序列化器。针对衍生的多表数据如何分离，Flink将采用Side Output机制，Spark预想使用Filter或Partition机制。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-sink-api"><strong>05</strong> Sink API<a href="#05-sink-api" class="hash-link" aria-label="05-sink-api的直接链接" title="05-sink-api的直接链接">​</a></h2><p>目前Sink所需的特性并不是很多，<strong>经过调研目前发现有三个需求</strong>：</p><ol><li>幂等写入，这个不需要写代码，主要看存储引擎是否能支持。</li><li>分布式事务，主流是二阶段提交，如Kafka都是可以支持分布式事务的。</li><li>聚合提交，对于Iceberg、hoodie等存储引擎而言，我们不希望有小文件问题，于是期望将这些文件聚合成一个文件，再进行提交。</li></ol><p>基于以上三个需求，我们有对应的<strong>三个API</strong>，分别是<strong>SinkWriter、SinkCommitter、SinkAggregated Committer</strong>。SinkWriter是作为基础写入，可能是幂等写入，也可能不是。SinkCommitter支持二阶段提交。SinkAggregatedCommitter支持聚合提交。</p><p><img loading="lazy" src="/zh-CN/assets/images/7-e9830d63f81f7139a7cd1d4d9b9f5e43.jpg" width="1440" height="810" class="img_ev3q"></p><p>理想状态下，<strong>AggregatedCommitter</strong>单并行的在Driver中运行，Writer与Committer运行在Worker中，可能有多个并行度，每个并行度都有自己的预提交工作，然后把自己提交的信息发送给Aggregated Committer再进行聚合。</p><p><strong>目前Spark和Flink的高版本都支持在Driver</strong>(Job Manager)运行AggregatedCommitter，worker(Job Manager)运行writer和Committer。</p><p><img loading="lazy" src="/zh-CN/assets/images/8-aa0537c76d15543b46623445ff00e490.jpg" width="1440" height="810" class="img_ev3q"></p><p>但是对于<strong>Flink低版本</strong>，无法支持AggregatedCommitter在JM中运行，我们也进行翻译适配的设计。Writer与Committer会作为前置的算子，使用Flink的ProcessFunction进行包裹，支持并发的预提交与写入工作，基于Flink的Checkpoint机制实现二阶段提交，这也是目前Flink的很多Connector的2PC实现方式。这个ProcessFunction会将预提交信息发送到下游的Aggregated Committer中，Aggregated Committer可以采用SinkFunction或Process Function等算子包裹，当然，<strong>我们需要保证AggregatedCommitter只会启动一个，即单并行度</strong>，否则聚合提交的逻辑就会出现问题。</p><p>感谢各位的观看，如果大家对具体实现感兴趣，可以去 Apache SeaTunnel (Incubating) 的社区查看<strong>api-draft</strong>分支代码，谢谢大家。</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[百亿级数据同步，如何基于 SeaTunnel 的 ClickHouse 实现？]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/05/10/ClickHouse</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/05/10/ClickHouse</guid>
            <pubDate>Tue, 10 May 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[作者 | Apache SeaTunnel(Incubating) Contributor 范佳]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="/zh-CN/assets/images/0-c3f068094d4f0308d7100502a6162925.jpg" width="1920" height="1275" class="img_ev3q"></p><p>作者 | Apache SeaTunnel(Incubating) Contributor 范佳</p><p>整理 | 测试工程师 冯秀兰</p><p>对于百亿级批数据的导入，传统的 JDBC 方式在一些海量数据同步场景下的表现并不尽如人意。为了提供更快的写入速度，Apache SeaTunnel(Incubating) 在刚刚发布的 2.1.1 版本中提供了 ClickhouseFile-Connector 的支持，以实现 Bulk load 数据写入。</p><p>Bulk load 指把海量数据同步到目标 DB 中，目前 SeaTunnel 已实现数据同步到 ClickHouse 中。</p><p>在 Apache SeaTunnel(Incubating) 4 月 Meetup 上，Apache SeaTunnel(Incubating) Contributor 范佳分享了《基于 SeaTunnel 的 ClickHouse bulk load 实现》，详细讲解了 ClickHouseFile 高效处理海量数据的具体实现原理和流程。</p><p>感谢本文整理志愿者 测试工程师 冯秀兰 对 Apache SeaTunnel(Incubating) 项目的支持！</p><p>本次演讲主要包含七个部分：</p><ul><li>ClickHouse Sink 现状
</li><li>ClickHouse Sink 弱场景
</li><li>ClickHouseFile 插件介绍
</li><li>ClickHouseFile 核心技术点
</li><li>ClickHouseFile 插件的实现解析
</li><li>插件能力对比
</li><li>后期优化方向
</li></ul><p><img loading="lazy" src="/zh-CN/assets/images/0-1-56defefcc273a6e21b09dd483bf95914.png" width="1171" height="1171" class="img_ev3q"></p><p>​范 佳白鲸开源 高级工程师</p><h1>01 ClickHouse Sink 现状</h1><p>现阶段，SeaTunnel 把数据同步到 ClickHouse 的流程是：只要是 SeaTunnel 支持的数据源，都可以把数据抽取出来，抽取出来之后，经过转换（也可以不转换），直接把源数据写入 ClickHouse sink connector 中，再通过 JDBC 写入到 ClickHouse 的服务器中。</p><p><img loading="lazy" src="/zh-CN/assets/images/1-76284c6612152506e0111e0f0d25d0f5.png" width="1139" height="585" class="img_ev3q"></p><p>但是，通过传统的 JDBC 写入到 ClickHouse 服务器中会存在一些问题。</p><p>首先，现阶段使用的工具是 ClickHouse 提供的驱动，实现方式是通过 HTTP，然而 HTTP 在某些场景下，实现效率不高。其次是海量数据，如果有重复数据或者一次性写入大量数据，使用传统的方式是生成对应的插入语句，通过 HTTP 发送到 ClickHouse 服务器端，在服务器端来进行逐条或分批次解析、执行，无法实现数据压缩。</p><p>最后就是我们通常会遇到的问题，数据量过大可能导致 SeaTunnel 端 OOM，或者服务器端因为写入数据量过大，频率过高，导致服务器端挂掉。</p><p>于是我们思考，是否有比 HTTP 更快的发送方式？如果可以在 SeaTunnel 端做数据预处理或数据压缩，那么网络带宽压力会降低，传输速率也会提高。</p><h1>02 ClickHouse Sink 的弱场景</h1><p>如果使用 HTTP 传输协议，当数据量过大，批处理以微批的形式发送请求，HTTP 可能处理不过来；</p><p>太多的 insert 请求，服务器压力大。假设带宽可以承受大量的请求，但服务器端不一定能承载。线上的服务器不仅需要数据插入，更重要的是查询数据为其他业务团队使用。若因为插入数据过多导致服务器集群宕机，是得不偿失的。</p><h1>03 ClickHouse File 核心技术点</h1><p>针对这些 ClickHouse 的弱场景，我们想，有没有一种方式，既能在 Spark 端就能完成数据压缩，还可以在数据写入时不增加 Server 的资源负载，并且能快速写入海量数据？于是我们开发了 ClickHouseFile 插件来满足这些需求。</p><p>ClickHouseFile 插件的关键技术是 ClickHouse -local。ClickHouse-local 模式可以让用户能够对本地文件执行快速处理，而无需部署和配置 ClickHouse 服务器。ClickHouse-local 使用与 ClickHouse Server 相同的核心，因此它支持大多数功能以及相同的格式和表引擎。</p><p>因为有这 2 个特点,这意味着用户可以直接处理本地文件，而无需在 ClickHouse 服务器端做处理。由于是相同的格式，我们在远端或者 SeaTunnel 端进行的操作所产生的数据和服务器端是无缝兼容的，可以使用 ClickHouse local 来进行数据写入。ClickHouse local 是实现 ClickHouseFile 的核心技术点，因为有了这个插件，现阶段才能够实现 ClickHouse file 连接器。</p><p>ClickHouse local 核心使用方式：</p><p><img loading="lazy" src="/zh-CN/assets/images/2-2367f70ae655c30a94a2ec65e67a6b26.png" width="1112" height="262" class="img_ev3q"></p><p>第一行：将数据通过 Linux 管道传递给 ClickHouse-local 程序的 test_table 表。</p><p>第二至五行：创建一个 result_table 表用于接收数据。</p><p>第六行：将数据从 test<!-- -->_<!-- -->table 到 result<!-- -->_<!-- -->table 表。</p><p>第七行：定义数据处理的磁盘路径。</p><p>通过调用 Clickhouse-local 组件，实现在 Apache SeaTunnel(Incubating) 端完成数据文件的生成，以及数据压缩等一系列操作。再通过和 Server 进行通信，将生成的数据直接发送到 Clickhouse 的不同节点，再将数据文件提供给节点查询。</p><p>原阶段和现阶段实现方式对比：</p><p><img loading="lazy" src="/zh-CN/assets/images/3-6204c709b48243f88914bfd492dc67f2.png" width="1272" height="576" class="img_ev3q"></p><p>原来是 Spark 把数据包括 insert 语句，发送给服务器端，服务器端做 SQL 的解析，表的数据文件生成、压缩，生成对应的文件、建立对应索引。若使用 ClickHouse local 技术，则由 SeaTunnel 端做数据文件的生成、文件压缩，以及索引的创建，最终产出就是给服务器端使用的文件或文件夹，同步给服务器后，服务器就只需对数据查询，不需要做额外的操作。</p><h1>04 核心技术点</h1><p><img loading="lazy" src="/zh-CN/assets/images/4-d47e1da865afa7ea4de50b2d6e4b6ac1.png" width="1164" height="435" class="img_ev3q"></p><p>以上流程可以促使数据同步更加高效，得益于我们对其中的三点优化。</p><p>第一，数据实际上师从管道传输到 ClickHouseFile，在长度和内存上会有限制。为此，我们将 ClickHouse connector，也就是 sink 端收到的数据通过 MMAP 技术写入临时文件，再由 ClickHouse local 读取临时文件的数据，生成我们的目标 local file，以达到增量读取数据的效果，解决 OM 的问题。</p><p><img loading="lazy" src="/zh-CN/assets/images/5-9f00635b1727843f705cd5a28632e2e4.png" width="1206" height="565" class="img_ev3q"></p><p>第二，支持分片。因为如果在集群中使用，如果只生成一个文件或文件夹，实际上文件只分发到一个节点上，会大大降低查询的性能。因此，我们进行了分片支持，用户可以在配置文件夹中设置分片的 key，算法会将数据分为多个 log file，写入到不同的集群节点中，大幅提升读取性能。</p><p><img loading="lazy" src="/zh-CN/assets/images/6-35b30550d6a18fbea49856083aa85094.png" width="1043" height="558" class="img_ev3q"></p><p>第三个重要的优化是文件传输，目前 SeaTunnel 支持两种文件传输方式，一种是 SCP，其特点是安全、通用、无需额外配置；另一种是 RSYNC，其有点事快速高效，支持断点续传，但需要额外配置，用户可以根据需要选择适合自己的方式。</p><h1>05 插件实现解析</h1><p>概括而言，ClickHouseFile 的总体实现流程如下：</p><p><img loading="lazy" src="/zh-CN/assets/images/7-1be978da30a55fe0289c683f2ae61aac.png" width="533" height="635" class="img_ev3q"></p><ul><li>缓存数据，缓存到 ClickHouse sink 端；
</li><li>调用本地的 ClickHouse-local 生成文件；
</li><li>将数据发送到 ClickHouse 服务端；
</li><li>执行 ATTACH 命令
</li></ul><p>通过以上四个步骤，生成的数据达到可查询的状态。</p><h1>06 插件能力对比</h1><p><img loading="lazy" src="/zh-CN/assets/images/8-261e7ba686f3fadf5d7c1445e9be5b66.png" width="1071" height="485" class="img_ev3q"></p><p>从数据传输角度来说，ClickHouseFile 更适用于海量数据，优势在于不需要额外的配置，通用性强，而 ClickHouseFile 配置比较复杂，目前支持的 engine 较少；</p><p>就环境复杂度来说，ClickHouse 更适合环境复杂度高的情况，不需要额外配置就能直接运行；</p><p>在通用性上，ClickHouse 由于是 SeaTunnel 官方支持的 JDBC diver，基本上支持所有的 engine 的数据写入，ClickHouseFile 支持的 engine 相对较少；从服务器压力方面来说，ClickHouseFile 的优势在海量数据传输时就体现出来了，不会对服务器造成太大的压力。</p><p>但这二者并不是竞争关系，需要根据使用场景来选择。</p><h1>07 后续计划</h1><p>目前虽然 SeaTunnel 支持 ClickHouseFile 插件，但是还有很多地方需要优化，主要包括：</p><ul><li>Rsync 支持；
</li><li>Exactly-Once 支持；
</li><li>支持 Zero Copy 传输数据文件；
</li><li>更多 Engine 的支持</li></ul>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[SeaTunnel 在孩子王的选型过程及应用改造实践]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2022/05/01/_Kidswant</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2022/05/01/_Kidswant</guid>
            <pubDate>Sun, 01 May 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[在Apache SeaTunnel(Incubating) 4 月Meetup上，孩子王大数据专家、OLAP平台架构师 袁洪军 为我们带来了《Apache SeaTunnel (Incubating)在孩子王的应用实践》。]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="/zh-CN/assets/images/0-38f7968af0b7239e9d427a85adee4452.png" width="1920" height="1080" class="img_ev3q"></p><p>在Apache SeaTunnel(Incubating) 4 月Meetup上，孩子王大数据专家、OLAP平台架构师 袁洪军 为我们带来了《Apache SeaTunnel (Incubating)在孩子王的应用实践》。</p><p>本次演讲主要包含五个部分：</p><ul><li><p>孩子王引入Apache SeaTunnel (Incubating)的背景介绍</p></li><li><p>大数据处理主流工具对比分析</p></li><li><p>Apache SeaTunnel (Incubating)的落地实践</p></li><li><p>Apache SeaTunnel (Incubating)改造中的常见问题</p></li><li><p>对孩子王未来发展方向的预测展望</p></li></ul><p><img loading="lazy" src="/zh-CN/assets/images/0-1-4c853aa726b29acc5954ba53240dc2b8.png" width="2578" height="2567" class="img_ev3q"></p><p>袁洪军</p><p>孩子王 大数据专家、OLAP 平台架构师。多年大数据平台研发管理经验，在数据资产、血缘图谱、数据治理、OLAP 等领域有着丰富的研究经验。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-背景介绍">01 背景介绍<a href="#01-背景介绍" class="hash-link" aria-label="01 背景介绍的直接链接" title="01 背景介绍的直接链接">​</a></h2><p><img loading="lazy" src="/zh-CN/assets/images/1-b67aae24cc6274ca2d6a4541848b847d.png" width="1583" height="979" class="img_ev3q"></p><p>目前孩子王的OLAP平台主要包含元数据层、任务层、存储层、SQL层、调度层、服务层以及监控层七部分，本次分享主要关注任务层中的离线任务。</p><p>其实孩子王内部有一套完整的采集推送系统，但由于一些历史遗留问题，公司现有的平台无法快速支持OLAP平台上线，因此当时公司只能选择放弃自身的平台，转而着手研发新的系统。</p><p>当时摆在OLAP面前的有三个选择：</p><p>1、给予采集推送系统做二次研发；</p><p>2、完全自研；</p><p>3、参与开源项目。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-大数据处理主流工具对比分析">02 大数据处理主流工具对比分析<a href="#02-大数据处理主流工具对比分析" class="hash-link" aria-label="02 大数据处理主流工具对比分析的直接链接" title="02 大数据处理主流工具对比分析的直接链接">​</a></h2><p>而这三项选择却各有优劣。若采基于采集推送做二次研发，其优点是有前人的经验，能够避免重复踩坑。但缺点是代码量大，研读时间、研读周期较长，而且抽象代码较少，与业务绑定的定制化功能较多，这也导致了其二开的难度较大。</p><p>若完全自研，其优点第一是开发过程自主可控，第二是可以通过Spark等一些引擎做贴合我们自身的架构，但缺点是可能会遭遇一些未知的问题。</p><p>最后如果使用开源框架，其优点一是抽象代码较多，二是经过其他大厂或公司的验证，框架在性能和稳定方面能够得到保障。因此孩子王在OLAP数据同步初期，我们主要研究了DATAX、Sqoop和SeaTunnel这三个开源数据同步工具。</p><p><img loading="lazy" src="/zh-CN/assets/images/2-a6b8a591b0c27aed589b35565edf7c02.png" width="876" height="915" class="img_ev3q"></p><p>从脑图我们可以看到，Sqoop的主要功能是针对RDB的数据同步，其实现方式是基于MAP/REDUCE。Sqoop拥有丰富的参数和命令行可以去执行各种操作。Sqoop的优点在于它首先贴合Hadoop生态，并已经支持大部分RDB到HIVE任意源的转换，拥有完整的命令集和API的分布式数据同步工具。</p><p>但其缺点是Sqoop只支持RDB的数据同步，并且对于数据文件有一定的限制，以及还没有数据清洗的概念。</p><p><img loading="lazy" src="/zh-CN/assets/images/3-3ff7bcde6e57af360cccd3d50e713b8e.png" width="2452" height="1485" class="img_ev3q"></p><p>DataX的主要功能是任意源的数据同步，通过配置化文件+多线程的方式实现，主要分为三个流程：Reader、Framework和Writer，其中Framework主要起到通信和留空的作用。</p><p>DataX的优点是它采用了插件式的开发，拥有自己的流控和数据管控，在社区活跃度上，DataX的官网上提供了许多不同源的数据推送。但DataX的缺点在于它基于内存，对数据量可能存在限制。</p><p><img loading="lazy" src="/zh-CN/assets/images/4-96dfdedcf3c28bf69a805f20cc51e960.png" width="2199" height="1860" class="img_ev3q"></p><p>Apache SeaTunnel (Incubating)做的也是任意源的数据同步，实现流程分为source、transform和sink三步，基于配置文件、Spark或Flink实现。其优点是目前官网2.1.0有非常多的插件和源的推送，基于插件式的思想也使其非常容易扩展，拥抱Spark和Flink的同时也做到了分布式的架构。要说Apache SeaTunnel (Incubating)唯一的缺点可能是目前缺少IP的调用，UI界面需要自己做管控。</p><p>综上所述，Sqoop虽然是分布式，但是仅支持RDB和HIVE、Hbase之间的数据同步且扩展能力差，不利于二开。DataX扩展性好，整体性稳定，但由于是单机版，无法分布式集群部署，且数据抽取能力和机器性能有强依赖关系。而SeaTunnel和DataX类似并弥补了DataX非分布式的问题，对于实时流也做了很好的支持，虽然是新产品，但社区活跃度高。基于是否支持分布式、是否需要单独机器部署等诸多因素的考量，最后我们选择了SeaTunnel。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-apache-seatunnel-incubating的落地实践">03 Apache SeaTunnel (Incubating)的落地实践<a href="#03-apache-seatunnel-incubating的落地实践" class="hash-link" aria-label="03 Apache SeaTunnel (Incubating)的落地实践的直接链接" title="03 Apache SeaTunnel (Incubating)的落地实践的直接链接">​</a></h2><p>在Apache SeaTunnel (Incubating)的官网我们可以看到Apache SeaTunnel (Incubating)的基础流程包括source、transform和sink三部分。根据官网的指南，Apache SeaTunnel (Incubating)的启动需要配置脚本，但经过我们的研究发现，Apache SeaTunnel (Incubating)的最终执行是依赖config文件的spark-submit提交的一个Application应用。</p><p>这种初始化方式虽然简单，但存在必须依赖Config文件的问题，每次运行任务后都会生成再进行清除，虽然可以在调度脚本中动态生成，但也产生了两个问题。1、频繁的磁盘操作是否有意义；2、是否存在更为高效的方式支持Apache SeaTunnel (Incubating)的运行。</p><p><img loading="lazy" src="/zh-CN/assets/images/5-94809b2d6f1fbbac48a249728119e5af.png" width="1597" height="973" class="img_ev3q"></p><p>基于以上考量，在最终的设计方案中，我们增加了一个统一配置模板平台模块。调度时只需要发起一个提交命令，由Apache SeaTunnel (Incubating)自身去统一配置模板平台中拉取配置信息，再去装载和初始化参数。</p><p><img loading="lazy" src="/zh-CN/assets/images/5-1-05477f37c0f5e72755d1335853ea650e.png" width="1496" height="1022" class="img_ev3q"></p><p>上图展示的便是孩子王OLAP的业务流程，主要分为三块。数据从Parquet，即Hive，通过Parquet表的方式到KYLIN和CK source的整体流程。</p><p><img loading="lazy" src="/zh-CN/assets/images/7-1317712145fdcaee3b0219f09f68a739.png" width="1205" height="766" class="img_ev3q"></p><p>这是我们建模的页面，主要通过拖拉拽的方式生成最终模型，每个表之间通过一些交易操作，右侧是针对Apache SeaTunnel (Incubating)的微处理。</p><p><img loading="lazy" src="/zh-CN/assets/images/8-d476fef7e065db158f63dd1b3291543b.jpg" width="1306" height="609" class="img_ev3q"></p><p>因此我们最终提交的命令如上，其中标红的首先是【-conf customconfig/jars】，指用户可以再统一配置模板平台进行处理，或者建模时单独指定。最后标红的【421 $start_time $end_time $taskType】Unicode，属于唯一编码。</p><p>下方图左就是我们最终调度脚本提交的38个命令，下方图右是针对Apache SeaTunnel (Incubating)做的改造，可以看到一个较为特殊的名为WaterdropContext的工具类。可以首先判断Unicode是否存在，再通过Unicode_code来获取不同模板的配置信息，避免了config文件的操作。</p><p>在最后的reportMeta则是用于在任务执行完成后上报一些信息，这也会在Apache SeaTunnel (Incubating)中完成。</p><p><img loading="lazy" src="/zh-CN/assets/images/9-018ad4a2a0de58d2d3c9c69fffbcd06c.png" width="1136" height="940" class="img_ev3q"></p><p><img loading="lazy" src="/zh-CN/assets/images/10-f0199109244a92f734e89024bef0b51b.png" width="944" height="388" class="img_ev3q"></p><p><img loading="lazy" src="data:image/png;base64,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" width="916" height="348" class="img_ev3q"></p><p>在最终完成的config文件如上，值得注意的是在transform方面，孩子王做了一些改造。首先是针对手机或者身份证号等做脱敏处理，如果用户指定字段，就按照字段做，如果不指定字段就扫描所有字段，然后根据模式匹配，进行脱敏加密。</p><p>第二transform还支持自定义处理，如上文说道OLAP建模的时候说到。加入了HideStr，可以保留一串字符的前十个字段，加密后方的所有字符，在数据安全上有所保障。</p><p>然后，在sink端，我们为了支持任务的幂等性，我们加入了pre_sql，这主要完成的任务是数据的删除，或分区的删除，因为任务在生产过程中不可能只运行一次，一旦出现重跑或补数等操作，就需要这一部分为数据的不同和正确性做考量。</p><p>在图右方的一个Clickhouse的Sink端，这里我们加入了一个is_senseless_mode，它组成了一个读写分离的无感模式，用户在查询和补数的时候不感知整体区域，而是用到CK的分区转换，即名为MOVE PARTITION TO TABLE的命令进行操作的。</p><p>此处特别说明KYLIN的Sink端，KYLIN是一个非常特殊的源，拥有自己一整套数据录入的逻辑，而且，他有自己的监控页面，因此我们给予KYLIN的改造只是简单地调用其API操作，在使用KYLIN时也只是简单的API调用和不断轮询的状态，所以KYLIN这块的资源在统一模板配置平台就被限制地很小。</p><p><img loading="lazy" src="/zh-CN/assets/images/12-61af6fe2e1a38126ec03a0291e15c1bf.jpg" width="1307" height="593" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-apache-seatunnel-incubating改造中的常见问题">04 Apache SeaTunnel (Incubating)改造中的常见问题<a href="#04-apache-seatunnel-incubating改造中的常见问题" class="hash-link" aria-label="04 Apache SeaTunnel (Incubating)改造中的常见问题的直接链接" title="04 Apache SeaTunnel (Incubating)改造中的常见问题的直接链接">​</a></h2><p>1、OOM&amp;Too many Parts</p><p>问题通常会出现在Hive到Hive的过程中，即使我们通过了自动资源的分配，但也存在数据突然间变大的情况，比如在举办了多次活动之后。这样的问题其实只能通过手动动态地调参，调整数据同步批量时间来避免。未来我们可能尽力去完成对于数据量的掌握，做到精细的控制。</p><p>2、字段、类型不一致问题</p><p>模型上线后，任务依赖的上游表或者字段，用户都会做一些修改，这些修改若无法感知，可能导致任务的失败。目前解决方法是依托血缘+快照的方式进行提前感知来避免错误。</p><p>3、自定义数据源&amp;自定义分隔符</p><p>如财务部门需要单独使用的分割符，或是jar信息，现在用户可以自己在统一配置模板平台指定加载额外jar信息以及分割符信息。</p><p>4、数据倾斜问题</p><p>这可能因为用户自己设置了并行度，但无法做到尽善尽美。这一块我们暂时还没有完成处理，后续的思路可能在Source模块中添加post处理，对数据进行打散，完成倾斜。</p><p>5、KYLIN全局字典锁问题</p><p>随着业务发展，一个cube无法满足用户使用，就能需要建立多个cube，如果多个cube之间用了相同的字段，就会遇到KYLIN全局字典锁的问题。目前解决的思路是把两个或多个任务之间的调度时间进行隔开，如果无法隔开，可以做一个分布式锁的控制。KYLIN的sink端必须要拿到锁才能运行。</p><p>05 对孩子王未来发展方向的预测展望</p><ul><li><p>多源数据同步，未来可能针对RDB源进行处理</p></li><li><p>基于实时Flink的实现</p></li><li><p>接管已有采集调度平台（主要解决分库分表的问题）</p></li><li><p>数据质量校验，像一些空值、整个数据的空置率、主时间的判断等</p></li></ul><p>我的分享就到这里，希望以后可以和社区多多交流，共同进步，谢谢！</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[智能化时代的数据集成技术革新]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/智能化时代的数据集成技术革新</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/智能化时代的数据集成技术革新</guid>
            <pubDate>Fri, 08 Apr 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[1]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" alt="1" src="/zh-CN/assets/images/1-3abb262dd4a58a521e31f7249b2058d2.png" width="900" height="383" class="img_ev3q"></p><p>可管理，可调用，可计算，可变现的数据资源才能成为资产，信息系统的互联互通使得多源和多维度的数据集成需求巨大，这就对数据处理和集成的工具提出了严苛的要求。</p><p>智能化时代，在“智慧城市”、“智慧治理”、“产品智能化”等的趋势下，企业大多面临如何实现高效数据推送，提高平台质量，以及保障数据安全的挑战。选对数据集成工具和平台，数据才能发挥出做大的作用。</p><p>Apache SeaTunnel (Incubating) 作为下一代高性能、分布式、海量数据集成框架，致力于让数据同步更简单，更高效，加快分布式数据处理能力在生产环境落地。</p><p>在 Apache SeaTunnel(Incubating) Meetup（2022&nbsp;年&nbsp;4&nbsp;月&nbsp;16日），Apache SeaTunnel(Incubating) 社区将邀请了 Apache SeaTunnel(Incubating)的资深用户，分享 Apache SeaTunnel(Incubating)在智能化生产环境中落地的最佳实践。此外，还会有贡献者现场进行 Apache SeaTunnel(Incubating)的源码解析，让你对 Apache SeaTunnel(Incubating)有一个更加全面而深入的了解。</p><p>无论你是对 Apache SeaTunnel(Incubating)抱有兴趣的初学者，还是在日常的生产实践中遭遇了复杂棘手的部署问题，都可以来到这里，与我们的讲师近距离沟通，得到你想要的答案。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-报-名-通-道">01 报 名 通 道<a href="#01-报-名-通-道" class="hash-link" aria-label="01 报 名 通 道的直接链接" title="01 报 名 通 道的直接链接">​</a></h2><p>Apache SeaTunnel (Incubating) Meetup | 4 月线上直播报名通道已开启，赶快预约吧！</p><p>时间：2022-4-16 14:00-17:00</p><p>形式：线上直播</p><p>点击链接或扫码预约报名（免费）：</p><p><img loading="lazy" alt="2" src="/zh-CN/assets/images/2-e9c01734f2c74a12f59e2f3eecd2a2c6.png" width="300" height="300" class="img_ev3q"></p><p>扫码预约报名</p><p><img loading="lazy" alt="3" src="/zh-CN/assets/images/4-b0dd1cb8a1432483e3f3202cbea5c271.png" width="498" height="507" class="img_ev3q"></p><p>扫码进直播群</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-活-动-亮-点">02 活 动 亮 点<a href="#02-活-动-亮-点" class="hash-link" aria-label="02 活 动 亮 点的直接链接" title="02 活 动 亮 点的直接链接">​</a></h2><ul><li>行业案例详解</li><li>特色功能分析</li><li>一线企业踩坑心得</li><li>开源社区实战攻略</li><li>行业技术专家面对面 Q&amp;A</li><li>惊喜礼品送不停</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-活-动-议-程">03 活 动 议 程<a href="#03-活-动-议-程" class="hash-link" aria-label="03 活 动 议 程的直接链接" title="03 活 动 议 程的直接链接">​</a></h2><p>活动当天，将有来自孩子王、oppo 的工程师现场分享来自厂商的一线前沿实践经验，还有来自白鲸开源的高级工程师对 Apache SeaTunnel(Incubating)的重要功能更新进行“硬核”讲解，干货满满。</p><p><img loading="lazy" alt="4" src="/zh-CN/assets/images/5-f80f0bce07f15e8f83f603d16825dde4.png" width="1080" height="1075" class="img_ev3q"></p><p>袁洪军&nbsp;孩子王 大数据专家、OLAP 平台架构师</p><p>多年大数据平台研发管理经验，在数据资产、血缘图谱、数据治理、OLAP 等领域有着丰富的研究经验</p><p>演讲时间：14:00-14:40</p><p>演讲题目：Apache SeaTunnel(Incubating) 在孩子王的应用实践</p><p>演讲概要： 如何实现高效数据推送？如何提高平台质量？如何保障数据安全？孩子王又对 Apache SeaTunnel(Incubating)做了哪些改造？</p><p><img loading="lazy" alt="6" src="/zh-CN/assets/images/6-8f02c4507ef50d7a109ddf227c45a7a5.png" width="1080" height="1080" class="img_ev3q"></p><p>范佳&nbsp;白鲸开源&nbsp; 高级工程师 Apache SeaTunnel Contributor</p><p>演讲时间： 14:40-15:20</p><p>演讲题目： 基于 Apache SeaTunnel(Incubating)的 Clickhouse Bulk Load 实现</p><p>演讲概要： 通过扩展 Apache SeaTunnel(Incubating)的 Connector实现 Clickhouse的 bulk load 数据同步功能。</p><p><img loading="lazy" alt="7" src="/zh-CN/assets/images/7-8b4c7798dd107093cb31b526eb3d8476.png" width="1080" height="1078" class="img_ev3q"></p><p>王子超&nbsp;oppo&nbsp;高级后端工程师</p><p>演讲时间： 15:50-16:30</p><p>演讲题目： oppo智能推荐样本中心基于 Apache SeaTunnel(Incubating)的技术革新</p><p>演讲概要： 介绍 oppo 智能推荐机器学习样本流程的演进及 Apache &nbsp;SeaTunnel(Incubating)&nbsp;在其中发挥的作用。</p><p>除了精彩的演讲之外，现场还设置了多个抽奖环节，参与抽奖有机会获得&nbsp;Apache&nbsp;SeaTunnel(Incubating)&nbsp;精美定制礼品，敬请期待~</p>]]></content:encoded>
            <category>Meetup</category>
        </item>
        <item>
            <title><![CDATA[2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall</guid>
            <pubDate>Fri, 18 Mar 2022 00:00:00 GMT</pubDate>
            <description><![CDATA[On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator.]]></description>
            <content:encoded><![CDATA[<p>On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator.</p><p><strong>Note:</strong> A&nbsp;<strong>software license</strong>&nbsp;is a legal instrument governing the use or redistribution of software. A typical software license grants the&nbsp;licensee, typically an&nbsp;end-user, permission to use one or more copies of the software in ways where such a use would otherwise potentially constitute copyright infringement of the software owner's&nbsp;exclusive rights&nbsp;under copyright. Effectively, a software license is a contract between the software developer and the user that guarantees the user will not be sued within the scope of the license. </p><p>Before and after entering the incubator, we spent a lot of time sorting through the external dependencies of the entire project to ensure compliance. It is important to note that the choice of License for open source software does not necessarily mean that the project itself is compliant. While the stringent version control process of ASF ensures compliance and legal distribution of the software license maximumly.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="release-note">Release Note<a href="#release-note" class="hash-link" aria-label="Release Note的直接链接" title="Release Note的直接链接">​</a></h2><p>We bring the following <strong>key features</strong>to this release:</p><ol><li>The kernel of the microkernel plug-in architecture is overall optimized, which is mainly in Java. And a lot of improvements are made to command line parameter parsing, plug-in loading, etc. At the same time, the users (or contributors) can choose the language to develop plug-in extensions, which greatly reduces the development threshold of plug-ins.</li><li>Overall support for Flink, while the users are free to choose the underlying engine. This version also brings a large number of Flink plug-ins and welcomes anyone to contribute more.</li><li>Provide local development fast startup environment support (example), allow contributors or users quickly and smoothly start without changing any code to facilitate rapid local development debugging. This is certainly exciting news for contributors or users who need to customize their plugins. In fact, we've had a large number of contributors use this approach to quickly test the plugin in our pre-release testing.</li><li>With Docker container installation provided, users can deploy and install Apache SeaTunnel(Incubating) via Docker extremely fast, and we will iterate around Docker &amp; K8s in the future, any interesting proposal on this is welcomed.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="specific-release-notes">Specific release notes：<a href="#specific-release-notes" class="hash-link" aria-label="Specific release notes：的直接链接" title="Specific release notes：的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="features">[Features]<a href="#features" class="hash-link" aria-label="[Features]的直接链接" title="[Features]的直接链接">​</a></h3><ul><li>Use JCommander to do command line parameter parsing, making developers focus on the logic itself.</li><li>Flink is upgraded from 1.9 to 1.13.5, keeping compatibility with older versions and preparing for subsequent CDC.</li><li>Support for Doris, Hudi, Phoenix, Druid, and other Connector plugins, and you can find complete plugin support here <a href="/zh-CN/blog/[https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel%5D(https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel)">plugins-supported-by-seatunnel</a>.</li><li>Local development extremely fast starts environment support. It can be achieved by using the example module without modifying any code, which is convenient for local debugging.</li><li>Support for installing and trying out Apache SeaTunnel(Incubating) via Docker containers.</li><li>SQL component supports SET statements and configuration variables.</li><li>Config module refactoring to facilitate understanding for the contributors while ensuring code compliance (License) of the project.</li><li>Project structure realigned to fit the new Roadmap.</li><li>CI&amp;CD support, code quality automation control (more plans will be carried out to support CI&amp;CD development).</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="acknowledgments">Acknowledgments<a href="#acknowledgments" class="hash-link" aria-label="Acknowledgments的直接链接" title="Acknowledgments的直接链接">​</a></h2><p>Thanks to the following contributors who participated in this version release (GitHub IDs, in no particular order).</p><p>Al-assad, BenJFan, CalvinKirs, JNSimba, JiangTChen, Rianico, TyrantLucifer, Yves-yuan, ZhangchengHu0923, agendazhang, an-shi-chi-fan, asdf2014, bigdataf, chaozwn, choucmei, dailidong, dongzl, felix-thinkingdata, fengyuceNv, garyelephant, kalencaya, kezhenxu94, legendtkl, leo65535, liujinhui1994, mans2singh, marklightning, mosence, nielifeng, ououtt, ruanwenjun, simon824, totalo, wntp, wolfboys, wuchunfu, xbkaishui, xtr1993, yx91490, zhangbutao, zhaomin1423, zhongjiajie, zhuangchong, zixi0825.</p><p>Also sincere gratitude to our Mentors: Zhenxu Ke, Willem Jiang, William Guo, LiDong Dai, Ted Liu, Kevin, JB for their help!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="planning-for-the-next-few-releases">Planning for the next few releases:<a href="#planning-for-the-next-few-releases" class="hash-link" aria-label="Planning for the next few releases:的直接链接" title="Planning for the next few releases:的直接链接">​</a></h2><ul><li>CDC support.</li><li>Support for the monitoring system.</li><li>UI system support.</li><li>More Connector and efficient Sink support, such as ClickHouse support will be available in the next release soon.
The follow-up <strong>Features</strong> are decided by the community consensus, and we sincerely appeal to more participation in the community construction.</li></ul><p>We need your attention and contributions:)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-status">Community Status<a href="#community-status" class="hash-link" aria-label="Community Status的直接链接" title="Community Status的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="recent-development">Recent Development<a href="#recent-development" class="hash-link" aria-label="Recent Development的直接链接" title="Recent Development的直接链接">​</a></h3><p>Since entering the Apache incubator, the contributor group has grown from 13 to 55 and continues to grow, with the average weekly community commits remaining at 20+. </p><p>Three contributors from different companies (Lei Xie, HuaJie Wang, Chunfu Wu) have been invited to become Committers on account of their contributions to the community. </p><p>We held two Meetups, where instructors from Bilibili, OPPO, Vipshop, and other companies shared their large-scale production practices based on SeaTunnel in their companies (we will hold one meetup monthly in the future, and welcome SeaTunnel users or contributors to come and share their stories about SeaTunnel).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="users-of-apache-seatunnelincubating">Users of Apache SeaTunnel(Incubating)<a href="#users-of-apache-seatunnelincubating" class="hash-link" aria-label="Users of Apache SeaTunnel(Incubating)的直接链接" title="Users of Apache SeaTunnel(Incubating)的直接链接">​</a></h3><p>Note: Only registered users are included.</p><p>Registered users of Apache SeaTunnel(Incubating) are shown below. If you are also using Apache SeaTunnel(Incubating), too, welcome to register on <a href="https://github.com/apache/incubator-seatunnel/issues/686" target="_blank" rel="noopener noreferrer">Who is using SeaTunne</a>!</p><div align="center"><img loading="lazy" src="/image/20220321/1.png" class="img_ev3q"></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ppmcs-word">PPMC's Word<a href="#ppmcs-word" class="hash-link" aria-label="PPMC's Word的直接链接" title="PPMC's Word的直接链接">​</a></h2><p>LiFeng Nie, PPMC of Apache SeaTunnel(Incubating), commented on the first Apache version release. </p><p>From the first day entering Apache Incubating, we have been working hard to learn the Apache Way and various Apache policies. Although the first release took a lot of time (mainly for compliance), we think it was well worth it, and that's one of the reasons we chose to enter Apache. We need to give our users peace of mind, and Apache is certainly the best choice, with its almost demanding license control that allows users to avoid compliance issues as much as possible and ensure that the software is circulating reasonably and legally. In addition, its practice of the Apache Way, such as public service mission, pragmatism, community over code, openness and consensus decision-making, and meritocracy, can drive the Apache SeaTunnel(Incubating) community to become more open, transparent, and diverse.</p>]]></content:encoded>
            <category>2.1.0</category>
            <category>Release</category>
        </item>
        <item>
            <title><![CDATA[如何快速地把 HDFS 中的数据导入 ClickHouse]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/hdfs-to-clickhouse</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/hdfs-to-clickhouse</guid>
            <pubDate>Thu, 30 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[ClickHouse 是面向 OLAP 的分布式列式 DBMS。我们部门目前已经把所有数据分析相关的日志数据存储至 ClickHouse 这个优秀的数据仓库之中，当前日数据量达到了 300 亿。]]></description>
            <content:encoded><![CDATA[<p>ClickHouse 是面向 OLAP 的分布式列式 DBMS。我们部门目前已经把所有数据分析相关的日志数据存储至 ClickHouse 这个优秀的数据仓库之中，当前日数据量达到了 300 亿。</p><p>之前介绍的有关数据处理入库的经验都是基于实时数据流，数据存储在 Kafka 中，我们使用 Java 或者 Golang 将数据从 Kafka 中读取、解析、清洗之后写入 ClickHouse 中，这样可以实现数据的快速接入。然而在很多同学的使用场景中，数据都不是实时的，可能需要将 HDFS 或者是 Hive 中的数据导入 ClickHouse。有的同学通过编写 Spark 程序来实现数据的导入，那么是否有更简单、高效的方法呢。</p><p>目前开源社区上有一款工具 <strong>Seatunnel</strong>，项目地址 <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a>，可以快速地将 HDFS 中的数据导入 ClickHouse。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hdfs-to-clickhouse">HDFS To ClickHouse<a href="#hdfs-to-clickhouse" class="hash-link" aria-label="HDFS To ClickHouse的直接链接" title="HDFS To ClickHouse的直接链接">​</a></h2><p>假设我们的日志存储在 HDFS 中，我们需要将日志进行解析并筛选出我们关心的字段，将对应的字段写入 ClickHouse 的表中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="log-sample">Log Sample<a href="#log-sample" class="hash-link" aria-label="Log Sample的直接链接" title="Log Sample的直接链接">​</a></h3><p>我们在 HDFS 中存储的日志格式如下， 是很常见的 Nginx 日志</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token number">10.41</span><span class="token plain">.1.28 github.com </span><span class="token number">114.250</span><span class="token plain">.140.241 </span><span class="token number">0</span><span class="token plain">.001s </span><span class="token string" style="color:rgb(255, 121, 198)">"127.0.0.1:80"</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">26</span><span class="token plain">/Oct/2018:03:09:32 +0800</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"GET /Apache/Seatunnel HTTP/1.1"</span><span class="token plain"> </span><span class="token number">200</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"-"</span><span class="token plain"> - </span><span class="token string" style="color:rgb(255, 121, 198)">"Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)"</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"196"</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"-"</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"mainpage"</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"443"</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"-"</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"172.16.181.129"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse-schema">ClickHouse Schema<a href="#clickhouse-schema" class="hash-link" aria-label="ClickHouse Schema的直接链接" title="ClickHouse Schema的直接链接">​</a></h3><p>我们的 ClickHouse 建表语句如下，我们的表按日进行分区</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE cms.cms_msg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> Date, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime DateTime, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time Float32, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">hostname</span><span class="token plain"> String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_size Int32, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    pool String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ENGINE </span><span class="token operator">=</span><span class="token plain"> MergeTree PARTITION BY </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> ORDER BY </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> SETTINGS index_granularity </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">16384</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-with-clickhouse">Seatunnel with ClickHouse<a href="#seatunnel-with-clickhouse" class="hash-link" aria-label="Seatunnel with ClickHouse的直接链接" title="Seatunnel with ClickHouse的直接链接">​</a></h2><p>接下来会给大家详细介绍，我们如何通过 Seatunnel 满足上述需求，将 HDFS 中的数据写入 ClickHouse 中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Seatunnel的直接链接" title="Seatunnel的直接链接">​</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> 是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在Spark之上。Seatunnel 拥有着非常丰富的插件，支持从 Kafka、HDFS、Kudu 中读取数据，进行各种各样的数据处理，并将结果写入 ClickHouse、Elasticsearch 或者 Kafka 中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Prerequisites的直接链接" title="Prerequisites的直接链接">​</a></h3><p>首先我们需要安装 Seatunnel，安装十分简单，无需配置系统环境变量</p><ol><li>准备 Spark 环境</li><li>安装 Seatunnel</li><li>配置 Seatunnel</li></ol><p>以下是简易步骤，具体安装可以参照 <a href="/zh-CN/docs/quick-start">Quick Start</a></p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">wget</span><span class="token plain"> https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">tar</span><span class="token plain"> -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">wget</span><span class="token plain"> https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">unzip</span><span class="token plain"> seatunnel-1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> seatunnel-1.1.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">vim</span><span class="token plain"> config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># 指定Spark安装路径</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">SPARK_HOME</span><span class="token operator">=</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">${SPARK_HOME</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">:-</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">usr</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">local</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">spark-2.2.0-bin-hadoop2.7}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="seatunnel Pipeline的直接链接" title="seatunnel Pipeline的直接链接">​</a></h3><p>我们仅需要编写一个 seatunnel Pipeline 的配置文件即可完成数据的导入。</p><p>配置文件包括四个部分，分别是 Spark、Input、filter 和 Output。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Spark的直接链接" title="Spark的直接链接">​</a></h4><p>这一部分是 Spark 的相关配置，主要配置 Spark 执行时所需的资源大小。</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"seatunnel"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"1g"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Input的直接链接" title="Input的直接链接">​</a></h4><p>这一部分定义数据源，如下是从 HDFS 文件中读取 text 格式数据的配置案例。</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hdfs </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        path </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"hdfs://nomanode:8020/rowlog/accesslog"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"access_log"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">format</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"text"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Filter的直接链接" title="Filter的直接链接">​</a></h4><p>在 Filter 部分，这里我们配置一系列的转化，包括正则解析将日志进行拆分、时间转换将 HTTPDATE 转化为 ClickHouse 支持的日期格式、对 Number 类型的字段进行类型转换以及通过 SQL 进行字段筛减等</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 使用正则解析原始日志</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"raw_message"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'%{IP:ha_ip}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\"%{DATA:upstream_ip}\"\\s\\[%{HTTPDATE:timestamp}\\]\\s\"%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\"\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\"%{DATA:user_agent}\"\\s%{DATA:uid}\\s%{DATA:session_id}\\s\"%{DATA:pool}\"\\s\"%{DATA:tag2}\"\\s%{DATA:tag3}\\s%{DATA:tag4}'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 将"dd/MMM/yyyy:HH:mm:ss Z"格式的数据转换为</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># "yyyy/MM/dd HH:mm:ss"格式的数据</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"timestamp"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"datetime"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"dd/MMM/yyyy:HH:mm:ss Z"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"yyyy/MM/dd HH:mm:ss"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 使用SQL筛选关注的字段，并对字段进行处理</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 甚至可以通过过滤条件过滤掉不关心的数据</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"access"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Output的直接链接" title="Output的直接链接">​</a></h4><p>最后我们将处理好的结构化数据写入 ClickHouse</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">host</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"your.clickhouse.host:8123"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"seatunnel"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"access_log"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"date"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"datetime"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"hostname"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"uri"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"http_code"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"request_time"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"data_size"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"domain"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"username"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"password"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-seatunnel">Running seatunnel<a href="#running-seatunnel" class="hash-link" aria-label="Running seatunnel的直接链接" title="Running seatunnel的直接链接">​</a></h3><p>我们将上述四部分配置组合成为我们的配置文件 <code>config/batch.conf</code>。</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">vim</span><span class="token plain"> config/batch.conf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"seatunnel"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"1g"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hdfs </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        path </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"hdfs://nomanode:8020/rowlog/accesslog"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"access_log"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">format</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"text"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 使用正则解析原始日志</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"raw_message"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">'%{IP:ha_ip}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\"%{DATA:upstream_ip}\"\\s\\[%{HTTPDATE:timestamp}\\]\\s\"%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\"\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\"%{DATA:user_agent}\"\\s%{DATA:uid}\\s%{DATA:session_id}\\s\"%{DATA:pool}\"\\s\"%{DATA:tag2}\"\\s%{DATA:tag3}\\s%{DATA:tag4}'</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 将"dd/MMM/yyyy:HH:mm:ss Z"格式的数据转换为</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># "yyyy/MM/dd HH:mm:ss"格式的数据</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"timestamp"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"datetime"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"dd/MMM/yyyy:HH:mm:ss Z"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"yyyy/MM/dd HH:mm:ss"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 使用SQL筛选关注的字段，并对字段进行处理</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 甚至可以通过过滤条件过滤掉不关心的数据</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"access"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">host</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"your.clickhouse.host:8123"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"seatunnel"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"access_log"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"date"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"datetime"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"hostname"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"uri"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"http_code"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"request_time"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"data_size"</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">"domain"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"username"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">"password"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>执行命令，指定配置文件，运行 Seatunnel，即可将数据写入 ClickHouse。这里我们以本地模式为例。</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m </span><span class="token string" style="color:rgb(255, 121, 198)">'local[2]'</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Conclusion的直接链接" title="Conclusion的直接链接">​</a></h2><p>在这篇文章中，我们介绍了如何使用 Seatunnel 将 HDFS 中的 Nginx 日志文件导入 ClickHouse 中。仅通过一个配置文件便可快速完成数据的导入，无需编写任何代码。除了支持 HDFS 数据源之外，Seatunnel 同样支持将数据从 Kafka 中实时读取处理写入 ClickHouse 中。我们的下一篇文章将会介绍，如何将 Hive 中的数据快速导入 ClickHouse 中。</p><p>当然，Seatunnel 不仅仅是 ClickHouse 数据写入的工具，在 Elasticsearch 以及 Kafka等 数据源的写入上同样可以扮演相当重要的角色。</p><p>希望了解 Seatunnel 和 ClickHouse、Elasticsearch、Kafka 结合使用的更多功能和案例，可以直接进入官网 <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p>]]></content:encoded>
            <category>HDFS</category>
            <category>ClickHouse</category>
        </item>
        <item>
            <title><![CDATA[如何快速地把 Hive 中的数据导入 ClickHouse]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/hive-to-clickhouse</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/hive-to-clickhouse</guid>
            <pubDate>Thu, 30 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[ClickHouse是面向OLAP的分布式列式DBMS。我们部门目前已经把所有数据分析相关的日志数据存储至ClickHouse这个优秀的数据仓库之中，当前日数据量达到了300亿。]]></description>
            <content:encoded><![CDATA[<p>ClickHouse是面向OLAP的分布式列式DBMS。我们部门目前已经把所有数据分析相关的日志数据存储至ClickHouse这个优秀的数据仓库之中，当前日数据量达到了300亿。</p><p>在之前的文章 <a href="/zh-CN/blog/i18n/zh-CN/docusaurus-plugin-content-blog/current/2021-12-30-hdfs-to-clickhouse.mdtent-blog/current/2021-12-30-hdfs-to-clickhouse.md">如何快速地把HDFS中的数据导入ClickHouse</a> 中我们提到过使用 Seatunnel <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> 对HDFS中的数据经过很简单的操作就可以将数据写入ClickHouse。HDFS中的数据一般是非结构化的数据，那么针对存储在Hive中的结构化数据，我们应该怎么操作呢？</p><p><img loading="lazy" src="/zh-CN/assets/images/hive-logo-c9aedd90b5ea9668c87fe25ad92a8e6c.png" width="962" height="518" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hive-to-clickhouse">Hive to ClickHouse<a href="#hive-to-clickhouse" class="hash-link" aria-label="Hive to ClickHouse的直接链接" title="Hive to ClickHouse的直接链接">​</a></h2><p>假定我们的数据已经存储在Hive中，我们需要读取Hive表中的数据并筛选出我们关心的字段，或者对字段进行转换，最后将对应的字段写入ClickHouse的表中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hive-schema">Hive Schema<a href="#hive-schema" class="hash-link" aria-label="Hive Schema的直接链接" title="Hive Schema的直接链接">​</a></h3><p>我们在Hive中存储的数据表结构如下，存储的是很常见的Nginx日志</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE `nginx_msg_detail`(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `hostname` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `domain` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `remote_addr` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `request_time` float,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `datetime` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `url` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `status` int,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `data_size` int,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `referer` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `cookie_info` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `user_agent` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `minute` string)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> PARTITIONED BY (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `date` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `hour` string)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse-schema">ClickHouse Schema<a href="#clickhouse-schema" class="hash-link" aria-label="ClickHouse Schema的直接链接" title="ClickHouse Schema的直接链接">​</a></h3><p>我们的ClickHouse建表语句如下，我们的表按日进行分区</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE cms.cms_msg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date Date,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime DateTime,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time Float32,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hostname String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_size Int32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">) ENGINE = MergeTree PARTITION BY date ORDER BY (date, hostname) SETTINGS index_granularity = 16384</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-with-clickhouse">Seatunnel with ClickHouse<a href="#seatunnel-with-clickhouse" class="hash-link" aria-label="Seatunnel with ClickHouse的直接链接" title="Seatunnel with ClickHouse的直接链接">​</a></h2><p>接下来会给大家介绍，我们如何通过 Seatunnel 将Hive中的数据写入ClickHouse中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Seatunnel的直接链接" title="Seatunnel的直接链接">​</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> 是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在Spark之上。Seatunnel 拥有着非常丰富的插件，支持从Kafka、HDFS、Kudu中读取数据，进行各种各样的数据处理，并将结果写入ClickHouse、Elasticsearch或者Kafka中。</p><p>Seatunnel的环境准备以及安装步骤这里就不一一赘述了，具体安装步骤可以参考上一篇文章或者访问 <a href="/zh-CN/docs/intro/about">Seatunnel Docs</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">Seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="Seatunnel Pipeline的直接链接" title="Seatunnel Pipeline的直接链接">​</a></h3><p>我们仅需要编写一个Seatunnel Pipeline的配置文件即可完成数据的导入。</p><p>配置文件包括四个部分，分别是Spark、Input、filter和Output。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Spark的直接链接" title="Spark的直接链接">​</a></h4><p>这一部分是Spark的相关配置，主要配置Spark执行时所需的资源大小。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  // 这个配置必需填写</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.catalogImplementation = "hive"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Input的直接链接" title="Input的直接链接">​</a></h4><p>这一部分定义数据源，如下是从Hive文件中读取text格式数据的配置案例。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hive {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = "select * from access.nginx_msg_detail"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>看，很简单的一个配置就可以从Hive中读取数据了。其中<code>pre_sql</code>是从Hive中读取数据SQL，<code>table_name</code>是将读取后的数据，注册成为Spark中临时表的表名，可为任意字段。</p><p>需要注意的是，必须保证hive的metastore是在服务状态。</p><p>在Cluster、Client、Local模式下运行时，必须把<code>hive-site.xml</code>文件置于提交任务节点的$HADOOP_CONF目录下</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Filter的直接链接" title="Filter的直接链接">​</a></h4><p>在Filter部分，这里我们配置一系列的转化，我们这里把不需要的minute和hour字段丢弃。当然我们也可以在读取Hive的时候通过<code>pre_sql</code>不读取这些字段</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remove {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = ["minute", "hour"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Output的直接链接" title="Output的直接链接">​</a></h4><p>最后我们将处理好的结构化数据写入ClickHouse</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        host = "your.clickhouse.host:8123"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = "nginx_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields = ["date", "datetime", "hostname", "url", "http_code", "request_time", "data_size", "domain"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username = "username"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = "password"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-seatunnel">Running Seatunnel<a href="#running-seatunnel" class="hash-link" aria-label="Running Seatunnel的直接链接" title="Running Seatunnel的直接链接">​</a></h3><p>我们将上述四部分配置组合成为我们的配置文件<code>config/batch.conf</code>。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/batch.conf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  // 这个配置必需填写</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.catalogImplementation = "hive"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hive {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = "select * from access.nginx_msg_detail"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remove {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = ["minute", "hour"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        host = "your.clickhouse.host:8123"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = "access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields = ["date", "datetime", "hostname", "uri", "http_code", "request_time", "data_size", "domain"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username = "username"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = "password"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>执行命令，指定配置文件，运行 Seatunnel，即可将数据写入ClickHouse。这里我们以本地模式为例。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m 'local[2]'</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Conclusion的直接链接" title="Conclusion的直接链接">​</a></h2><p>在这篇文章中，我们介绍了如何使用 Seatunnel 将Hive中的数据导入ClickHouse中。仅仅通过一个配置文件便可快速完成数据的导入，无需编写任何代码，十分简单。</p><p>希望了解 Seatunnel 与ClickHouse、Elasticsearch、Kafka、Hadoop结合使用的更多功能和案例，可以直接进入官网 <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p>]]></content:encoded>
            <category>Hive</category>
            <category>ClickHouse</category>
        </item>
        <item>
            <title><![CDATA[如何使用 Spark 快速将数据写入 Elasticsearch]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/spark-execute-elasticsearch</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/spark-execute-elasticsearch</guid>
            <pubDate>Thu, 30 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[说到数据写入 Elasticsearch，最先想到的肯定是Logstash。Logstash因为其简单上手、可扩展、可伸缩等优点被广大用户接受。但是尺有所短，寸有所长，Logstash肯定也有它无法适用的应用场景，比如：]]></description>
            <content:encoded><![CDATA[<p>说到数据写入 Elasticsearch，最先想到的肯定是Logstash。Logstash因为其简单上手、可扩展、可伸缩等优点被广大用户接受。但是尺有所短，寸有所长，Logstash肯定也有它无法适用的应用场景，比如：</p><ul><li>海量数据ETL</li><li>海量数据聚合</li><li>多源数据处理</li></ul><p>为了满足这些场景，很多同学都会选择Spark，借助Spark算子进行数据处理，最后将处理结果写入Elasticsearch。</p><p>我们部门之前利用Spark对Nginx日志进行分析，统计我们的Web服务访问情况，将Nginx日志每分钟聚合一次最后将结果写入Elasticsearch，然后利用Kibana配置实时监控Dashboard。Elasticsearch和Kibana都很方便、实用，但是随着类似需求越来越多，如何快速通过Spark将数据写入Elasticsearch成为了我们的一大问题。</p><p>今天给大家推荐一款能够实现数据快速写入的黑科技 Seatunnel <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> 一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在Spark之上，简单易用，灵活配置，无需开发。</p><p><img loading="lazy" src="/zh-CN/assets/images/wd-struct-fd963482dc80fdee6e4930107709bd28.png" width="818" height="466" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="kafka-to-elasticsearch">Kafka to Elasticsearch<a href="#kafka-to-elasticsearch" class="hash-link" aria-label="Kafka to Elasticsearch的直接链接" title="Kafka to Elasticsearch的直接链接">​</a></h2><p>和Logstash一样，Seatunnel同样支持多种类型的数据输入，这里我们以最常见的Kakfa作为输入源为例，讲解如何使用 Seatunnel 将数据快速写入Elasticsearch</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="log-sample">Log Sample<a href="#log-sample" class="hash-link" aria-label="Log Sample的直接链接" title="Log Sample的直接链接">​</a></h3><p>原始日志格式如下:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">127.0.0.1 elasticsearch.cn 114.250.140.241 0.001s "127.0.0.1:80" [26/Oct/2018:21:54:32 +0800] "GET /article HTTP/1.1" 200 123 "-" - "Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)"</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="elasticsearch-document">Elasticsearch Document<a href="#elasticsearch-document" class="hash-link" aria-label="Elasticsearch Document的直接链接" title="Elasticsearch Document的直接链接">​</a></h3><p>我们想要统计，一分钟每个域名的访问情况，聚合完的数据有以下字段:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">domain String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hostname String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">status int</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">datetime String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">count int</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-with-elasticsearch">Seatunnel with Elasticsearch<a href="#seatunnel-with-elasticsearch" class="hash-link" aria-label="Seatunnel with Elasticsearch的直接链接" title="Seatunnel with Elasticsearch的直接链接">​</a></h2><p>接下来会给大家详细介绍，我们如何通过 Seatunnel 读取Kafka中的数据，对数据进行解析以及聚合，最后将处理结果写入Elasticsearch中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Seatunnel的直接链接" title="Seatunnel的直接链接">​</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> 同样拥有着非常丰富的插件，支持从Kafka、HDFS、Hive中读取数据，进行各种各样的数据处理，并将结果写入Elasticsearch、Kudu或者Kafka中。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Prerequisites的直接链接" title="Prerequisites的直接链接">​</a></h3><p>首先我们需要安装seatunnel，安装十分简单，无需配置系统环境变量</p><ol><li>准备Spark环境</li><li>安装 Seatunnel</li><li>配置 Seatunnel</li></ol><p>以下是简易步骤，具体安装可以参照 <a href="/zh-CN/docs/quick-start">Quick Start</a></p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//archive.apache.org/dist/spark/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">xvf https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//archive.apache.org/dist/spark/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># 指定Spark安装路径</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=$</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">SPARK_HOME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">/usr/local/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">Seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="Seatunnel Pipeline的直接链接" title="Seatunnel Pipeline的直接链接">​</a></h3><p>与Logstash一样，我们仅需要编写一个Seatunnel Pipeline的配置文件即可完成数据的导入，相信了解Logstash的朋友可以很快入手 Seatunnel 配置。</p><p>配置文件包括四个部分，分别是Spark、Input、filter和Output。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Spark的直接链接" title="Spark的直接链接">​</a></h4><p>这一部分是Spark的相关配置，主要配置Spark执行时所需的资源大小。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.streaming.batchDuration = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Input的直接链接" title="Input的直接链接">​</a></h4><p>这一部分定义数据源，如下是从Kafka中读取数据的配置案例，</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    topics = "seatunnel-es"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.group.id = "seatunnel_es_group"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.rebalance.max.retries = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Filter的直接链接" title="Filter的直接链接">​</a></h4><p>在Filter部分，这里我们配置一系列的转化，包括正则解析将日志进行拆分、时间转换将HTTPDATE转化为Elasticsearch支持的日期格式、对Number类型的字段进行类型转换以及通过SQL进行数据聚合</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 使用正则解析原始日志</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 最开始数据都在raw_message字段中</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = "raw_message"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern = '%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">IP</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">remote_addr</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">request_time</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">s\\s\"%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">upstream_ip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\"\\s\\</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">HTTPDATE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">\\s\"%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">method</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">url</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">http_ver</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\"\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">body_bytes_send</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">referer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">cookie_info</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s\"%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">user_agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">'</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># 将"dd/MMM/yyyy:HH:mm:ss Z"格式的数据转换为</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Elasticsearch中支持的格式</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = "timestamp"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field = "datetime"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format = "dd/MMM/yyyy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">HH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">mm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">ss Z"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format = "yyyy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">MM</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">dd'T'HH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">mm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">ss.SSS+08</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">00"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)">## 利用SQL对数据进行聚合</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> int(status)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> datetime</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> count(</span><span class="token important">*)</span><span class="token plain"> from access_log group by domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> datetime"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Output的直接链接" title="Output的直接链接">​</a></h4><p>最后我们将处理好的结构化数据写入Elasticsearch。</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    elasticsearch </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        hosts = </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">"localhost:9200"</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index = "seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">$</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">now</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        es.batch.size.entries = 100000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index_time_format = "yyyy.MM.dd"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-seatunnel">Running Seatunnel<a href="#running-seatunnel" class="hash-link" aria-label="Running Seatunnel的直接链接" title="Running Seatunnel的直接链接">​</a></h3><p>我们将上述四部分配置组合成为我们的配置文件 <code>config/batch.conf</code>。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/batch.conf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.streaming.batchDuration = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = "seatunnel-es"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.group.id = "seatunnel_es_group"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.rebalance.max.retries = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # 使用正则解析原始日志</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # 最开始数据都在raw_message字段中</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = "raw_message"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern = '%{IP:hostname}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\"%{DATA:upstream_ip}\"\\s\\[%{HTTPDATE:timestamp}\\]\\s\"%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\"\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\"%{DATA:user_agent}'</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # 将"dd/MMM/yyyy:HH:mm:ss Z"格式的数据转换为</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Elasticsearch中支持的格式</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = "timestamp"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field = "datetime"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format = "dd/MMM/yyyy:HH:mm:ss Z"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format = "yyyy-MM-dd'T'HH:mm:00.SSS+08:00"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ## 利用SQL对数据进行聚合</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select domain, hostname, status, datetime, count(*) from access_log group by domain, hostname, status, datetime"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    elasticsearch {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        hosts = ["localhost:9200"]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index = "seatunnel-${now}"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        es.batch.size.entries = 100000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index_time_format = "yyyy.MM.dd"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>执行命令，指定配置文件，运行 Seatunnel，即可将数据写入Elasticsearch。这里我们以本地模式为例。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m 'local[2]'</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>最后，写入Elasticsearch中的数据如下，再配上Kibana就可以实现Web服务的实时监控了^_^.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">"_source": {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    "domain": "elasticsearch.cn",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    "hostname": "localhost",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    "status": "200",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    "datetime": "2018-11-26T21:54:00.000+08:00",</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    "count": 26</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Conclusion的直接链接" title="Conclusion的直接链接">​</a></h2><p>在这篇文章中，我们介绍了如何通过 Seatunnel 将Kafka中的数据写入Elasticsearch中。仅仅通过一个配置文件便可快速运行一个Spark Application，完成数据的处理、写入，无需编写任何代码，十分简单。</p><p>当数据处理过程中有遇到Logstash无法支持的场景或者Logstah性能无法达到预期的情况下，都可以尝试使用 Seatunnel 解决问题。</p><p>希望了解 Seatunnel 与Elasticsearch、Kafka、Hadoop结合使用的更多功能和案例，可以直接进入官网 <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p><strong>我们近期会再发布一篇《如何用Spark和Elasticsearch做交互式数据分析》，敬请期待.</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contract-us">Contract us<a href="#contract-us" class="hash-link" aria-label="Contract us的直接链接" title="Contract us的直接链接">​</a></h2><ul><li>邮件列表 : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. 发送任意内容至 <code>dev-subscribe@seatunnel.apache.org</code>， 按照回复订阅邮件列表。</li><li>Slack: 发送 <code>Request to join SeaTunnel slack</code> 邮件到邮件列表 (<code>dev@seatunnel.apache.org</code>), 我们会邀请你加入（在此之前请确认已经注册Slack）.</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B站 视频</a></li></ul>]]></content:encoded>
            <category>Spark</category>
            <category>Kafka</category>
            <category>Elasticsearch</category>
        </item>
        <item>
            <title><![CDATA[怎么用 Spark 在 TiDB 上做 OLAP 分析]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/spark-execute-tidb</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/spark-execute-tidb</guid>
            <pubDate>Thu, 30 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[TiDB 是一款定位于在线事务处理/在线分析处理的融合型数据库产品，实现了一键水平伸缩，强一致性的多副本数据安全，分布式事务，实时 OLAP 等重要特性。]]></description>
            <content:encoded><![CDATA[<p><img loading="lazy" src="https://download.pingcap.com/images/tidb-planet.jpg" class="img_ev3q"></p><p><a href="https://github.com/pingcap/tidb" target="_blank" rel="noopener noreferrer">TiDB</a> 是一款定位于在线事务处理/在线分析处理的融合型数据库产品，实现了一键水平伸缩，强一致性的多副本数据安全，分布式事务，实时 OLAP 等重要特性。</p><p>TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。它借助 Spark 平台，同时融合 TiKV 分布式集群的优势。</p><p>直接使用 TiSpark 完成 OLAP 操作需要了解 Spark，还需要一些开发工作。那么，有没有一些开箱即用的工具能帮我们更快速地使用 TiSpark 在 TiDB 上完成 OLAP 分析呢？</p><p>目前开源社区上有一款工具 <strong>Seatunnel</strong>，项目地址 <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> ，可以基于Spark，在 TiSpark 的基础上快速实现 TiDB 数据读取和 OLAP 分析。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="使用-seatunnel-操作tidb">使用 Seatunnel 操作TiDB<a href="#使用-seatunnel-操作tidb" class="hash-link" aria-label="使用 Seatunnel 操作TiDB的直接链接" title="使用 Seatunnel 操作TiDB的直接链接">​</a></h2><p>在我们线上有这么一个需求，从 TiDB 中读取某一天的网站访问数据，统计每个域名以及服务返回状态码的访问次数，最后将统计结果写入 TiDB 另外一个表中。 我们来看看 Seatunnel 是如何实现这么一个功能的。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Seatunnel的直接链接" title="Seatunnel的直接链接">​</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> 是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在 Spark 之上。Seatunnel 拥有着非常丰富的插件，支持从 TiDB、Kafka、HDFS、Kudu 中读取数据，进行各种各样的数据处理，然后将结果写入 TiDB、ClickHouse、Elasticsearch 或者 Kafka 中。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="准备工作">准备工作<a href="#准备工作" class="hash-link" aria-label="准备工作的直接链接" title="准备工作的直接链接">​</a></h4><h5 class="anchor anchorWithStickyNavbar_LWe7" id="1-tidb-表结构介绍">1. TiDB 表结构介绍<a href="#1-tidb-表结构介绍" class="hash-link" aria-label="1. TiDB 表结构介绍的直接链接" title="1. TiDB 表结构介绍的直接链接">​</a></h5><p><strong>Input</strong>（存储访问日志的表）</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE access_log (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain VARCHAR(255),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime VARCHAR(63),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr VARCHAR(63),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    http_ver VARCHAR(15),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    body_bytes_send INT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status INT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time FLOAT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url TEXT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">+-----------------+--------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| Field           | Type         | Null | Key  | Default | Extra |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+-----------------+--------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| domain          | varchar(255) | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| datetime        | varchar(63)  | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| remote_addr     | varchar(63)  | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| http_ver        | varchar(15)  | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| body_bytes_send | int(11)      | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| status          | int(11)      | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| request_time    | float        | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| url             | text         | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+-----------------+--------------+------+------+---------+-------+</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>Output</strong>（存储结果数据的表）</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE access_collect (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date VARCHAR(23),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain VARCHAR(63),</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status INT,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hit INT</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">+--------+-------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| Field  | Type        | Null | Key  | Default | Extra |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+--------+-------------+------+------+---------+-------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| date   | varchar(23) | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| domain | varchar(63) | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| status | int(11)     | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| hit    | int(11)     | YES  |      | NULL    |       |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+--------+-------------+------+------+---------+-------+</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h5 class="anchor anchorWithStickyNavbar_LWe7" id="2-安装-seatunnel">2. 安装 Seatunnel<a href="#2-安装-seatunnel" class="hash-link" aria-label="2. 安装 Seatunnel的直接链接" title="2. 安装 Seatunnel的直接链接">​</a></h5><p>有了 TiDB 输入和输出表之后， 我们需要安装 Seatunnel，安装十分简单，无需配置系统环境变量</p><ol><li>准备 Spark环境</li><li>安装 Seatunnel</li><li>配置 Seatunnel</li></ol><p>以下是简易步骤，具体安装可以参照 <a href="/zh-CN/docs/quick-start">Quick Start</a></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 下载安装Spark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar -xvf https://archive.apache.org/dist/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 下载安装seatunnel</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">https://github.com/InterestingLab/seatunnel/releases/download/v1.2.0/seatunnel-1.2.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel-1.2.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel-1.2.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 指定Spark安装路径</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.1.0-bin-hadoop2.7}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="实现-seatunnel-处理流程">实现 Seatunnel 处理流程<a href="#实现-seatunnel-处理流程" class="hash-link" aria-label="实现 Seatunnel 处理流程的直接链接" title="实现 Seatunnel 处理流程的直接链接">​</a></h3><p>我们仅需要编写一个 Seatunnel 配置文件即可完成数据的读取、处理、写入。</p><p>Seatunnel 配置文件由四个部分组成，分别是 <code>Spark</code>、<code>Input</code>、<code>Filter</code> 和 <code>Output</code>。<code>Input</code> 部分用于指定数据的输入源，<code>Filter</code> 部分用于定义各种各样的数据处理、聚合，<code>Output</code> 部分负责将处理之后的数据写入指定的数据库或者消息队列。</p><p>整个处理流程为 <code>Input</code> -&gt; <code>Filter</code> -&gt; <code>Output</code>，整个流程组成了 Seatunnel 的 处理流程（Pipeline）。</p><blockquote><p>以下是一个具体配置，此配置来源于线上实际应用，但是为了演示有所简化。</p></blockquote><h5 class="anchor anchorWithStickyNavbar_LWe7" id="input-tidb">Input (TiDB)<a href="#input-tidb" class="hash-link" aria-label="Input (TiDB)的直接链接" title="Input (TiDB)的直接链接">​</a></h5><p>这里部分配置定义输入源，如下是从 TiDB 一张表中读取数据。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = "nginx"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = "select * from nginx.access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "spark_nginx_input"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h5 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Filter的直接链接" title="Filter的直接链接">​</a></h5><p>在Filter部分，这里我们配置一系列的转化, 大部分数据分析的需求，都是在Filter完成的。Seatunnel 提供了丰富的插件，足以满足各种数据分析需求。这里我们通过 SQL 插件完成数据的聚合操作。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "spark_nginx_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)='2019-01-20' group by domain, status, substring(datetime, 1, 10)"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h5 class="anchor anchorWithStickyNavbar_LWe7" id="output-tidb">Output (TiDB)<a href="#output-tidb" class="hash-link" aria-label="Output (TiDB)的直接链接" title="Output (TiDB)的直接链接">​</a></h5><p>最后， 我们将处理后的结果写入TiDB另外一张表中。TiDB Output是通过JDBC实现的</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        url = "jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;characterEncoding=utf8"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = "access_collect"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        user = "username"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = "password"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        save_mode = "append"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h5 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Spark的直接链接" title="Spark的直接链接">​</a></h5><p>这一部分是 Spark 的相关配置，主要配置 Spark 执行时所需的资源大小以及其他 Spark 配置。</p><p>我们的 TiDB Input 插件是基于 TiSpark 实现的，而 TiSpark 依赖于 TiKV 集群和 Placement Driver (PD)。因此我们需要指定 PD 节点信息以及 TiSpark 相关配置<code>spark.tispark.pd.addresses</code>和<code>spark.sql.extensions</code>。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel-tidb"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  # Set for TiSpark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.tispark.pd.addresses = "localhost:2379"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.extensions = "org.apache.spark.sql.TiExtensions"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="运行-seatunnel">运行 Seatunnel<a href="#运行-seatunnel" class="hash-link" aria-label="运行 Seatunnel的直接链接" title="运行 Seatunnel的直接链接">​</a></h4><p>我们将上述四部分配置组合成我们最终的配置文件 <code>conf/tidb.conf</code></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.app.name = "seatunnel-tidb"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Set for TiSpark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.tispark.pd.addresses = "localhost:2379"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    spark.sql.extensions = "org.apache.spark.sql.TiExtensions"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = "nginx"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = "select * from nginx.access_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "spark_table"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "spark_nginx_log"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select count(*) as hit, domain, status, substring(datetime, 1, 10) as date from spark_nginx_log where substring(datetime, 1, 10)='2019-01-20' group by domain, status, substring(datetime, 1, 10)"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    tidb {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        url = "jdbc:mysql://127.0.0.1:4000/nginx?useUnicode=true&amp;characterEncoding=utf8"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = "access_collect"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        user = "username"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = "password"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        save_mode = "append"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>执行命令，指定配置文件，运行 Seatunnel ，即可实现我们的数据处理逻辑。</p><ul><li>Local</li></ul><blockquote><p>./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master 'local<!-- -->[2]<!-- -->'</p></blockquote><ul><li>yarn-client</li></ul><blockquote><p>./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode client --master yarn</p></blockquote><ul><li>yarn-cluster</li></ul><blockquote><p>./bin/start-seatunnel.sh --config config/tidb.conf --deploy-mode cluster -master yarn</p></blockquote><p>如果是本机测试验证逻辑，用本地模式（Local）就可以了，一般生产环境下，都是使用<code>yarn-client</code>或者<code>yarn-cluster</code>模式。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="检查结果">检查结果<a href="#检查结果" class="hash-link" aria-label="检查结果的直接链接" title="检查结果的直接链接">​</a></h4><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">mysql&gt; select * from access_collect;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+------------+--------+--------+------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| date       | domain | status | hit  |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+------------+--------+--------+------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| 2019-01-20 | b.com  |    200 |   63 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">| 2019-01-20 | a.com  |    200 |   85 |</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">+------------+--------+--------+------+</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">2 rows in set (0.21 sec)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="总结">总结<a href="#总结" class="hash-link" aria-label="总结的直接链接" title="总结的直接链接">​</a></h2><p>在这篇文章中，我们介绍了如何使用 Seatunnel 从 TiDB 中读取数据，做简单的数据处理之后写入 TiDB 另外一个表中。仅通过一个配置文件便可快速完成数据的导入，无需编写任何代码。</p><p>除了支持 TiDB 数据源之外，Seatunnel 同样支持Elasticsearch, Kafka, Kudu, ClickHouse等数据源。</p><p><strong>于此同时，我们正在研发一个重要功能，就是在 Seatunnel 中，利用 TiDB 的事务特性，实现从 Kafka 到 TiDB 流式数据处理，并且支持端（Kafka）到端（TiDB）的 Exactly-Once 数据一致性。</strong></p><p>希望了解 Seatunnel 和 TiDB，ClickHouse、Elasticsearch、Kafka结合使用的更多功能和案例，可以直接进入官网 <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="联系我们">联系我们<a href="#联系我们" class="hash-link" aria-label="联系我们的直接链接" title="联系我们的直接链接">​</a></h2><ul><li>邮件列表 : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. 发送任意内容至 <code>dev-subscribe@seatunnel.apache.org</code>， 按照回复订阅邮件列表。</li><li>Slack: 发送 <code>Request to join SeaTunnel slack</code> 邮件到邮件列表 (<code>dev@seatunnel.apache.org</code>), 我们会邀请你加入（在此之前请确认已经注册Slack）.</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B站 视频</a></li></ul><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p>]]></content:encoded>
            <category>Spark</category>
            <category>TiDB</category>
        </item>
        <item>
            <title><![CDATA[如何支持的 Spark StructuredStreaming]]></title>
            <link>https://seatunnel.apache.org/zh-CN/blog/spark-structured-streaming</link>
            <guid>https://seatunnel.apache.org/zh-CN/blog/spark-structured-streaming</guid>
            <pubDate>Thu, 30 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[前言]]></description>
            <content:encoded><![CDATA[<h3 class="anchor anchorWithStickyNavbar_LWe7" id="前言">前言<a href="#前言" class="hash-link" aria-label="前言的直接链接" title="前言的直接链接">​</a></h3><p>StructuredStreaming是Spark 2.0以后新开放的一个模块，相比SparkStreaming，它有一些比较突出的优点：<br> <!-- --> <!-- --> <!-- -->一、它能做到更低的延迟;<br>
<!-- --> <!-- --> <!-- -->二、可以做实时的聚合，例如实时计算每天每个商品的销售总额；<br>
<!-- --> <!-- --> <!-- -->三、可以做流与流之间的关联，例如计算广告的点击率，需要将广告的曝光记录和点击记录关联。<br>
以上几点如果使用SparkStreaming来实现可能会比较麻烦或者说是很难实现，但是使用StructuredStreaming实现起来会比较轻松。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="如何使用structuredstreaming">如何使用StructuredStreaming<a href="#如何使用structuredstreaming" class="hash-link" aria-label="如何使用StructuredStreaming的直接链接" title="如何使用StructuredStreaming的直接链接">​</a></h3><p>可能你没有详细研究过StructuredStreaming，但是发现StructuredStreaming能很好的解决你的需求，如何快速利用StructuredStreaming来解决你的需求？目前社区有一款工具 <strong>Seatunnel</strong>，项目地址：<a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> ,
可以高效低成本的帮助你利用StructuredStreaming来完成你的需求。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Seatunnel的直接链接" title="Seatunnel的直接链接">​</a></h3><p>Seatunnel 是一个非常易用，高性能，能够应对海量数据的实时数据处理产品，它构建在Spark之上。Seatunnel 拥有着非常丰富的插件，支持从Kafka、HDFS、Kudu中读取数据，进行各种各样的数据处理，并将结果写入ClickHouse、Elasticsearch或者Kafka中</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="准备工作">准备工作<a href="#准备工作" class="hash-link" aria-label="准备工作的直接链接" title="准备工作的直接链接">​</a></h3><p>首先我们需要安装 Seatunnel，安装十分简单，无需配置系统环境变量</p><ol><li>准备Spark环境</li><li>安装 Seatunnel</li><li>配置 Seatunnel</li></ol><p>以下是简易步骤，具体安装可以参照 <a href="/zh-CN/docs/quick-start">Quick Start</a></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https://github.com/InterestingLab/seatunnel/releases/download/v1.3.0/seatunnel-1.3.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel-1.3.0.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel-1.3.0</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"># 指定Spark安装路径</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=${SPARK_HOME:-/usr/local/spark-2.2.0-bin-hadoop2.7}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">Seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="Seatunnel Pipeline的直接链接" title="Seatunnel Pipeline的直接链接">​</a></h3><p>我们仅需要编写一个 Seatunnel Pipeline的配置文件即可完成数据的导入。</p><p>配置文件包括四个部分，分别是Spark、Input、filter和Output。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Spark的直接链接" title="Spark的直接链接">​</a></h4><p>这一部分是Spark的相关配置，主要配置Spark执行时所需的资源大小。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Input的直接链接" title="Input的直接链接">​</a></h4><p>下面是一个从kafka读取数据的例子</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    topics = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    schema = "{\"name\":\"string\",\"age\":\"integer\",\"addrs\":{\"country\":\"string\",\"city\":\"string\"}}"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>通过上面的配置就可以读取kafka里的数据了 ，topics是要订阅的kafka的topic，同时订阅多个topic可以以逗号隔开，consumer.bootstrap.servers就是Kafka的服务器列表，schema是可选项，因为StructuredStreaming从kafka读取到的值(官方固定字段value)是binary类型的，详见<a href="http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" target="_blank" rel="noopener noreferrer">http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html</a>
但是如果你确定你kafka里的数据是json字符串的话，你可以指定schema，input插件将按照你指定的schema解析</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Filter的直接链接" title="Filter的直接链接">​</a></h4><p>下面是一个简单的filter例子</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "student"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select name,age from student"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><code>table_name</code>是注册成的临时表名，以便于在下面的sql使用</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Output的直接链接" title="Output的直接链接">​</a></h4><p>处理好的数据往外输出，假设我们的输出也是kafka</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafka {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topic = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        producer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        streaming_output_mode = "update"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        checkpointLocation = "/your/path"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><code>topic</code> 是你要输出的topic，<code> producer.bootstrap.servers</code>是kafka集群列表，<code>streaming_output_mode</code>是StructuredStreaming的一个输出模式参数，有三种类型<code>append|update|complete</code>，具体使用参见文档<a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes" target="_blank" rel="noopener noreferrer">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes</a></p><p><code>checkpointLocation</code>是StructuredStreaming的checkpoint路径，如果配置了的话，这个目录会存储程序的运行信息，比如程序退出再启动的话会接着上次的offset进行消费。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="场景分析">场景分析<a href="#场景分析" class="hash-link" aria-label="场景分析的直接链接" title="场景分析的直接链接">​</a></h3><p>以上就是一个简单的例子，接下来我们就来介绍的稍微复杂一些的业务场景</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="场景一实时聚合场景">场景一：实时聚合场景<a href="#场景一实时聚合场景" class="hash-link" aria-label="场景一：实时聚合场景的直接链接" title="场景一：实时聚合场景的直接链接">​</a></h4><p>假设现在有一个商城，上面有10种商品，现在需要实时求每天每种商品的销售额，甚至是求每种商品的购买人数（不要求十分精确）。
这么做的巨大的优势就是海量数据可以在实时处理的时候，完成聚合，再也不需要先将数据写入数据仓库，再跑离线的定时任务进行聚合，
操作起来还是很方便的。</p><p>kafka的数据如下</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{"good_id":"abc","price":300,"user_id":123456,"time":1553216320}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>那我们该怎么利用 Seatunnel 来完成这个需求呢，当然还是只需要配置就好了。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#spark里的配置根据业务需求配置</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#配置input</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = "good_topic"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        schema = "{\"good_id\":\"string\",\"price\":\"integer\",\"user_id\":\"Long\",\"time\":\"Long\"}"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#配置filter    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #在程序做聚合的时候，内部会去存储程序从启动开始的聚合状态，久而久之会导致OOM,如果设置了watermark，程序自动的会去清理watermark之外的状态</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #这里表示使用ts字段设置watermark，界限为1天</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Watermark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        time_field = "time"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        time_type = "UNIX"              #UNIX表示时间字段为10为的时间戳，还有其他的类型详细可以查看插件文档</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        time_pattern = "yyyy-MM-dd"     #这里之所以要把ts对其到天是因为求每天的销售额，如果是求每小时的销售额可以对其到小时`yyyy-MM-dd HH`</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        delay_threshold = "1 day"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        watermark_field = "ts"          #设置watermark之后会新增一个字段，`ts`就是这个字段的名字</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #之所以要group by ts是要让watermark生效，approx_count_distinct是一个估值，并不是精确的count_distinct</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "good_table_2"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select good_id,sum(price) total, approx_count_distinct(user_id) person from good_table_2 group by ts,good_id"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#接下来我们选择将结果实时输出到Kafka</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output{</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafka {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topic = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        producer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        streaming_output_mode = "update"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        checkpointLocation = "/your/path"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>如上配置完成，启动 Seatunnel，就可以获取你想要的结果了。</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="场景二多个流关联场景">场景二：多个流关联场景<a href="#场景二多个流关联场景" class="hash-link" aria-label="场景二：多个流关联场景的直接链接" title="场景二：多个流关联场景的直接链接">​</a></h4><p>假设你在某个平台投放了广告，现在要实时计算出每个广告的CTR(点击率)，数据分别来自两个topic，一个是广告曝光日志，一个是广告点击日志,
此时我们就需要把两个流数据关联到一起做计算，而 Seatunnel 最近也支持了此功能，让我们一起看一下该怎么做：</p><p>点击topic数据格式</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{"ad_id":"abc","click_time":1553216320,"user_id":12345}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>曝光topic数据格式</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">{"ad_id":"abc","show_time":1553216220,"user_id":12345}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">#spark里的配置根据业务需求配置</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = "1g"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#配置input</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = "click_topic"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        schema = "{\"ad_id\":\"string\",\"user_id\":\"Long\",\"click_time\":\"Long\"}"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "click_table"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = "show_topic"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        schema = "{\"ad_id\":\"string\",\"user_id\":\"Long\",\"show_time\":\"Long\"}"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "show_table"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #左关联右表必须设置watermark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #右关左右表必须设置watermark</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    #http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#inner-joins-with-optional-watermarking</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Watermark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              source_table_name = "click_table" #这里可以指定为某个临时表添加watermark，不指定的话就是为input中的第一个</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              time_field = "time"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              time_type = "UNIX"               </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              delay_threshold = "3 hours"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              watermark_field = "ts" </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">              result_table_name = "click_table_watermark" #添加完watermark之后可以注册成临时表，方便后续在sql中使用</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    Watermark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                source_table_name = "show_table" </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                time_field = "time"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                time_type = "UNIX"               </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                delay_threshold = "2 hours"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                watermark_field = "ts" </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">                result_table_name = "show_table_watermark" </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">     }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = "show_table_watermark"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = "select a.ad_id,count(b.user_id)/count(a.user_id) ctr from show_table_watermark as a left join click_table_watermark as b on a.ad_id = b.ad_id and a.user_id = b.user_id "</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">#接下来我们选择将结果实时输出到Kafka</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafka {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topic = "seatunnel"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        producer.bootstrap.servers = "localhost:9092"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        streaming_output_mode = "append" #流关联只支持append模式</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        checkpointLocation = "/your/path"</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>通过配置，到这里流关联的案例也完成了。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="结语">结语<a href="#结语" class="hash-link" aria-label="结语的直接链接" title="结语的直接链接">​</a></h3><p>通过配置能很快的利用StructuredStreaming做实时数据处理，但是还是需要对StructuredStreaming的一些概念了解，比如其中的watermark机制，还有程序的输出模式。</p><p>最后，Seatunnel 当然还支持spark streaming和spark 批处理。
如果你对这两个也感兴趣的话，可以阅读我们以前发布的文章《<a href="/zh-CN/blog/i18n/zh-CN/docusaurus-plugin-content-blog/current/2021-12-30-hive-to-clickhouse.mdtent-blog/current/2021-12-30-hive-to-clickhouse.md">如何快速地将Hive中的数据导入ClickHouse</a>》、
《<a href="/zh-CN/blog/i18n/zh-CN/docusaurus-plugin-content-blog/current/2021-12-30-spark-execute-tidb.mdtent-blog/current/2021-12-30-spark-execute-tidb.md">优秀的数据工程师，怎么用Spark在TiDB上做OLAP分析</a>》、
《<a href="/zh-CN/blog/i18n/zh-CN/docusaurus-plugin-content-blog/2021-12-30-spark-execute-elasticsearch.md/current/2021-12-30-spark-execute-elasticsearch.md">如何使用Spark快速将数据写入Elasticsearch</a>》</p><p>希望了解 Seatunnel 和 HBase, ClickHouse、Elasticsearch、Kafka、MySQL 等数据源结合使用的更多功能和案例，可以直接进入官网 <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="联系我们">联系我们<a href="#联系我们" class="hash-link" aria-label="联系我们的直接链接" title="联系我们的直接链接">​</a></h2><ul><li>邮件列表 : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. 发送任意内容至 <code>dev-subscribe@seatunnel.apache.org</code>， 按照回复订阅邮件列表。</li><li>Slack: 发送 <code>Request to join SeaTunnel slack</code> 邮件到邮件列表 (<code>dev@seatunnel.apache.org</code>), 我们会邀请你加入（在此之前请确认已经注册Slack）.</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B站 视频</a></li></ul>]]></content:encoded>
            <category>Spark</category>
            <category>StructuredStreaming</category>
        </item>
    </channel>
</rss>