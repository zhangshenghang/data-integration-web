<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">Blog | Apache SeaTunnel</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://seatunnel.apache.org/blog/page/2"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Apache SeaTunnel"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/image/favicon.ico"><link data-rh="true" rel="canonical" href="https://seatunnel.apache.org/blog/page/2"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/blog/page/2" hreflang="en"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/zh-CN/blog/page/2" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/blog/page/2" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S2J1A7LWND-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache SeaTunnel RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache SeaTunnel Atom Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Apache SeaTunnel" href="/opensearch.xml">



<link rel="alternate" type="application/rss+xml" href="/user_cases/rss.xml" title="Apache SeaTunnel RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/user_cases/atom.xml" title="Apache SeaTunnel Atom Feed"><link rel="stylesheet" href="/assets/css/styles.72e1e0e2.css">
<link rel="preload" href="/assets/js/runtime~main.4be0285b.js" as="script">
<link rel="preload" href="/assets/js/main.33ff0e4e.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:rgb(70, 125, 175, 0.8)" role="banner"><div class="content_knG7 announcementBarContent_xLdY">ü§î Have queries regarding Apache SeaTunnel, Join Slack channel to discuss them join <a target="_blank" rel="noopener noreferrer" href="https://s.apache.org/seatunnel-slack">#SeaTunnel</a> channel! üåü</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/image/logo.png" alt="Apache SeaTunnel Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/image/logo.png" alt="Apache SeaTunnel Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Apache SeaTunnel</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Document</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/2.3.7/about">2.3.7</a></li><li><a class="dropdown__link" href="/docs/2.3.6/about">2.3.6</a></li><li><a class="dropdown__link" href="/docs/2.3.5/about">2.3.5</a></li><li><a class="dropdown__link" href="/docs/2.3.4/about">2.3.4</a></li><li><a class="dropdown__link" href="/docs/2.3.3/about">2.3.3</a></li><li><a class="dropdown__link" href="/docs/about">Next</a></li><li><a class="dropdown__link" href="/versions/">All versions</a></li></ul></div><a class="navbar__item navbar__link" href="/download">Download</a><a class="navbar__item navbar__link" href="/community/contribution_guide/contribute">Community</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/user_cases">UserCases</a><a class="navbar__item navbar__link" href="/team">Team</a><a class="navbar__item navbar__link" href="/user">Users</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">ASF</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Foundation</a></li><li><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="dropdown__link">License</a></li><li><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="dropdown__link">Events</a></li><li><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sponsorship</a></li><li><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Thanks</a></li><li><a href="https://apache.org/foundation/policies/privacy.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Privacy</a></li></ul></div><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a class="navbar__item navbar__link" href="/security">Security</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/blog/page/2" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-CN/blog/page/2" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get">SeaTunnel 2.3.1 is released! The refactored AI Compatible feature allows ChatGPT to automatically generate Connector code</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md">Performance Test Report: SeaTunnel Synchronizes data in batches 420% Faster than GLUE!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md">SeaTunnel now supports CDC (Capture Change Data) writing by ClickHouse Connector!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!">In the recently released SeaTunnel 2.3.0 official version</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection">SeaTunnel supports IoTDB to implement IoT data synchronization</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="More than a month after the release of Apache SeaTunnel(Incubating) 2.1.2, we have been collecting user and developer feedback to bring you version 2.1.3. The new version introduces the Assert Sink connector, which is an inurgent need in the community, and two Transforms, NullRate and Nulltf. Some usability problems in the previous version have also been fixed, improving stability and efficiency."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2022/09/12/SeaTunnel-2.1.3-released">SeaTunnel 2.1.3 released! Introducing in Assert Sink connector and NullRate, Nulltf Transform</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-09-12T00:00:00.000Z" itemprop="datePublished">September 12, 2022</time> ¬∑ <!-- -->3 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="https://miro.medium.com/max/1400/1*7jtTFNpvwC6nquA-BLfqGg.png" class="img_ev3q"></p><p>More than a month after the release of Apache SeaTunnel(Incubating) 2.1.2, we have been collecting user and developer feedback to bring you version 2.1.3. The new version introduces the Assert Sink connector, which is an inurgent need in the community, and two Transforms, NullRate and Nulltf. Some usability problems in the previous version have also been fixed, improving stability and efficiency.</p><p>This article will introduce the details of the update of Apache SeaTunnel(Incubating) <strong>version 2.1.3</strong>.</p><ul><li>Release Note: <a href="https://github.com/apache/incubator-seatunnel/blob/2.1.3/release-note.md" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/blob/2.1.3/release-note.md</a></li><li>Download address: <a href="https://seatunnel.apache.org/download" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/download</a></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="major-feature-updates">Major feature updates<a href="#major-feature-updates" class="hash-link" aria-label="Direct link to Major feature updates" title="Direct link to Major feature updates">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="introduces-assert-sink-connector">Introduces Assert Sink connector<a href="#introduces-assert-sink-connector" class="hash-link" aria-label="Direct link to Introduces Assert Sink connector" title="Direct link to Introduces Assert Sink connector">‚Äã</a></h3><p>Assert Sink connector is introduced in SeaTunnel version 2.1.3to verify data correctness. Special thanks to Lhyundeadsoul for his contribution.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="add-two-transforms">Add two Transforms<a href="#add-two-transforms" class="hash-link" aria-label="Direct link to Add two Transforms" title="Direct link to Add two Transforms">‚Äã</a></h3><p>In addition, the 2.1.3 version also adds two Transforms, NullRate and Nulltf, which are used to detect data quality and convert null values ‚Äã‚Äãin the data to generate default values. These two Transforms can effectively improve the availability of data and reduce the frequency of abnormal situations. Special thanks to wsyhj and Interest1-wyt for their contributions.</p><p>At present, SeaTunnel has supported 9 types of Transforms including Common Options, Json, NullRate, Nulltf, Replace, Split, SQL, UDF, and UUID, and the community is welcome to contribute more Transform types.</p><p>For details of Transform, please refer to the official documentation: <a href="https://seatunnel.apache.org/docs/2.1.3/category/transform" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.1.3/category/transform</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="clickhousefile-connector-supports-rsync-data-transfer-method-now">ClickhouseFile connector supports Rsync data transfer method now<a href="#clickhousefile-connector-supports-rsync-data-transfer-method-now" class="hash-link" aria-label="Direct link to ClickhouseFile connector supports Rsync data transfer method now" title="Direct link to ClickhouseFile connector supports Rsync data transfer method now">‚Äã</a></h3><p>At the same time, SeaTunnel 2.1.3 version brings Rsync data transfer mode support to ClickhouseFile connector, users can now choose SCP and Rsync data transfer modes. Thanks to Emor-nj for contributing to this feature.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="specific-feature-updates">Specific feature updates:<a href="#specific-feature-updates" class="hash-link" aria-label="Direct link to Specific feature updates:" title="Direct link to Specific feature updates:">‚Äã</a></h3><ul><li>Flink Fake data supports BigInteger type <a href="https://github.com/apache/incubator-seatunnel/pull/2118" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2118</a></li><li>Add Flink Assert Sink connector <a href="https://github.com/apache/incubator-seatunnel/pull/2022" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2022</a></li><li>Spark ClickhouseFile connector supports Rsync data file transfer method <a href="https://github.com/apache/incubator-seatunnel/pull/2074" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2074</a></li><li>Add Flink Assert Sink e2e module <a href="https://github.com/apache/incubator-seatunnel/pull/2036" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2036</a></li><li>Add NullRate Transform for detecting data quality <a href="https://github.com/apache/incubator-seatunnel/pull/1978" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/1978</a></li><li>Add Nulltf Transform for setting defaults <a href="https://github.com/apache/incubator-seatunnel/pull/1958" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/1958</a></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization">Optimization<a href="#optimization" class="hash-link" aria-label="Direct link to Optimization" title="Direct link to Optimization">‚Äã</a></h3><ul><li>Refactored Spark TiDB-related parameter information</li><li>Refactor the code to remove redundant code warning information</li><li>Optimize connector jar package loading logic</li><li>Add Plugin Discovery module</li><li>Add documentation for some modules</li><li>Upgrade common-collection from version 4 to 4.4</li><li>Upgrade common-codec version to 1.13</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bug-fix">Bug Fix<a href="#bug-fix" class="hash-link" aria-label="Direct link to Bug Fix" title="Direct link to Bug Fix">‚Äã</a></h3><p>In addition, in response to the feedback from users of version 2.1.2, we also fixed some usability issues, such as the inability to use the same components of Source and Sink, and further improved the stability.</p><ul><li>Fixed the problem of Hudi Source loading twice</li><li>Fix the problem that the field TwoPhaseCommit is not recognized after Doris 0.15</li><li>Fixed abnormal data output when accessing Hive using Spark JDBC</li><li>Fix JDBC data loss when partition_column (partition mode) is set</li><li>Fix KafkaTableStream schema JSON parsing error</li><li>Fix Shell script getting APP_DIR path error</li><li>Updated Flink RunMode enumeration to get correct help messages for run modes</li><li>Fix the same source and sink registered connector cache error</li><li>Fix command line parameter -t( ‚Äî check) conflict with Flink deployment target parameter</li><li>Fix Jackson type conversion error problem</li><li>Fix the problem of failure to run scripts in paths other than SeaTunnel_Home</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="acknowledgment">Acknowledgment<a href="#acknowledgment" class="hash-link" aria-label="Direct link to Acknowledgment" title="Direct link to Acknowledgment">‚Äã</a></h3><p>Thanks to all the contributors (GitHub ID, in no particular order,), it is your efforts that fuel the launch of this version, and we look forward to more contributions to the Apache SeaTunnel(Incubating) community!</p><p><code>leo65535, CalvinKirs, mans2singh, ashulin, wanghuan2054, lhyundeadsoul, tobezhou33, Hisoka-X, ic4y, wsyhj, Emor-nj, gleiyu, smallhibiscus, Bingz2, kezhenxu94, youyangkou, immustard, Interest1-wyt, superzhang0929, gaaraG, runwenjun</code></p></div><footer class="row docusaurus-mt-lg"></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Upcoming API Connector Development Analysis">Upcoming API Connector Development Analysis</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-06-23T00:00:00.000Z" itemprop="datePublished">June 23, 2022</time> ¬∑ <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><p>After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="priliminary-setup">Priliminary Setup<a href="#priliminary-setup" class="hash-link" aria-label="Direct link to Priliminary Setup" title="Direct link to Priliminary Setup">‚Äã</a></h2><ul><li><p>Environment configuration: JDK8 and Scala2.11 are recommended.</p></li><li><p>As before, we need to download the latest code locally through git and import it into the IDE, project address: <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> . At the same time, switch the branch to api-draft, and currently use this branch to develop the new version of the API and the corresponding Connector. The project structure is as follows:</p><p><img loading="lazy" alt="Project Structure" src="/assets/images/0-82b23e9c80c8b70ce10feaccfc96a2a6.png" width="583" height="1014" class="img_ev3q"></p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">‚Äã</a></h2><ul><li><p>At present, in order to distinguish different Connectors, we put the connectors that support</p><ul><li>Flink/Spark under the <code>seatunnel-connectors/seatunnel-connectors-flink(spark)</code> module.</li><li>New version of the Connector is placed under the <code>seatunnel-connectors/seatunnel-connectors-seatunnel</code> module.</li></ul><p>As we can see from the above figure, we have implemented Fake, Console, Kafka Connector, and Clickhouse Connector is also being implemented.</p></li><li><p>At present, the data type we support is SeaTunnelRow, so no matter the type of data generated by the Source or the type of data consumed by the Sink, it should be SeaTunnelRow.</p></li></ul><h1>Development of Connector</h1><p>Taking Fake Connector as an example, let&#x27;s introduce how to implement a new Connector:</p><ul><li><p>Create a corresponding module with a path under <code>seatunnel-connectors-seatunnel</code>, which is at the same level as other new connectors.</p></li><li><p>Modify the <code>seatunnel-connectors-seatunnel/pom.xml</code> file, add a new module to modules, modify <code>seatunnel-connectors-seatunnel/seatunnel-connector-seatunnel-fake/pom.xml</code>, add seatunnel-api dependencies, and correct parent Quote. The resulting style is as follows:</p><p><img loading="lazy" alt="Style" src="/assets/images/1-27a269d360e9ee05b1dd696eeb0aa8e4.png" width="949" height="568" class="img_ev3q"></p></li><li><p>The next step is to create the corresponding package and related classes, create FakeSource, and need to inherit SeaTunnel Source.</p><ul><li>Note : The Source of SeaTunnel adopts the design of stream and batch integration. The Source of SeaTunnel determines whether current Source is a stream or batch through attribute getBoundedness.</li></ul><p>So you can specify a Source as a stream or batch by dynamic configuration (refer to the default method). The configuration defined by the user in the configuration file can be obtained through the prepare method to realize the customized configuration.</p><p>Then create FakeSourceReader, FakeSource SplitEnumerator, and FakeSourceSplit to inherit the corresponding abstract classes (which can be found in the corresponding classes). As long as we implement the corresponding methods of these classes, then our SeaTunnel Source Connector is basically completed.</p></li><li><p>Next, just follow the existing example to write the corresponding code. The most important one is the FakeSource Reader, which defines how we obtain data from the outside, which is the most critical part of the Source Connector. Every time a piece of data is generated, we need to place it in the collector as shown:</p><p><img loading="lazy" alt="Source" src="/assets/images/2-6e56482ef5f497868040295fe7edff23.png" width="935" height="424" class="img_ev3q"></p></li><li><p>After the code development is complete, we need to configure the configuration file <code>plugin-mapping.properties</code> located under <code>seatunnel-connectors/modules</code>. Adding a seatunnel
<code>.source.FakeSource = seatunnel-connector-fake</code>
means that SeaTunnel can find the jar package corresponding to the project by looking for a Source named FakeSource. This allows the Connector to be used in the normal configuration file.</p></li><li><p>For a detailed description of writing Source and Sink and SeaTunnel API, please refer to the introduction at <code>seatunnel-connectors/seatunnel-connectors-seatunnel/ README.zh.md</code>.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="connector-testing">Connector Testing<a href="#connector-testing" class="hash-link" aria-label="Direct link to Connector Testing" title="Direct link to Connector Testing">‚Äã</a></h2><ul><li>For testing, we can find the <code>seatunnel-flink(spark)-new-connector-example</code> module in seatunnel-examples, and test it against different engines to ensure that the performance of the Connector is as consistent as possible. If you find any discrepancies, you can mark them in the document, modify the configuration file under resource, add our Connector to the configuration, and introduce <code>seatunnel-flink(spark)-new-connector-example/pom.xml</code> dependency, you can execute <code>SeaTunnelApiExample</code> to test.</li><li>The default is stream processing mode, and the execution mode is switched to batch mode by modifying <code>job.mode=BATCH</code> in the environment of the configuration file.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="submit-pr">Submit PR<a href="#submit-pr" class="hash-link" aria-label="Direct link to Submit PR" title="Direct link to Submit PR">‚Äã</a></h2><p>When our Connector is ready, we can submit PR to github. After reviewing by other partners, our contributed Connector will become part of SeaTunnel!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Translator | Critina"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/How dose Apache SeaTunnel refactor the API to decouple with the computing engine">How dose Apache SeaTunnel refactor the API to decouple with the computing engine</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-31T00:00:00.000Z" itemprop="datePublished">May 31, 2022</time> ¬∑ <!-- -->12 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-1b0b7ccac22c107239f10a37321c7719.jpg" width="1920" height="1316" class="img_ev3q"></p><p>Translator | Critina</p><p>In the May joint Meetup between Apache SeaTunnel and Apache Inlong, Li Zongwen, a senior engineer at WhaleOps, shared his experiences about finding and refactoring of the the four major problems with Apache SeaTunnel (Incubating).i.e. the connectors of SeaTunnel have to be implemented many times,the inconsistent parameters, SeaTunnel is not supportive of multiple versions of the engine, and it‚Äôs difficult to upgrade the engine. In order to solve these problems, Li Zongwen aimed to decouple Apache SeaTunnel (Incubating) from thw computing engines, and re-factor the Source and Sink apis to improve the development experience.</p><p>This speech mainly consists of five parts.The first part is about Apache SeaTunnel (Incubator) refactoring background and motivation. The second part introduces Apache SeaTunnel (Incubating) Target for refactoring.The third part discusses Apache SeaTunnel (Incubating) overall design for refactoring. The last two parts is about Apache SeaTunnel (Incubating) Source API design and Apache SeaTunnel (Incubating) Sink API design.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-background-and-motivation-for-refactoring">01 Background and motivation for refactoring<a href="#01-background-and-motivation-for-refactoring" class="hash-link" aria-label="Direct link to 01 Background and motivation for refactoring" title="Direct link to 01 Background and motivation for refactoring">‚Äã</a></h2><p>Those of you who have used Apache SeaTunnel (Incubator) or developers should know that Apache SeaTunnel (Incubator) is now fully coupled with the engine, which is entirely based on Spark or Flink, and so are the configuration file parameters. From the perspective of contributors and users, we can find they face some problems.</p><p>In the view of the contributors, repeated implementing connector is meaningless and it is unable for potential contributors to contribute to the community due to inconsistent engine versions.</p><p>At present, many companies use Lambda architecture, Spark is used for offline operations and Flink is used for real-time operations. In the view of the users, it can be found that Spark may have the Connector of SeaTunnel, but Flink does not, and the parameters of the two engines for the Connector of the same storage engine are not unified, thus resulting a high cost of and deviating from its original intention of being easy to use. And some users question that Flink version 1.14 is not supported nowadays. While with the current SeaTunnel architecture, we must discard the previous version in order to support Flink version 1.14, which will bring great trouble for early version users.</p><p>As a result, it was difficult for us to either upgrade engine or support more versions.</p><p>In addition, Spark and Flink both adopt the Checkpoint fault-tolerant mechanism implemented by Chandy-Lamport algorithm and internally unify DataSet and DataStream. On this premise, we believe decoupling is feasible.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-apache-seatunnel-incubating-decouples-with-computing-engine">02 Apache SeaTunnel (Incubating) decouples with computing engine<a href="#02-apache-seatunnel-incubating-decouples-with-computing-engine" class="hash-link" aria-label="Direct link to 02 Apache SeaTunnel (Incubating) decouples with computing engine" title="Direct link to 02 Apache SeaTunnel (Incubating) decouples with computing engine">‚Äã</a></h2><p>Therefore, in order to solve the problems raised above, we set the following goals.</p><ol><li><p>Connector is only implemented once. To solve the problems that parameters are not unified and Connector is implemented for too many times, we hope to achieve a unified Source and Sink API;</p></li><li><p>Multiple versions of Spark and Flink engines are supported. A translation layer above the Source and Sink API is added to support multiple versions of Spark and Flink engines.</p></li><li><p>The logic for parallel shard of Source and the Sink submission should be clarified. We must provide a good API to support Connector development.</p></li><li><p>The full database synchronization in real-time scenarios should be supported. This is a derivative requirement that many users have mentioned for CDC support. I once participated the Flink CDC community before and many users pointed out that in the CDC scenario, if you wanted to use the Flink CDC directly, each table would have a link and there would be thousands of links for thousands of tables when you need to synchronize the whole library, which was unacceptable for both the database and the DBA. To solve this problem, the simplest way was to introduce Canal„ÄÅDebezium or other components, which were used to pull incremental data to Kafka or other MQ for intermediate storage, and then we could use Flink SQL for synchronization. This actually contradicted the original idea of the Flink CDC to reduce links. However, the Flink CDC aimed only a Connector and was unable to deal with the whole link, so the proposal was not seen in the SeaTunnel community. By the chance of the reconstruction, we submitted the proposal to the SeaTunnel community.</p></li><li><p>Automatic discovery and storage of meta information are realized. The users should have awful experience due to the storage engines such as Kafka lacking of record of the data structure, when we need to read structured data, the user must define the topic of structured data types before read one topic at a time . We hope once the configuration is completed, there is no need to do any redundant work again.</p></li></ol><p>Some people may wonder why we don‚Äôt use Apache Beam directly. That is because Beam sources are divided into BOUNDED and UNBOUNDED sources, which means it needs to be implemented twice. Moreover, some features of Source and Sink are not supported, which will be mentioned later.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-apache-seatunnelincubating-overall-design-for-refactoring">03 Apache SeaTunnel(Incubating) overall design for refactoring<a href="#03-apache-seatunnelincubating-overall-design-for-refactoring" class="hash-link" aria-label="Direct link to 03 Apache SeaTunnel(Incubating) overall design for refactoring" title="Direct link to 03 Apache SeaTunnel(Incubating) overall design for refactoring">‚Äã</a></h2><p><img loading="lazy" src="/assets/images/1-cbc169e3bc121f1008b807103abe9fd8.jpg" width="1440" height="810" class="img_ev3q"></p><p>The Apache SeaTunnel(Incubating) API architecture is described in the picture above.</p><p>The Source &amp; Sink API is one of the core APIS of data integration, which defines the logic for parallel shard of Source and the commitment of Sink to realize the Connector.</p><p>The Engine API includes the translation and the execution layers. The translation is used to translate Souce and Sink API of SeaTunnel into connectors that can be run inside the engine.</p><p>The execution defines the execution logic of Source, Transform, Sink and other operations in the engine.</p><p>The Table SPI is mainly used to expose the interface of Source and Sink in SPI mode, and to specify mandatory and optional parameters of Connector etc.</p><p>The DataType includes SeaTunnel data structure used to isolate engines and declare Table schema.</p><p>The Catalog is Used to obtain Table schemes and Options, etc. The Catalog Storage is used to store Table Schemes defined by unstructured engines such as Kafka.</p><p><img loading="lazy" src="/assets/images/2-507471aef3b2e7dc6ee4bc03188bc784.jpg" width="1440" height="810" class="img_ev3q"></p><p>The execution flow we assumed nowadays can be see in the above picture.</p><ol><li><p>Obtain task parameters from configuration files or UI.</p></li><li><p>Obtain the Table Schema, Option and other information by analyzing the parameters from Catalog.</p></li><li><p>Pull up the Connector of SeaTunnel in SPI mode and inject Table information.</p></li><li><p>Translate the Connector from SeaTunnel into the Connector within the engine.</p></li><li><p>Execute the operation logic of the engine. The multi-table distribution in the picture only exists in the synchronization of the whole database of CDC, while other connectors are single tables and do not need the distribution logic.</p></li></ol><p>It can be seen that the hardest part of the plan is to translate Source and Sink into an internal Source and Sink in the engine.</p><p>Many users today use Apache SeaTunnel (Incubating) not only as a data integration tool but also as a data storage tool, and use a lot of Spark and Flink SQLs. We want to preserve that SQL capability for users to upgrade seamlessly.</p><p><img loading="lazy" src="/assets/images/3-68113e215aa8e91a5d2469ccb37a3c22.jpg" width="1440" height="810" class="img_ev3q"></p><p>According to our research, the feature above shows the ideal execution logic of Source and Sink. Since SeaTunnel is incubated as WaterDrop, the terms in the figure are tended towards Spark.</p><p>Ideally, the Source and Sink coordinators can be run on the Driver, and the Source Reader and Sink Writer will run on the Worker. In terms of the Source Coordinator, we expect it to support several features.</p><p>The first capability is that the slicing logic of data can be dynamically added to the Reader.</p><p>The second is that the coordination of Reader can be supported. Source Reader is used to read data, and then send the data to the engine, and finally to the Source Writer for data writing. Meanwhile, Writer can support the two-phase transaction submission, and the coordinator of Sink supports the aggregation submission requirements of Connector such as Iceberg.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-source-api">04 Source API<a href="#04-source-api" class="hash-link" aria-label="Direct link to 04 Source API" title="Direct link to 04 Source API">‚Äã</a></h2><p>After research, we found the following features that are required by Source.</p><ol><li><p>Unified offline and real-time API , which supports that source is implemented only once and supports both offline and real-time API;</p></li><li><p>Supportive of parallel reading. For example that Kafka generates a reader for each partition and execute in parallel.</p></li><li><p>Supporting dynamic slice-adding. For example, Kafka defines a regular topic, when a new topic needs to be added due to the volume of business, the Source API allows to dynamically add the slice to the job.</p></li><li><p>Supporting the work of coordinating reader, which is currently only needed in the CDC Connector. CDC is currently supported by NetFilx‚Äôs DBlog parallel algorithms, which requires reader coordination between full synchronization and incremental synchronization.</p></li><li><p>Supporting a single reader to process multiple tables, i.e. to allows the whole database synchronization in the real-time scenario as mentioned above.</p></li></ol><p><img loading="lazy" src="/assets/images/4-ac907e3f8e305b9f6585d2171013a973.jpg" width="1440" height="810" class="img_ev3q"></p><p>Based on the above requirements, we have created the basic API as shown in the figure above. And the code has been submitted to the API-Draft branch in the Apache SeaTunnel(Incubator) community. If you‚Äôre interested, you can view the code in detail.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-adapt-to-spark-and-flink-engines">How to adapt to Spark and Flink engines<a href="#how-to-adapt-to-spark-and-flink-engines" class="hash-link" aria-label="Direct link to How to adapt to Spark and Flink engines" title="Direct link to How to adapt to Spark and Flink engines">‚Äã</a></h3><p>Flink and Spark unify the API of DataSet and DataStream, and they can support the first two features. Then, for the remaining three features, how do we</p><ul><li>Support dynamic slice-addingÔºü</li><li>Support the work of coordinating readerÔºü</li><li>Support a single reader to process multiple tablesÔºü</li></ul><p>Let&#x27;s review the design with questions.</p><p><img loading="lazy" src="/assets/images/5-f743cf882b36bb3c383c66dec4bad95f.jpg" width="1440" height="810" class="img_ev3q"></p><p>We found that other connectors do not need coordinators, except for CDC. For those connectors that do not need coordinators, we have a Source that supports parallel execution and engine translation.</p><p>As shown in the figure above, there is a slice enumerator on the left, which can list which slices the source needs and show what there are. After enumerating slices in real time, each slice would be distributed to SourceReader, the real data reading module. Boundedness marker is used to differentiate offline and real-time operations. Connector can mark whether there is a stop Offset in a slice. For example, Kafka can support real-time and offline operations. The degree of parallelism can be set for the ParallelSource in the engine to support parallel reading.</p><p>As shown in the figure above, in a scenario where a coordinator is required, Event transmission is done between the Reader and Enumerator. Enumerator coordinates events by the Event sent by the Reader. The Coordinated Source needs to ensure single parallelism at the engine level to ensure data consistency. Of course, this does not make good use of the engine‚Äôs memory management mechanism, but trade-offs are necessary.</p><p><img loading="lazy" src="/assets/images/6-ef5d53449db28f4466c14827335d07a6.jpg" width="1440" height="810" class="img_ev3q"></p><p>For the last question, how can we support a single reader to process multiple tables? This involves the Table API layer. Once all the required tables have been read from the Catalog, some of the tables may belong to a single job and can be read by a link, and some may need to be separated, depending on how Source is implemented. Since this is a special requirement, we want to make it easier for the developers. In the Table API layer, we will provide a SupportMultipleTable interface to declare that Source supports multiple Table reads. The Source is implemented based on the corresponding deserializer of multiple tables. As for how to separate derived multi-table data, Flink will adopt Side Output mechanism, while Spark is going to use Filter or Partition mechanism.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-sink-api">5 Sink API<a href="#5-sink-api" class="hash-link" aria-label="Direct link to 5 Sink API" title="Direct link to 5 Sink API">‚Äã</a></h2><p>At present, there are not many features required by Sink, but three mojor requirements are considerable according to our research.</p><p>The first is about idempotent writing, which requires no code and depends on whether the storage engine can support it.</p><p>The second is about distributed transactions. The mainstream method is two-phase commitments, such as Kafka etc.</p><p>The third is about the submission of aggregation. For Storage engines like Iceberg and Hoodie, we hope there is no issues triggered by small files, so we expect to aggregate these files into a single file and commit it as a whole.</p><p>Based on these three requirements, we built three APIS: SinkWriter, SinkCommitter, and SinkAggregated Committer. SinkWriter plays a role of writing, which may or may not be idempotent. SinkCommitter supports for two-phase commitments. SinkAggregatedCommitter supports for aggregated commitments.</p><p><img loading="lazy" src="/assets/images/7-e9830d63f81f7139a7cd1d4d9b9f5e43.jpg" width="1440" height="810" class="img_ev3q"></p><p>Ideally, AggregatedCommitter runs in Driver in single or parallel, and Writer and Committer run in Worker with multiple parallels, with each parallel carrying its own pre-commit work and then send Aggregated messages to Aggregated committers.</p><p>Current advanced versions of Spark and Flink all support AggregatedCommitter running on the Driver(Job Manager) and Writer and Committer running on the worker(Task Manager).</p><p><img loading="lazy" src="/assets/images/8-aa0537c76d15543b46623445ff00e490.jpg" width="1440" height="810" class="img_ev3q"></p><p>However, for the lower versions of Flink, AggregatedCommitter cannot be supported to run in JM, so we are also carrying translation adaptation. Writer and Committer will act as pre-operators, packaged by Flink‚Äôs ProcessFunction, supports concurrent pre-delivery and write, and implement two-phase commitment based on Flink‚Äôs Checkpoint mechanism. This is also the current 2PC implementation of many of Flink connectors. The ProcessFunction can send messages about pre-commits to downstream Aggregated committers, which can be wrapped around operators such as SinkFunction or ProcessFunction. Of course, We need to ensure that only one single parallel will be started by the AggregatedCommitter in case of the broken of the logic of the aggregated commitment.</p><p>Thank you for watching. If you‚Äôre interested in the specific implementations mentioned in my speech, you can refer to the Apache SeaTunnel (Incubating) community and check out the API-Draft branch code. Thank you again.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Author | Fan Jia, Apache SeaTunnel(Incubating) Contributor"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/How to synchronize tens of billions of data based on SeaTunnel&#x27;s ClickHouse">How to synchronize tens of billions of data based on SeaTunnel&#x27;s ClickHouse</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-10T00:00:00.000Z" itemprop="datePublished">May 10, 2022</time> ¬∑ <!-- -->8 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-c3f068094d4f0308d7100502a6162925.jpg" width="1920" height="1275" class="img_ev3q"></p><p>Author | Fan Jia, Apache SeaTunnel(Incubating) Contributor
Editor | Test Engineer Feng Xiulan</p><p>For importing billions of batches of data, the traditional JDBC approach does not perform as well as it should in some massive data synchronization scenarios. To write data faster, Apache SeaTunnel (Incubating) has just released version 2.1.1 to provide support for ClickhouseFile-Connector to implement Bulk load data writing.</p><p>Bulk load means synchronizing large amounts of data to the target DB. SeaTunnel currently supports data synchronization to ClickHouse.</p><p>At the Apache SeaTunnel (Incubating) April Meetup, Apache SeaTunnel (Incubating) contributor Fan Jia shared the topic of &quot;ClickHouse bulk load implementation based on SeaTunnel&quot;, explaining in detail the implementation principle and process of ClickHouseFile for efficient processing of large amounts of data.</p><p>Thanks to the test engineer Feng Xiulan for the article arrangement!</p><p>This presentation contains seven parts.</p><ul><li>State of ClickHouse Sink</li><li>Scenarios that ClickHouse Sink isn&#x27;t good at </li><li>Introduction to the ClickHouseFile plugin</li><li>ClickHouseFile core technologies</li><li>Analysis of ClickHouseFile plugin implementation</li><li>Comparison of plug-in capabilities</li><li>Post-optimization directions</li></ul><p><img loading="lazy" src="/assets/images/0-1-56defefcc273a6e21b09dd483bf95914.png" width="1171" height="1171" class="img_ev3q"></p><p>Fan Jia,  Apache SeaTunnel (Incubating) contributor, Senior Enginee of WhaleOps.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-status-of-clickhouse-sink">01 Status of ClickHouse Sink<a href="#01-status-of-clickhouse-sink" class="hash-link" aria-label="Direct link to 01 Status of ClickHouse Sink" title="Direct link to 01 Status of ClickHouse Sink">‚Äã</a></h2><p>At present, the process of synchronizing data from SeaTunnel to ClickHouse is as follows: as long as the data source is supported by SeaTunnel, the data can be extracted, converted (or not), and written directly to the ClickHouse sink connector, and then written to the ClickHouse server via JDBC. </p><p><img loading="lazy" src="/assets/images/1-76284c6612152506e0111e0f0d25d0f5.png" width="1139" height="585" class="img_ev3q"></p><p>However, there are some problems with writing to the ClickHouse server via traditional JDBC.</p><p>Firstly, the tool used now is the driver provided by ClickHouse and implemented via HTTP, however, HTTP is not very efficient to implement in certain scenarios. The second is the huge amount of data, if there is duplicate data or a large amount of data written at once, it needs to generate the corresponding insert statement and send it via HTTP to the ClickHouse server-side by the traditional method, where it is parsed and executed item by item or in batches, which does not allow data compression.</p><p>Finally, there is the problem we often encounter, i.e. too much data may lead to an OOM on the SeaTunnel side or a server-side hang due to too much data being written to the server-side too often.</p><p>So we thought, is there a faster way to send than HTTP? If data pre-processing or data compression could be done on the SeaTunnel side, then the network bandwidth pressure would be reduced and the transmission rate would be increased.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-scenarios-that-clickhouse-sink-isnt-good-at">02 Scenarios that ClickHouse Sink isn&#x27;t good at<a href="#02-scenarios-that-clickhouse-sink-isnt-good-at" class="hash-link" aria-label="Direct link to 02 Scenarios that ClickHouse Sink isn&#x27;t good at" title="Direct link to 02 Scenarios that ClickHouse Sink isn&#x27;t good at">‚Äã</a></h2><ol><li>If the HTTP transfer protocol is used, HTTP may not be able to handle it when the volume of data is too large and the batch is sending requests in micro-batches.</li><li>Too many INSERT requests may put too much pressure on the server. The bandwidth can handle a large number of requests, but the server-side is not always able to carry them. The online server not only needs data inserts but more importantly, the query data can be used by other business teams. If the server cluster goes down due to too much-inserted data, it is more than worth the cost.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-clickhouse-file-core-technologies">03 ClickHouse File core technologies<a href="#03-clickhouse-file-core-technologies" class="hash-link" aria-label="Direct link to 03 ClickHouse File core technologies" title="Direct link to 03 ClickHouse File core technologies">‚Äã</a></h2><p>In response to these scenarios that ClickHouse is not good at, we wondered is there a way to do data compression right on the Spark side, without increasing the resource load on the Server when writing data, and with the ability to write large amounts of data quickly? So we developed the ClickHouseFile plugin to solve the problem.</p><p>The key technology of the ClickHouseFile plugin is ClickHouse -local. ClickHouse-local mode allows users to perform fast processing of local files without having to deploy and configure a ClickHouse Server. C lickHouse-local uses the same core as ClickHouse Server, so it supports most features as well as the same format and table engine.</p><p>These two features mean that users can work directly with local files without having to do the processing on the ClickHouse Server side. Because it is the same format, the data generated by the operations we perform on the remote or SeaTunnel side is seamlessly compatible with the server-side and can be written to using ClickHouse local. ClickHouse local is the core technology for the implementation of ClickHouseFile, which allows for implementing the ClickHouse file connector.</p><p>ClickHouse local core is used in the following ways.</p><p><img loading="lazy" src="/assets/images/2-2367f70ae655c30a94a2ec65e67a6b26.png" width="1112" height="262" class="img_ev3q"></p><p>First line: pass the data to the test_table table of the ClickHouse-local program via the Linux pipeline.</p><p>Lines two to five: create a result_table for receiving data.</p><p>The sixth line: pass data from test_table to the result_table.</p><p>Line 7: Define the disk path for data processing.</p><p>By calling the Clickhouse-local component, the Apache SeaTunnel (Incubating) is used to generate the data files and compress the data. By communicating with the Server, the generated data is sent directly to the different nodes of Clickhouse and the data files are then made available to the nodes for the query.</p><p>Comparison of the original and current implementations.</p><p><img loading="lazy" src="/assets/images/3-6204c709b48243f88914bfd492dc67f2.png" width="1272" height="576" class="img_ev3q"></p><p>Originally, the data, including the insert statements was sent by Spark to the server, and the server did the SQL parsing, generated and compressed the table data files, generated the corresponding files, and created the corresponding indexes. If we use ClickHouse local technology, the data file generation, file compression and index creation are done by SeaTunnel, and the final output is a file or folder for the server-side, which is synchronized to the server and the server can queries the data without additional operations.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-core-technical-points">04 Core technical points<a href="#04-core-technical-points" class="hash-link" aria-label="Direct link to 04 Core technical points" title="Direct link to 04 Core technical points">‚Äã</a></h2><p><img loading="lazy" src="/assets/images/4-d47e1da865afa7ea4de50b2d6e4b6ac1.png" width="1164" height="435" class="img_ev3q"></p><p>The above process makes data synchronization more efficient, thanks to three optimizations we have made to it.</p><p>Firstly, the data is transferred from the pipeline to the ClickHouseFile by the division, which imposes limitations in terms of length and memory. For this reason, we write the data received by the ClickHouse connector, i.e. the sink side, to a temporary file via MMAP technology, and then the ClickHouse local reads the data from the temporary file to generate our target local file, in order to achieve the effect of incremental data reading and solve the OM problem.</p><p><img loading="lazy" src="/assets/images/5-9f00635b1727843f705cd5a28632e2e4.png" width="1206" height="565" class="img_ev3q"></p><p>Secondly, it supports sharding. If only one file or folder is generated in a cluster, the file is distributed to only one node, which will greatly reduce the performance of the query. Therefore, we carry out slicing support. Users can set the key for slicing in the configuration folder, and the algorithm will divide the data into multiple log files and write them to different cluster nodes, significantly improving the read performance.</p><p><img loading="lazy" src="/assets/images/6-35b30550d6a18fbea49856083aa85094.png" width="1043" height="558" class="img_ev3q"></p><p>The third key optimization is file transfer. Currently, SeaTunnel supports two file transfer methods, one is SCP, which is characterized by security, versatility, and no additional configuration; the other is RSYNC, which is somewhat fast and efficient and supports breakpoint resume, but requires additional configuration, users can choose between the way suits their needs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-plugin-implementation-analysis">05 Plugin implementation analysis<a href="#05-plugin-implementation-analysis" class="hash-link" aria-label="Direct link to 05 Plugin implementation analysis" title="Direct link to 05 Plugin implementation analysis">‚Äã</a></h2><p>In summary, the general implementation process of ClickHouseFile is as follows.</p><p><img loading="lazy" src="/assets/images/7-1be978da30a55fe0289c683f2ae61aac.png" width="533" height="635" class="img_ev3q"></p><p>1.caching data to the ClickHouse sink side.
2.calling ClickHouse-local to generate the file.
3.sending the data to the ClickHouse server.
4.Execution of the ATTACH command.</p><p>With the above four steps, the generated data reaches a queryable state.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="06-comparison-of-plug-in-capabilities">06 Comparison of plug-in capabilities<a href="#06-comparison-of-plug-in-capabilities" class="hash-link" aria-label="Direct link to 06 Comparison of plug-in capabilities" title="Direct link to 06 Comparison of plug-in capabilities">‚Äã</a></h2><p><img loading="lazy" src="/assets/images/8-261e7ba686f3fadf5d7c1445e9be5b66.png" width="1071" height="485" class="img_ev3q"></p><p>(a) In terms of data transfer, ClickHouseFile is more suitable for massive amounts of data, with the advantage that no additional configuration is required and it is highly versatile, while ClickHouseFile is more complex to configure and currently supports fewer engines.</p><p>In terms of environmental complexity, ClickHouse is more suitable for complex environments and can be run directly without additional configuration.</p><p>In terms of versatility, ClickHouse, due to being an officially supported JDBC diver by SeaTunnel, basically supports all engines for data writing, while ClickHouseFile supports relatively few engines.</p><p>In terms of server pressure, ClickHouseFile&#x27;s advantage shows when it comes to massive data transfers that don&#x27;t put too much pressure on the server.</p><p>However, the two are not in competition and the choice needs to be based on the usage scenario.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="07-follow-up-plans">07 Follow-up plans<a href="#07-follow-up-plans" class="hash-link" aria-label="Direct link to 07 Follow-up plans" title="Direct link to 07 Follow-up plans">‚Äã</a></h2><p>Although SeaTunnel currently supports the ClickHouseFile plugin, there are still many defects that need to be optimized, mainly including</p><ul><li>Rsync support.</li><li>Exactly-Once support.</li><li>Zero Copy support for transferring data files.</li><li>More Engine support.</li></ul><p>Anyone interested in the above issues is welcome to contribute to the follow-up plans, or tell me your ideas!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="At the Apache SeaTunnel (Incubating) Meetup in April, Yuan Hongjun, a big data expert and OLAP platform architect at Kidswant, shared a topic of SeaTunnel Application and Refactoring at Kidswant."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/SeaTunnel Application and Refactoring at Kidswant">SeaTunnel Application and Refactoring at Kidswant</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-01T00:00:00.000Z" itemprop="datePublished">May 1, 2022</time> ¬∑ <!-- -->10 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-38f7968af0b7239e9d427a85adee4452.png" width="1920" height="1080" class="img_ev3q"></p><p>At the Apache SeaTunnel (Incubating) Meetup in April, Yuan Hongjun, a big data expert and OLAP platform architect at Kidswant, shared a topic of SeaTunnel Application and Refactoring at Kidswant.</p><p>The presentation contains five parts.</p><ul><li>Background of the introduction of Apache SeaTunnel (Incubating) by Kidswant</li><li>A comparison of mainstream tools for big data processing</li><li>The implementation of Apache SeaTunnel (Incubating)</li><li>Common problems in Apache SeaTunnel (Incubating) refactoring</li><li>Predictions on the future development of Kidswant</li></ul><p><img loading="lazy" src="/assets/images/0-1-4c853aa726b29acc5954ba53240dc2b8.png" width="2578" height="2567" class="img_ev3q"></p><p>Yuan Hongjun, Big data expert, OLAP platform architect of Kidswant. He has many years of experience in big data platform development and management, and has rich research experience in data assets, data lineage mapping, data governance, OLAP, and other fields.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-background">01 Background<a href="#01-background" class="hash-link" aria-label="Direct link to 01 Background" title="Direct link to 01 Background">‚Äã</a></h2><p><img loading="lazy" src="/assets/images/1-198d9a9b685f80a9814d5620d1194355.png" width="1166" height="720" class="img_ev3q"></p><p>At present, Kidswant‚Äôs OLAP platform consists of seven parts: metadata layer, task layer, storage layer, SQL layer, scheduling layer, service layer, and monitoring layer. This sharing focuses on offline tasks in the task layer.</p><p>In fact, Kidswant had a complete internal collection and push system, but due to some historical legacy issues, the company‚Äôs existing platform could not quickly support the OLAP platform getting online, so at that time the company had to abandon its own platform and start developing a new system instead.
There were three options in front of OLAP at the time.</p><p>1, Re-develop the collection and push system.</p><p>2„ÄÅSelf-R&amp;D.</p><p>3, Participate in open source projects.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-big-data-processing-mainstream-tools-comparison">02 Big data processing mainstream tools comparison<a href="#02-big-data-processing-mainstream-tools-comparison" class="hash-link" aria-label="Direct link to 02 Big data processing mainstream tools comparison" title="Direct link to 02 Big data processing mainstream tools comparison">‚Äã</a></h2><p>These three options have their own pros and cons. Carrying re-research and development based on the collection and push system is convenient for us to take advantage of the experience of previous results and avoid repeatedly stepping into the pit. But the disadvantage is that it requires a large amount of code, time, a longer research period, and with less abstract code and lots of customized functions bound to the business, it‚Äôs difficult to do the re-development.</p><p>If completely self-developed, though the development process is autonomous and controllable, some engines such as Spark can be done to fit our own architecture, while the disadvantage is that we may encounter some unknown problems.</p><p>For the last choice, if we use open-source frameworks, the advantage is that there is more abstract code, and the framework can be guaranteed in terms of performance and stability after verification by other major companies. Therefore Kidswant mainly studied three open-source data synchronization tools, DATAX, Sqoop, and SeaTunnel in the early stages of OLAP data synchronization refactoring.</p><p><img loading="lazy" src="/assets/images/2-fa4598ecce2b564564a8dfb2f7ccae87.png" width="919" height="720" class="img_ev3q"></p><p>From the diagram we can see that Sqoop‚Äôs main function is data synchronization for RDB, and its implementation is based on MAP/REDUCE. Sqoop has rich parameters and command lines to perform various operations. The advantage of Sqoop is that it fits Hadoop ecology, and already supports most of the conversion from RDB to HIVE arbitrary source, with a complete set of commands and APIs.</p><p>The disadvantages are that Sqoop only supports RDB data synchronization and has some limitations on data files, and there is no concept of data cleansing yet.</p><p><img loading="lazy" src="/assets/images/3-53716e33ac191ff35ff66591ba4e1711.png" width="1174" height="720" class="img_ev3q"></p><p>DataX mainly aims at synchronizing data from any source by configurable files + multi-threading, which runs three main processes: Reader, Framework, and Writer, where Framework mainly plays the role of communication and leaving empty space.</p><p>The advantage of DataX is that it uses plug-in development, has its own flow control and data control, and is active in the community, with DataX‚Äôs official website offering data pushes from many different sources. The disadvantage of DataX, however, is that it is memory-based and there may be limitations on the amount of data available.</p><p><img loading="lazy" src="/assets/images/4-1897837b0d7e4b52bc086c94b5aa4aea.png" width="1025" height="720" class="img_ev3q"></p><p>Apache SeaTunnel (Incubating) also does data synchronization from any source and implements the process in three steps: source, transform and sink based on configuration files, Spark or Flink. </p><p>The advantage is that the current 2.1.0 version has a very large number of plug-ins and source pushes, based on the idea of plug-ins also makes it very easy to extend and embrace Spark and Flink while with a distributed architecture. The only downside to Apache SeaTunnel (Incubating) is probably the lack of IP calls at the moment and the need to manage the UI interface by yourself.</p><p>In summary, although Sqoop is distributed, it only supports data synchronization between RDB and HIVE, Hbase and has poor scalability, which is not convenient for re-development. DataX is scalable and stable overall, but because it is a standalone version, it cannot be deployed in a distributed cluster, and there is a strong dependency between data extraction capability and machine performance. SeaTunnel, on the other hand, is similar to DataX and makes up for the flaw of non-distributed DataX. It also supports real-time streaming, and the community is highly active as a new product. We chose SeaTunnel based on a number of factors such as whether it supported distributed or not, and whether it needed to be deployed on a separate machine.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-implementation">03 Implementation<a href="#03-implementation" class="hash-link" aria-label="Direct link to 03 Implementation" title="Direct link to 03 Implementation">‚Äã</a></h2><p>On the Apache SeaTunnel (Incubating) website, we can see that the basic process of Apache SeaTunnel (Incubating) consists of three parts: source, transform and sink. According to the guidelines on the website, Apache SeaTunnel (Incubating) requires a configuration script to start, but after some research, we found that the final execution of Apache SeaTunnel (Incubating) is bansed on an application submitted by spark-submit that relies on the config file.</p><p>This initialization, although simple, has the problem of having to rely on the config file, which is generated and then cleared after each run, and although it can be dynamically generated in the scheduling script, it raises two questions: 1) whether frequent disk operations make sense; and 2) whether there is a more efficient way to support Apache SeaTunnel (Incubating).</p><p><img loading="lazy" src="/assets/images/5-c2ec6a581e98f15b50a0925325be6acf.png" width="1196" height="720" class="img_ev3q"></p><p>With these considerations in mind, we added a Unified Configuration Template Platform module to the final design solution. Scheduling is done by initiating a commit command, and Apache SeaTunnel (Incubating) itself pulls the configuration information from the unified configuration template platform, then loads and initializes the parameters.</p><p><img loading="lazy" src="/assets/images/6-5d4653fed2f38a65b3e8c04428e1d53e.png" width="1048" height="720" class="img_ev3q"></p><p>The diagram above shows the business process for Kidswant‚Äôs OLAP, which is divided into three sections. The overall flow of data from Parquet, i.e. Hive, through the Parquet tables to KYLIN and CK source.</p><p><img loading="lazy" src="/assets/images/7-2eed2f9ff97c6bfcc712e992568dd102.png" width="1080" height="687" class="img_ev3q"></p><p>This is the page where we construct the model, which is generated mainly through drag and drop, with some transactional operations between each table, and micro-processing for Apache SeaTunnel (Incubating) on the right.</p><p><img loading="lazy" src="/assets/images/8-8d444012b45ea1a51088db223d29efa6.png" width="1247" height="596" class="img_ev3q"></p><p>So we end up submitting the commands as above, where the first one marked in red is <!-- -->[-conf customconfig/jars]<!-- -->, referring to the fact that the user can then unify the configuration template platform for processing, or specify it separately when modeling. The last one marked in red is <!-- -->[421 $start_time $end_time $taskType]<!-- --> Unicode, which is a unique encoding.</p><p>Below, on the left, are the 38 commands submitted by our final dispatch script. Below, on the right, is a modification made for Apache SeaTunnel (Incubating), and you can see a more specific tool class called WaterdropContext. It can first determine if Unicode exists and then use Unicode_code to get the configuration information for the different templates, avoiding the need to manipulate the config file.</p><p>In the end, the reportMeta is used to report some information after the task is completed, which is also done in Apache SeaTunnel (Incubating).</p><p><img loading="lazy" src="/assets/images/9-54db46eb9209112af1884f1ef6454536.png" width="720" height="752" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/10-0472de8100ba32800e40ea9e86f3b35f.png" width="1280" height="585" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/11-910cd332e1ebf6b9d7d3ea9186a5c8fa.png" width="1280" height="512" class="img_ev3q"></p><p>In the finalized config file as above, it is worth noting that in terms of transforms, Kidswant has made some changes. The first is to do desensitization for mobile phones or ID numbers etc. If the user specifies a field, they do it by field, if not they will scan all fields and then desensitize and encrypt them according to pattern matching.</p><p>Second, transform also supports custom processing, as mentioned above when talking about OLAP modeling. With the addition of HideStr, the first ten fields of a string of characters can be retained and all characters at the back encrypted, providing some security in the data.</p><p>Then, on the sink side, we added pre_sql in order to support the idempotency of the task, which is mainly done for tasks such as data deletion, or partition deletion, as the task cannot be run only once during production, and this design needed to account for the data deviation and correctness once operations such as reruns or complement occur.</p><p>On the right side of the diagram, on the Sink side of a Clickhouse, we have added an is_senseless_mode, which forms a read/write senseless mode, where the user does not perceive the whole area when querying and complementing but uses the CK partition conversion, i.e. the command called MOVE PARTITION TO TABLE to operate.</p><p>A special note here is the Sink side of KYLIN. KYLIN is a very special source with its own set of data entry logic and its monitoring page, so the transformation we have done on KYLIN is simply a call to its API operation and a simple API call and constant polling of the state when using KYLIN, so the resources for KYLIN are limited in the Unified Template Configuration platform.</p><p><img loading="lazy" src="/assets/images/12-859f200352aab300d0e421ba01e86da6.png" width="1249" height="586" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-common-problems-about-the-apache-seatunnel-incubating-transformation">04 Common problems about the Apache SeaTunnel (Incubating) transformation<a href="#04-common-problems-about-the-apache-seatunnel-incubating-transformation" class="hash-link" aria-label="Direct link to 04 Common problems about the Apache SeaTunnel (Incubating) transformation" title="Direct link to 04 Common problems about the Apache SeaTunnel (Incubating) transformation">‚Äã</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="01-oom--too-many-parts">01 OOM &amp; Too Many Parts<a href="#01-oom--too-many-parts" class="hash-link" aria-label="Direct link to 01 OOM &amp; Too Many Parts" title="Direct link to 01 OOM &amp; Too Many Parts">‚Äã</a></h4><p>The problem usually arises during the Hive to Hive process, even if we go through automatic resource allocation, but there are cases where the data amount suddenly gets bigger, for example after holding several events. Such problems can only be avoided by manually and dynamically tuning the reference and adjusting the data synchronization batch time. In the future, we may try to control the data volume to achieve fine control.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="02-field-and-type-inconsistency-issues">02 Field and type inconsistency issues<a href="#02-field-and-type-inconsistency-issues" class="hash-link" aria-label="Direct link to 02 Field and type inconsistency issues" title="Direct link to 02 Field and type inconsistency issues">‚Äã</a></h4><p>When the model runs, the user will make some changes to the upstream tables or fields that the task depends on, and these changes may lead to task failure if they are not perceived. The current solution is to rely on data lineage+ snapshots for advance awareness to avoid errors.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="03-custom-data-sources--custom-separators">03 Custom data sources &amp; custom separators<a href="#03-custom-data-sources--custom-separators" class="hash-link" aria-label="Direct link to 03 Custom data sources &amp; custom separators" title="Direct link to 03 Custom data sources &amp; custom separators">‚Äã</a></h4><p>If the finance department requires a customized separator or jar information, the user can now specify the loading of additional jar information as well as the separator information themselves in the unified configuration template platform.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="04-data-skewing-issues">04 Data skewing issues<a href="#04-data-skewing-issues" class="hash-link" aria-label="Direct link to 04 Data skewing issues" title="Direct link to 04 Data skewing issues">‚Äã</a></h4><p>This may be due to users setting their parallelism but not being able to do so perfectly. We haven‚Äôt finished dealing with this issue yet, but we may add post-processing to the Source module to break up the data and complete the skew.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="05-kylin-global-dictionary-lock-problem">05 KYLIN global dictionary lock problem<a href="#05-kylin-global-dictionary-lock-problem" class="hash-link" aria-label="Direct link to 05 KYLIN global dictionary lock problem" title="Direct link to 05 KYLIN global dictionary lock problem">‚Äã</a></h4><p>As the business grows, one cube will not be able to meet the needs of the users, so it will be necessary to create more than one cube. If the same fields are used between multiple cubes, the problem of KYLIN global dictionary lock will be encountered. The current solution is to separate the scheduling time between two or more tasks, or if this is not possible, we can make a distributed lock control, where the sink side of KYLIN has to get the lock to run.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-an-outlook-on-the-future-of-kidswant">05 An outlook on the future of Kidswant<a href="#05-an-outlook-on-the-future-of-kidswant" class="hash-link" aria-label="Direct link to 05 An outlook on the future of Kidswant" title="Direct link to 05 An outlook on the future of Kidswant">‚Äã</a></h2><ol><li>Multi-source data synchronization, maybe processing for RDB sources</li><li>Real-time Flink-based implementation</li><li>Take over the existing collection and scheduling platform (mainly to solve the problem of splitting library and tables)</li><li>Data quality verification, like some null values, the vacancy rate of the whole data, main time judgment, etc.</li></ol><p>This is all I have to share, I hope we can communicate more with the community in the future and make progress together, thanks!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="1"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Innovation of Data Integration Technology in the Intelligent Era">Innovation of Data Integration Technology in the Intelligent Era</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-04-08T00:00:00.000Z" itemprop="datePublished">April 8, 2022</time> ¬∑ <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="1" src="/assets/images/1-2af82fe19d2d0ad453f547d8c27647c4.png" width="1600" height="900" class="img_ev3q"></p><p>As we know, only manageable, callable, computable, and magnetizable data resources can be deposited as assets. The interconnection of information systems has created a huge demand for multi-source and multidimensional data integration, which imposes strict requirements on data processing and integration tools.</p><p>In the era of intelligence, under the trends of ‚Äúsmart city‚Äù, ‚Äúsmart governance‚Äù, and ‚Äúintelligent products‚Äù, enterprises are mostly faced with the challenge of how to achieve efficient data push, improve platform quality, and ensure data security. Only by choosing the right data integration tools and platforms can data play a key role.</p><p>As a next-generation high-performance, distributed, and massive data integration framework, Apache SeaTunnel is committed to making data synchronization simpler and more efficient and accelerating the implementation of distributed data processing capabilities in the production environment.</p><p>At the Apache SeaTunnel Meetup (April 16, 2022), the community will invite experienced Apache SeaTunnel users to share the best practices of the project in intelligent production environments. In addition, there will be contributors to analyze the source code of Apache SeaTunnel, guiding you to have a comprehensive and in-depth understanding of this powerful data integration tool.</p><p>Whether you are a beginner who is interested in Apache SeaTunnel or users who encounter complex and difficult deployment problems in daily practice, you can come here to communicate with our instructors and get the answers you want.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-sign-up">01 Sign up<a href="#01-sign-up" class="hash-link" aria-label="Direct link to 01 Sign up" title="Direct link to 01 Sign up">‚Äã</a></h2><p>Apache SeaTunnel Meetup | April online live registration has been started, hurry up and register!</p><p>Time: 2022‚Äì4‚Äì16 14:00‚Äì17:00</p><p>Format: live online</p><p>Click the link to register (free):¬†<a href="https://www.slidestalk.com/m/780" target="_blank" rel="noopener noreferrer">https://www.slidestalk.com/m/780</a></p><p>Join Slack:</p><p><a href="https://join.slack.com/t/apacheseatunnel/shared_invite/zt-10u1eujlc-g4E~ppbinD0oKpGeoo_dAw" target="_blank" rel="noopener noreferrer">https://join.slack.com/t/apacheseatunnel/shared_invite/zt-10u1eujlc-g4E~ppbinD0oKpGeoo_dAw</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-highlights">02 Highlights<a href="#02-highlights" class="hash-link" aria-label="Direct link to 02 Highlights" title="Direct link to 02 Highlights">‚Äã</a></h2><ul><li>Detailed case study</li><li>Feature Analysis</li><li>Tips to avoid stepping into the pit from enterprises</li><li>Open-source community growth strategy</li><li>Face-to-face Q&amp;A with industry technical experts</li><li>Surprise gifts</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-event-agenda">03 Event Agenda<a href="#03-event-agenda" class="hash-link" aria-label="Direct link to 03 Event Agenda" title="Direct link to 03 Event Agenda">‚Äã</a></h2><p>On the day of the event, big data engineers from Kidswant and oppo will share the front-line practical experience, and senior engineers from WhaleOps will give a ‚Äúhard-core‚Äù explanation of the important function updates of Apache SeaTunnel.</p><p><img loading="lazy" alt="2" src="/assets/images/2-a5ffa068e39b9488fa6ebdc44358b9a3.png" width="1080" height="1075" class="img_ev3q"></p><p>Yuan Hongjun, Kidswant Big Data Expert, OLAP Platform Architect</p><p>Years of experience in R&amp;D and management of big data platforms, rich research experience in data assets, data linkage, data governance, OLAP, and other fields</p><p>Time: 14:00‚Äì14:40</p><p>Topic: Application Practice of Apache SeaTunnel in Kidswant</p><p>Speech outline: How to push data efficiently? How to improve the quality of the platform? How to ensure data security? What changes did Kidswant make to Apache SeaTunnel?</p><p><img loading="lazy" alt="3" src="/assets/images/3-22d7703611173db9493fd49adc5687ae.png" width="1080" height="1080" class="img_ev3q"></p><p>Fan Jia, WhaleOps Senior Engineer</p><p>Time: 14:40‚Äì15:20</p><p>Topic: Clickhouse Bulk Load Implementation Based on Apache SeaTunnel</p><p>Speech outline: How to implement the bulk load data synchronization function of Clickhouse by extending the Connector of Apache SeaTunnel?</p><p><img loading="lazy" alt="4" src="/assets/images/4-acd48a1d76d384e7e930ab44862aded8.png" width="1080" height="1078" class="img_ev3q"></p><p>Wang Zichao, Oppo Senior Backend Engineer</p><p>Time: 15:50‚Äì16:30</p><p>Topic: The technological innovation of oppo intelligent recommendation sample center based on Apache SeaTunnel</p><p>Speech outline: Introduce the evolution of oppo‚Äôs intelligent recommendation machine learning sample dealing process and the role of Apache SeaTunnel in it.</p><p>In addition to the wonderful speeches, a number of lucky draw sessions were also set up on the meetup. Anyone participating in the lucky draw will have the opportunity to win exquisite customized gifts from Apache SeaTunnel, so stay tuned~</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-seatunnel"><strong>About SeaTunnel</strong><a href="#about-seatunnel" class="hash-link" aria-label="Direct link to about-seatunnel" title="Direct link to about-seatunnel">‚Äã</a></h2><p>SeaTunnel (formerly Waterdrop) is an easy-to-use, ultra-high-performance distributed data integration platform that supports real-time synchronization of massive amounts of data and can synchronize hundreds of billions of data per day in a stable and efficient manner.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-do-we-need-seatunnel"><strong>Why do we need SeaTunnel?</strong><a href="#why-do-we-need-seatunnel" class="hash-link" aria-label="Direct link to why-do-we-need-seatunnel" title="Direct link to why-do-we-need-seatunnel">‚Äã</a></h2><p>SeaTunnel does everything it can to solve the problems you may encounter in synchronizing massive amounts of data.</p><ul><li>Data loss and duplication</li><li>Task buildup and latency</li><li>Low throughput</li><li>Long application-to-production cycle time</li><li>Lack of application status monitoring</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-usage-scenarios"><strong>SeaTunnel Usage Scenarios</strong><a href="#seatunnel-usage-scenarios" class="hash-link" aria-label="Direct link to seatunnel-usage-scenarios" title="Direct link to seatunnel-usage-scenarios">‚Äã</a></h2><ul><li>Massive data synchronization</li><li>Massive data integration</li><li>ETL of large volumes of data</li><li>Massive data aggregation</li><li>Multi-source data processing</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="features-of-seatunnel"><strong>Features of SeaTunnel</strong><a href="#features-of-seatunnel" class="hash-link" aria-label="Direct link to features-of-seatunnel" title="Direct link to features-of-seatunnel">‚Äã</a></h2><ul><li>Rich components</li><li>High scalability</li><li>Easy to use</li><li>Mature and stable</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-started-with-seatunnel-quickly"><strong>How to get started with SeaTunnel quickly?</strong><a href="#how-to-get-started-with-seatunnel-quickly" class="hash-link" aria-label="Direct link to how-to-get-started-with-seatunnel-quickly" title="Direct link to how-to-get-started-with-seatunnel-quickly">‚Äã</a></h2><p>Want to experience SeaTunnel quickly? SeaTunnel 2.1.0 takes 10 seconds to get you up and running.</p><p><a href="https://seatunnel.apache.org/docs/2.1.0/developement/setup" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.1.0/developement/setup</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-i-contribute"><strong>How can I contribute?</strong><a href="#how-can-i-contribute" class="hash-link" aria-label="Direct link to how-can-i-contribute" title="Direct link to how-can-i-contribute">‚Äã</a></h2><p>We invite all partners who are interested in making local open-source global to join the SeaTunnel contributors family and foster open-source together!</p><p>Submit an issue:</p><p><a href="https://github.com/apache/incubator-seatunnel/issues" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues</a></p><p>Contribute code to:</p><p><a href="https://github.com/apache/incubator-seatunnel/pulls" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pulls</a></p><p>Subscribe to the community development mailing list :</p><p><a href="mailto:dev-subscribe@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev-subscribe@seatunnel.apache.org</a></p><p>Development Mailing List :</p><p><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></p><p>Join Slack:</p><p><a href="https://the-asf.slack.com/archives/C053HND1D6X" target="_blank" rel="noopener noreferrer">https://the-asf.slack.com/archives/C053HND1D6X</a></p><p>Follow Twitter:</p><p><a href="https://twitter.com/ASFSeaTunnel" target="_blank" rel="noopener noreferrer">https://twitter.com/ASFSeaTunnel</a></p><p>Come and join us!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/2.1.0-Released-Apache-SeaTunnel-Incubating-First-Apache-Release-Refactors-Kernel-and-Supports-Flink-Overall">2.1.0 Released! Apache SeaTunnel(Incubating) First Apache Release Refactors Kernel and Supports Flink Overall</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-03-18T00:00:00.000Z" itemprop="datePublished">March 18, 2022</time> ¬∑ <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>On December 9, 2021, Apache SeaTunnel(Incubating) entered the Apache Incubator, and after nearly four months of endeavor by the community contributors, we passed the first Apache version control in one go and released it on March 18, 2022. This means that version 2.1.0 is an official release that is safe for corporate and individual users to use, which has been voted on by the Apache SeaTunnel(Incubating) community and the Apache Incubator.</p><p><strong>Note:</strong> A¬†<strong>software license</strong>¬†is a legal instrument governing the use or redistribution of software. A typical software license grants the¬†licensee, typically an¬†end-user, permission to use one or more copies of the software in ways where such a use would otherwise potentially constitute copyright infringement of the software owner&#x27;s¬†exclusive rights¬†under copyright. Effectively, a software license is a contract between the software developer and the user that guarantees the user will not be sued within the scope of the license. </p><p>Before and after entering the incubator, we spent a lot of time sorting through the external dependencies of the entire project to ensure compliance. It is important to note that the choice of License for open source software does not necessarily mean that the project itself is compliant. While the stringent version control process of ASF ensures compliance and legal distribution of the software license maximumly.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="release-note">Release Note<a href="#release-note" class="hash-link" aria-label="Direct link to Release Note" title="Direct link to Release Note">‚Äã</a></h2><p>We bring the following <strong>key features</strong>to this release:</p><ol><li>The kernel of the microkernel plug-in architecture is overall optimized, which is mainly in Java. And a lot of improvements are made to command line parameter parsing, plug-in loading, etc. At the same time, the users (or contributors) can choose the language to develop plug-in extensions, which greatly reduces the development threshold of plug-ins.</li><li>Overall support for Flink, while the users are free to choose the underlying engine. This version also brings a large number of Flink plug-ins and welcomes anyone to contribute more.</li><li>Provide local development fast startup environment support (example), allow contributors or users quickly and smoothly start without changing any code to facilitate rapid local development debugging. This is certainly exciting news for contributors or users who need to customize their plugins. In fact, we&#x27;ve had a large number of contributors use this approach to quickly test the plugin in our pre-release testing.</li><li>With Docker container installation provided, users can deploy and install Apache SeaTunnel(Incubating) via Docker extremely fast, and we will iterate around Docker &amp; K8s in the future, any interesting proposal on this is welcomed.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="specific-release-notes">Specific release notesÔºö<a href="#specific-release-notes" class="hash-link" aria-label="Direct link to Specific release notesÔºö" title="Direct link to Specific release notesÔºö">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="features">[Features]<a href="#features" class="hash-link" aria-label="Direct link to [Features]" title="Direct link to [Features]">‚Äã</a></h3><ul><li>Use JCommander to do command line parameter parsing, making developers focus on the logic itself.</li><li>Flink is upgraded from 1.9 to 1.13.5, keeping compatibility with older versions and preparing for subsequent CDC.</li><li>Support for Doris, Hudi, Phoenix, Druid, and other Connector plugins, and you can find complete plugin support here <a href="/blog/page/[https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel%5D(https://github.com/apache/incubator-seatunnel#plugins-supported-by-seatunnel)">plugins-supported-by-seatunnel</a>.</li><li>Local development extremely fast starts environment support. It can be achieved by using the example module without modifying any code, which is convenient for local debugging.</li><li>Support for installing and trying out Apache SeaTunnel(Incubating) via Docker containers.</li><li>SQL component supports SET statements and configuration variables.</li><li>Config module refactoring to facilitate understanding for the contributors while ensuring code compliance (License) of the project.</li><li>Project structure realigned to fit the new Roadmap.</li><li>CI&amp;CD support, code quality automation control (more plans will be carried out to support CI&amp;CD development).</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="acknowledgments">Acknowledgments<a href="#acknowledgments" class="hash-link" aria-label="Direct link to Acknowledgments" title="Direct link to Acknowledgments">‚Äã</a></h2><p>Thanks to the following contributors who participated in this version release (GitHub IDs, in no particular order).</p><p>Al-assad, BenJFan, CalvinKirs, JNSimba, JiangTChen, Rianico, TyrantLucifer, Yves-yuan, ZhangchengHu0923, agendazhang, an-shi-chi-fan, asdf2014, bigdataf, chaozwn, choucmei, dailidong, dongzl, felix-thinkingdata, fengyuceNv, garyelephant, kalencaya, kezhenxu94, legendtkl, leo65535, liujinhui1994, mans2singh, marklightning, mosence, nielifeng, ououtt, ruanwenjun, simon824, totalo, wntp, wolfboys, wuchunfu, xbkaishui, xtr1993, yx91490, zhangbutao, zhaomin1423, zhongjiajie, zhuangchong, zixi0825.</p><p>Also sincere gratitude to our Mentors: Zhenxu Ke, Willem Jiang, William Guo, LiDong Dai, Ted Liu, Kevin, JB for their help!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="planning-for-the-next-few-releases">Planning for the next few releases:<a href="#planning-for-the-next-few-releases" class="hash-link" aria-label="Direct link to Planning for the next few releases:" title="Direct link to Planning for the next few releases:">‚Äã</a></h2><ul><li>CDC support.</li><li>Support for the monitoring system.</li><li>UI system support.</li><li>More Connector and efficient Sink support, such as ClickHouse support will be available in the next release soon.
The follow-up <strong>Features</strong> are decided by the community consensus, and we sincerely appeal to more participation in the community construction.</li></ul><p>We need your attention and contributions:)</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="community-status">Community Status<a href="#community-status" class="hash-link" aria-label="Direct link to Community Status" title="Direct link to Community Status">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="recent-development">Recent Development<a href="#recent-development" class="hash-link" aria-label="Direct link to Recent Development" title="Direct link to Recent Development">‚Äã</a></h3><p>Since entering the Apache incubator, the contributor group has grown from 13 to 55 and continues to grow, with the average weekly community commits remaining at 20+. </p><p>Three contributors from different companies (Lei Xie, HuaJie Wang, Chunfu Wu) have been invited to become Committers on account of their contributions to the community. </p><p>We held two Meetups, where instructors from Bilibili, OPPO, Vipshop, and other companies shared their large-scale production practices based on SeaTunnel in their companies (we will hold one meetup monthly in the future, and welcome SeaTunnel users or contributors to come and share their stories about SeaTunnel).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="users-of-apache-seatunnelincubating">Users of Apache SeaTunnel(Incubating)<a href="#users-of-apache-seatunnelincubating" class="hash-link" aria-label="Direct link to Users of Apache SeaTunnel(Incubating)" title="Direct link to Users of Apache SeaTunnel(Incubating)">‚Äã</a></h3><p>Note: Only registered users are included.</p><p>Registered users of Apache SeaTunnel(Incubating) are shown below. If you are also using Apache SeaTunnel(Incubating), too, welcome to register on <a href="https://github.com/apache/incubator-seatunnel/issues/686" target="_blank" rel="noopener noreferrer">Who is using SeaTunne</a>!</p><div align="center"><img loading="lazy" src="/image/20220321/1.png" class="img_ev3q"></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ppmcs-word">PPMC&#x27;s Word<a href="#ppmcs-word" class="hash-link" aria-label="Direct link to PPMC&#x27;s Word" title="Direct link to PPMC&#x27;s Word">‚Äã</a></h2><p>LiFeng Nie, PPMC of Apache SeaTunnel(Incubating), commented on the first Apache version release. </p><p>From the first day entering Apache Incubating, we have been working hard to learn the Apache Way and various Apache policies. Although the first release took a lot of time (mainly for compliance), we think it was well worth it, and that&#x27;s one of the reasons we chose to enter Apache. We need to give our users peace of mind, and Apache is certainly the best choice, with its almost demanding license control that allows users to avoid compliance issues as much as possible and ensure that the software is circulating reasonably and legally. In addition, its practice of the Apache Way, such as public service mission, pragmatism, community over code, openness and consensus decision-making, and meritocracy, can drive the Apache SeaTunnel(Incubating) community to become more open, transparent, and diverse.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/2-1-0">2.1.0</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/release">Release</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="ClickHouse is a distributed columnar DBMS for OLAP. Our department has now stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/hdfs-to-clickhouse">How to quickly import data from HDFS into ClickHouse</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>ClickHouse is a distributed columnar DBMS for OLAP. Our department has now stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.</p><p>The experience of data processing and storage introduced earlier is based on real-time data streams. The data is stored in Kafka. We use Java or Golang to read, parse, and clean the data from Kafka and write it into ClickHouse, so that the data can be stored in ClickHouse. Quick access. However, in the usage scenarios of many students, the data is not real-time, and it may be necessary to import the data in HDFS or Hive into ClickHouse. Some students implement data import by writing Spark programs, so is there a simpler and more efficient way?</p><p>At present, there is a tool <strong>Seatunnel</strong> in the open source community, the project address <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a>, can quickly Data in HDFS is imported into ClickHouse.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hdfs-to-clickhouse">HDFS To ClickHouse<a href="#hdfs-to-clickhouse" class="hash-link" aria-label="Direct link to HDFS To ClickHouse" title="Direct link to HDFS To ClickHouse">‚Äã</a></h2><p>Assuming that our logs are stored in HDFS, we need to parse the logs and filter out the fields we care about, and write the corresponding fields into the ClickHouse table.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="log-sample">Log Sample<a href="#log-sample" class="hash-link" aria-label="Direct link to Log Sample" title="Direct link to Log Sample">‚Äã</a></h3><p>The log format we store in HDFS is as follows, which is a very common Nginx log</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token number">10.41</span><span class="token plain">.1.28 github.com </span><span class="token number">114.250</span><span class="token plain">.140.241 </span><span class="token number">0</span><span class="token plain">.001s </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;127.0.0.1:80&quot;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token number">26</span><span class="token plain">/Oct/2018:03:09:32 +0800</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;GET /Apache/Seatunnel HTTP/1.1&quot;</span><span class="token plain"> </span><span class="token number">200</span><span class="token plain"> </span><span class="token number">0</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;-&quot;</span><span class="token plain"> - </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;196&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;-&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;mainpage&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;443&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;-&quot;</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;172.16.181.129&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse-schema">ClickHouse Schema<a href="#clickhouse-schema" class="hash-link" aria-label="Direct link to ClickHouse Schema" title="Direct link to ClickHouse Schema">‚Äã</a></h3><p>Our ClickHouse table creation statement is as follows, our table is partitioned by day</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE cms.cms_msg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> Date, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime DateTime, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time Float32, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">hostname</span><span class="token plain"> String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr String, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_size Int32, </span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    pool String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">)</span><span class="token plain"> ENGINE </span><span class="token operator">=</span><span class="token plain"> MergeTree PARTITION BY </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> ORDER BY </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> SETTINGS index_granularity </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">16384</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-with-clickhouse">Seatunnel with ClickHouse<a href="#seatunnel-with-clickhouse" class="hash-link" aria-label="Direct link to Seatunnel with ClickHouse" title="Direct link to Seatunnel with ClickHouse">‚Äã</a></h2><p>Next, I will introduce to you in detail how we can meet the above requirements through Seatunnel and write the data in HDFS into ClickHouse.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Direct link to Seatunnel" title="Direct link to Seatunnel">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plugins that support reading data from Kafka, HDFS, Kudu, performing various data processing, and writing the results to ClickHouse, Elasticsearch or Kafka.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">‚Äã</a></h3><p>First we need to install Seatunnel, the installation is very simple, no need to configure system environment variables</p><ol><li>Prepare the Spark environment</li><li>Install Seatunnel</li><li>Configure Seatunnel</li></ol><p>The following are simple steps, the specific installation can refer to <a href="/docs/quick-start">Quick Start</a></p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">wget</span><span class="token plain"> https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">tar</span><span class="token plain"> -xvf https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">wget</span><span class="token plain"> https://github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel-1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">unzip</span><span class="token plain"> seatunnel-1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token builtin class-name" style="color:rgb(189, 147, 249)">cd</span><span class="token plain"> seatunnel-1.1.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token function" style="color:rgb(80, 250, 123)">vim</span><span class="token plain"> config/seatunnel-env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify the Spark installation path</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token assign-left variable" style="color:rgb(189, 147, 249);font-style:italic">SPARK_HOME</span><span class="token operator">=</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">${SPARK_HOME</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">:-</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">usr</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">local</span><span class="token variable operator" style="color:rgb(189, 147, 249);font-style:italic">/</span><span class="token variable" style="color:rgb(189, 147, 249);font-style:italic">spark-2.2.0-bin-hadoop2.7}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="Direct link to seatunnel Pipeline" title="Direct link to seatunnel Pipeline">‚Äã</a></h3><p>We only need to write a configuration file of seatunnel Pipeline to complete the data import.</p><p>The configuration file consists of four parts, Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Direct link to Spark" title="Direct link to Spark">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the size of the resources required for Spark to execute.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;1g&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Direct link to Input" title="Direct link to Input">‚Äã</a></h4><p>This part defines the data source. The following is a configuration example for reading data in text format from HDFS files.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hdfs </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        path </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hdfs://nomanode:8020/rowlog/accesslog&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">format</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Direct link to Filter" title="Direct link to Filter">‚Äã</a></h4><p>In the Filter section, here we configure a series of transformations, including regular parsing to split the log, time transformation to convert HTTPDATE to the date format supported by ClickHouse, type conversion to Number type fields, and field filtering through SQL, etc.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parse raw logs using regular expressions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;raw_message&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;%{IP:ha_ip}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\&quot;%{DATA:upstream_ip}\&quot;\\s\\[%{HTTPDATE:timestamp}\\]\\s\&quot;%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\&quot;\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\&quot;%{DATA:user_agent}\&quot;\\s%{DATA:uid}\\s%{DATA:session_id}\\s\&quot;%{DATA:pool}\&quot;\\s\&quot;%{DATA:tag2}\&quot;\\s%{DATA:tag3}\\s%{DATA:tag4}&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Data in &quot;yyyy/MM/dd HH:mm:ss&quot; format</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;timestamp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;yyyy/MM/dd HH:mm:ss&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use SQL to filter the fields of interest and process the fields</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># You can even filter out data you don&#x27;t care about by filter conditions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Direct link to Output" title="Direct link to Output">‚Äã</a></h4><p>Finally, we write the processed structured data to ClickHouse</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">host</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;your.clickhouse.host:8123&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;date&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hostname&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;uri&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;http_code&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;request_time&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;data_size&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;domain&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;username&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;password&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-seatunnel">Running seatunnel<a href="#running-seatunnel" class="hash-link" aria-label="Direct link to Running seatunnel" title="Direct link to Running seatunnel">‚Äã</a></h3><p>We combine the above four-part configuration into our configuration file <code>config/batch.conf</code>.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token function" style="color:rgb(80, 250, 123)">vim</span><span class="token plain"> config/batch.conf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores </span><span class="token operator">=</span><span class="token plain"> </span><span class="token number">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;1g&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hdfs </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        path </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hdfs://nomanode:8020/rowlog/accesslog&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">format</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;text&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parse raw logs using regular expressions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;raw_message&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;%{IP:ha_ip}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\&quot;%{DATA:upstream_ip}\&quot;\\s\\[%{HTTPDATE:timestamp}\\]\\s\&quot;%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\&quot;\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\&quot;%{DATA:user_agent}\&quot;\\s%{DATA:uid}\\s%{DATA:session_id}\\s\&quot;%{DATA:pool}\&quot;\\s\&quot;%{DATA:tag2}\&quot;\\s%{DATA:tag3}\\s%{DATA:tag4}&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Data in &quot;yyyy/MM/dd HH:mm:ss&quot; format</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token function" style="color:rgb(80, 250, 123)">date</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;timestamp&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;dd/MMM/yyyy:HH:mm:ss Z&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;yyyy/MM/dd HH:mm:ss&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Use SQL to filter the fields of interest and process the fields</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># You can even filter out data you don&#x27;t care about by filter conditions</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;select substring(date, 1, 10) as date, datetime, hostname, url, http_code, float(request_time), int(data_size), domain from access&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        </span><span class="token function" style="color:rgb(80, 250, 123)">host</span><span class="token plain"> </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;your.clickhouse.host:8123&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;seatunnel&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;access_log&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields </span><span class="token operator">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;date&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;datetime&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;hostname&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;uri&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;http_code&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;request_time&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;data_size&quot;</span><span class="token plain">, </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;domain&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;username&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password </span><span class="token operator">=</span><span class="token plain"> </span><span class="token string" style="color:rgb(255, 121, 198)">&quot;password&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to write data to ClickHouse. Here we take the local mode as an example.</p><div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m </span><span class="token string" style="color:rgb(255, 121, 198)">&#x27;local[2]&#x27;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2><p>In this post, we covered how to import Nginx log files from HDFS into ClickHouse using Seatunnel. Data can be imported quickly with only one configuration file without writing any code. In addition to supporting HDFS data sources, Seatunnel also supports real-time reading and processing of data from Kafka to ClickHouse. Our next article will describe how to quickly import data from Hive into ClickHouse.</p><p>Of course, Seatunnel is not only a tool for ClickHouse data writing, but also plays a very important role in the writing of data sources such as Elasticsearch and Kafka.</p><p>If you want to know more functions and cases of Seatunnel combined with ClickHouse, Elasticsearch and Kafka, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hdfs">HDFS</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/click-house">ClickHouse</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="ClickHouse is a distributed columnar DBMS for OLAP. Our department has stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/hive-to-clickhouse">How to quickly import data from Hive into ClickHouse</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->5 min read</div></header><div class="markdown" itemprop="articleBody"><p>ClickHouse is a distributed columnar DBMS for OLAP. Our department has stored all log data related to data analysis in ClickHouse, an excellent data warehouse, and the current daily data volume has reached 30 billion.</p><p>In the previous article <!-- -->[How to quickly import data from HDFS into ClickHouse]<!-- --> (2021-12-30-hdfs-to-clickhouse.md), we mentioned the use of Seatunnel <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator -seatunnel</a> After a very simple operation on the data in HDFS, the data can be written to ClickHouse. The data in HDFS is generally unstructured data, so what should we do with the structured data stored in Hive?</p><p><img loading="lazy" src="/assets/images/hive-logo-c9aedd90b5ea9668c87fe25ad92a8e6c.png" width="962" height="518" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="hive-to-clickhouse">Hive to ClickHouse<a href="#hive-to-clickhouse" class="hash-link" aria-label="Direct link to Hive to ClickHouse" title="Direct link to Hive to ClickHouse">‚Äã</a></h2><p>Assuming that our data has been stored in Hive, we need to read the data in the Hive table and filter out the fields we care about, or convert the fields, and finally write the corresponding fields into the ClickHouse table.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="hive-schema">Hive Schema<a href="#hive-schema" class="hash-link" aria-label="Direct link to Hive Schema" title="Direct link to Hive Schema">‚Äã</a></h3><p>The structure of the data table we store in Hive is as follows, which stores common Nginx logs.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE `nginx_msg_detail`(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `hostname` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `domain` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `remote_addr` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `request_time` float,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `datetime` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `url` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `status` int,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `data_size` int,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `referer` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `cookie_info` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `user_agent` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `minute` string)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> PARTITIONED BY (</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `date` string,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   `hour` string)</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse-schema">ClickHouse Schema<a href="#clickhouse-schema" class="hash-link" aria-label="Direct link to ClickHouse Schema" title="Direct link to ClickHouse Schema">‚Äã</a></h3><p>Our ClickHouse table creation statement is as follows, our table is partitioned by day</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">CREATE TABLE cms.cms_msg</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">(</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date Date,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    datetime DateTime,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    url String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    request_time Float32,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    status String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hostname String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    domain String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remote_addr String,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    data_size Int32</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">) ENGINE = MergeTree PARTITION BY date ORDER BY (date, hostname) SETTINGS index_granularity = 16384</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-with-clickhouse">Seatunnel with ClickHouse<a href="#seatunnel-with-clickhouse" class="hash-link" aria-label="Direct link to Seatunnel with ClickHouse" title="Direct link to Seatunnel with ClickHouse">‚Äã</a></h2><p>Next, I will introduce to you how we write data from Hive to ClickHouse through Seatunnel.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Direct link to Seatunnel" title="Direct link to Seatunnel">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> is a very easy-to-use, high-performance, real-time data processing product that can deal with massive data. It is built on Spark. Seatunnel has a very rich set of plug-ins that support reading data from Kafka, HDFS, and Kudu, performing various data processing, and writing the results to ClickHouse, Elasticsearch or Kafka.</p><p>The environment preparation and installation steps of Seatunnel will not be repeated here. For specific installation steps, please refer to the previous article or visit <a href="/docs/intro/about">Seatunnel Docs</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">Seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="Direct link to Seatunnel Pipeline" title="Direct link to Seatunnel Pipeline">‚Äã</a></h3><p>We only need to write a configuration file of Seatunnel Pipeline to complete the data import.</p><p>The configuration file includes four parts, namely Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Direct link to Spark" title="Direct link to Spark">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  // This configuration is required</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.catalogImplementation = &quot;hive&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Direct link to Input" title="Direct link to Input">‚Äã</a></h4><p>This part defines the data source. The following is a configuration example of reading data in text format from a Hive file.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hive {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = &quot;select * from access.nginx_msg_detail&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>See, a very simple configuration can read data from Hive. <code>pre_sql</code> is the SQL to read data from Hive, and <code>table_name</code> is the name of the table that will register the read data as a temporary table in Spark, which can be any field.</p><p>It should be noted that it must be ensured that the metastore of hive is in the service state.</p><p>When running in Cluster, Client, Local mode, the <code>hive-site.xml</code> file must be placed in the $HADOOP_CONF directory of the submit task node</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Direct link to Filter" title="Direct link to Filter">‚Äã</a></h4><p>In the Filter section, here we configure a series of transformations, and here we discard the unnecessary minute and hour fields. Of course, we can also not read these fields through <code>pre_sql</code> when reading Hive</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remove {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = [&quot;minute&quot;, &quot;hour&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Direct link to Output" title="Direct link to Output">‚Äã</a></h4><p>Finally, we write the processed structured data to ClickHouse</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        host = &quot;your.clickhouse.host:8123&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = &quot;nginx_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields = [&quot;date&quot;, &quot;datetime&quot;, &quot;hostname&quot;, &quot;url&quot;, &quot;http_code&quot;, &quot;request_time&quot;, &quot;data_size&quot;, &quot;domain&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username = &quot;username&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = &quot;password&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-seatunnel">Running Seatunnel<a href="#running-seatunnel" class="hash-link" aria-label="Direct link to Running Seatunnel" title="Direct link to Running Seatunnel">‚Äã</a></h3><p>We combine the above four-part configuration into our configuration file <code>config/batch.conf</code>.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/batch.conf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  // This configuration is required</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.sql.catalogImplementation = &quot;hive&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    hive {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pre_sql = &quot;select * from access.nginx_msg_detail&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    remove {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = [&quot;minute&quot;, &quot;hour&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    clickhouse {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        host = &quot;your.clickhouse.host:8123&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        database = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        fields = [&quot;date&quot;, &quot;datetime&quot;, &quot;hostname&quot;, &quot;uri&quot;, &quot;http_code&quot;, &quot;request_time&quot;, &quot;data_size&quot;, &quot;domain&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        username = &quot;username&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        password = &quot;password&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to write data to ClickHouse. Here we take the local mode as an example.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m &#x27;local[2]&#x27;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2><p>In this post, we covered how to import data from Hive into ClickHouse using Seatunnel. The data import can be completed quickly through only one configuration file without writing any code, which is very simple.</p><p>If you want to know more functions and cases of Seatunnel combined with ClickHouse, Elasticsearch, Kafka, Hadoop, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p>-- Power by <a href="https://github.com/InterestingLab" target="_blank" rel="noopener noreferrer">InterestingLab</a></p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/hive">Hive</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/click-house">ClickHouse</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="When it comes to writing data to Elasticsearch, the first thing that comes to mind must be Logstash. Logstash is accepted by the majority of users because of its simplicity, scalability, and scalability. However, the ruler is shorter and the inch is longer, and Logstash must have application scenarios that it cannot apply to, such as:"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/spark-execute-elasticsearch">How to quickly write data to Elasticsearch using Spark</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2021-12-30T00:00:00.000Z" itemprop="datePublished">December 30, 2021</time> ¬∑ <!-- -->6 min read</div></header><div class="markdown" itemprop="articleBody"><p>When it comes to writing data to Elasticsearch, the first thing that comes to mind must be Logstash. Logstash is accepted by the majority of users because of its simplicity, scalability, and scalability. However, the ruler is shorter and the inch is longer, and Logstash must have application scenarios that it cannot apply to, such as:</p><ul><li>Massive data ETL</li><li>Massive data aggregation</li><li>Multi-source data processing</li></ul><p>In order to meet these scenarios, many students will choose Spark, use Spark operators to process data, and finally write the processing results to Elasticsearch.</p><p>Our department used Spark to analyze Nginx logs, counted our web service access, aggregated Nginx logs every minute and finally wrote the results to Elasticsearch, and then used Kibana to configure real-time monitoring of the Dashboard. Both Elasticsearch and Kibana are convenient and practical, but with more and more similar requirements, how to quickly write data to Elasticsearch through Spark has become a big problem for us.</p><p>Today, I would like to recommend a black technology Seatunnel <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> that can realize fast data writing. It is very easy to use , a high-performance, real-time data processing product that can deal with massive data. It is built on Spark and is easy to use, flexibly configured, and requires no development.</p><p><img loading="lazy" src="/assets/images/wd-struct-fd963482dc80fdee6e4930107709bd28.png" width="818" height="466" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="kafka-to-elasticsearch">Kafka to Elasticsearch<a href="#kafka-to-elasticsearch" class="hash-link" aria-label="Direct link to Kafka to Elasticsearch" title="Direct link to Kafka to Elasticsearch">‚Äã</a></h2><p>Like Logstash, Seatunnel also supports multiple types of data input. Here we take the most common Kakfa as the input source as an example to explain how to use Seatunnel to quickly write data to Elasticsearch</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="log-sample">Log Sample<a href="#log-sample" class="hash-link" aria-label="Direct link to Log Sample" title="Direct link to Log Sample">‚Äã</a></h3><p>The original log format is as follows:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">127.0.0.1 elasticsearch.cn 114.250.140.241 0.001s &quot;127.0.0.1:80&quot; [26/Oct/2018:21:54:32 +0800] &quot;GET /article HTTP/1.1&quot; 200 123 &quot;-&quot; - &quot;Dalvik/2.1.0 (Linux; U; Android 7.1.1; OPPO R11 Build/NMF26X)&quot;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="elasticsearch-document">Elasticsearch Document<a href="#elasticsearch-document" class="hash-link" aria-label="Direct link to Elasticsearch Document" title="Direct link to Elasticsearch Document">‚Äã</a></h3><p>We want to count the visits of each domain name in one minute. The aggregated data has the following fields:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">domain String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">hostname String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">status int</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">datetime String</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">count int</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-with-elasticsearch">Seatunnel with Elasticsearch<a href="#seatunnel-with-elasticsearch" class="hash-link" aria-label="Direct link to Seatunnel with Elasticsearch" title="Direct link to Seatunnel with Elasticsearch">‚Äã</a></h2><p>Next, I will introduce you in detail, how we read the data in Kafka through Seatunnel, parse and aggregate the data, and finally write the processing results into Elasticsearch.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel">Seatunnel<a href="#seatunnel" class="hash-link" aria-label="Direct link to Seatunnel" title="Direct link to Seatunnel">‚Äã</a></h3><p><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">Seatunnel</a> also has a very rich plug-in that supports reading data from Kafka, HDFS, Hive, performing various data processing, and converting the results Write to Elasticsearch, Kudu or Kafka.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">‚Äã</a></h3><p>First of all, we need to install seatunnel, the installation is very simple, no need to configure system environment variables</p><ol><li>Prepare the Spark environment</li><li>Install Seatunnel</li><li>Configure Seatunnel</li></ol><p>The following are simple steps, the specific installation can refer to <a href="/docs/quick-start">Quick Start</a></p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd /usr/local</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//archive.apache.org/dist/spark/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">tar </span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">xvf https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//archive.apache.org/dist/spark/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7.tgz</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">wget https</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">//github.com/InterestingLab/seatunnel/releases/download/v1.1.1/seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">unzip seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1.zip</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">cd seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">1.1.1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">env.sh</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token comment" style="color:rgb(98, 114, 164)"># Specify the Spark installation path</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">SPARK_HOME=$</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">SPARK_HOME</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">/usr/local/spark</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">2.2.0</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">bin</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">hadoop2.7</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-pipeline">Seatunnel Pipeline<a href="#seatunnel-pipeline" class="hash-link" aria-label="Direct link to Seatunnel Pipeline" title="Direct link to Seatunnel Pipeline">‚Äã</a></h3><p>Like Logstash, we only need to write a configuration file of Seatunnel Pipeline to complete the data import. I believe that friends who know Logstash can start Seatunnel configuration soon.</p><p>The configuration file includes four parts, namely Spark, Input, filter and Output.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="spark">Spark<a href="#spark" class="hash-link" aria-label="Direct link to Spark" title="Direct link to Spark">‚Äã</a></h4><p>This part is the related configuration of Spark, which mainly configures the resource size required for Spark execution.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.streaming.batchDuration = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="input">Input<a href="#input" class="hash-link" aria-label="Direct link to Input" title="Direct link to Input">‚Äã</a></h4><p>This part defines the data source. The following is a configuration example of reading data from Kafka,</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    topics = &quot;seatunnel-es&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.group.id = &quot;seatunnel_es_group&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    consumer.rebalance.max.retries = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="filter">Filter<a href="#filter" class="hash-link" aria-label="Direct link to Filter" title="Direct link to Filter">‚Äã</a></h4><p>In the Filter section, here we configure a series of conversions, including regular parsing to split logs, time conversion to convert HTTPDATE to a date format supported by Elasticsearch, type conversion for fields of type Number, and data aggregation through SQL</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Parse the original log using regex</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># The initial data is in the raw_message field</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;raw_message&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern = &#x27;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">IP</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">remote_addr</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">request_time</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">s\\s\&quot;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">upstream_ip</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\&quot;\\s\\</span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token plain">%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">HTTPDATE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">timestamp</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain">\\s\&quot;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">method</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">url</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">http_ver</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\&quot;\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NUMBER</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">body_bytes_send</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">referer</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">NOTSPACE</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">cookie_info</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">\\s\&quot;%</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">DATA</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">user_agent</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)"># format supported in Elasticsearch</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;timestamp&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field = &quot;datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format = &quot;dd/MMM/yyyy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">HH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">mm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">ss Z&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format = &quot;yyyy</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">MM</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">dd&#x27;T&#x27;HH</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">mm</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">ss.SSS+08</span><span class="token punctuation" style="color:rgb(248, 248, 242)">:</span><span class="token plain">00&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token comment" style="color:rgb(98, 114, 164)">## Aggregate data with SQL</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> int(status)</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> datetime</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> count(</span><span class="token important">*)</span><span class="token plain"> from access_log group by domain</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> hostname</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> status</span><span class="token punctuation" style="color:rgb(248, 248, 242)">,</span><span class="token plain"> datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h4 class="anchor anchorWithStickyNavbar_LWe7" id="output">Output<a href="#output" class="hash-link" aria-label="Direct link to Output" title="Direct link to Output">‚Äã</a></h4><p>Finally, we write the processed structured data to Elasticsearch.</p><div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">output </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    elasticsearch </span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        hosts = </span><span class="token punctuation" style="color:rgb(248, 248, 242)">[</span><span class="token string" style="color:rgb(255, 121, 198)">&quot;localhost:9200&quot;</span><span class="token punctuation" style="color:rgb(248, 248, 242)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index = &quot;seatunnel</span><span class="token punctuation" style="color:rgb(248, 248, 242)">-</span><span class="token plain">$</span><span class="token punctuation" style="color:rgb(248, 248, 242)">{</span><span class="token plain">now</span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain">&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        es.batch.size.entries = 100000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index_time_format = &quot;yyyy.MM.dd&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"></span><span class="token punctuation" style="color:rgb(248, 248, 242)">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="running-seatunnel">Running Seatunnel<a href="#running-seatunnel" class="hash-link" aria-label="Direct link to Running Seatunnel" title="Direct link to Running Seatunnel">‚Äã</a></h3><p>We combine the above four-part configuration into our configuration file <code>config/batch.conf</code>.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">vim config/batch.conf</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.app.name = &quot;seatunnel&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.instances = 2</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.cores = 1</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.executor.memory = &quot;1g&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  spark.streaming.batchDuration = 5</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">input {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    kafkaStream {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        topics = &quot;seatunnel-es&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.bootstrap.servers = &quot;localhost:9092&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.group.id = &quot;seatunnel_es_group&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        consumer.rebalance.max.retries = 100</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">filter {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Parse the original log using regex</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # The initial data is in the raw_message field</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    grok {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;raw_message&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        pattern = &#x27;%{IP:hostname}\\s%{NOTSPACE:domain}\\s%{IP:remote_addr}\\s%{NUMBER:request_time}s\\s\&quot;%{DATA:upstream_ip}\&quot;\\s\\[%{HTTPDATE:timestamp}\\]\\s\&quot;%{NOTSPACE:method}\\s%{DATA:url}\\s%{NOTSPACE:http_ver}\&quot;\\s%{NUMBER:status}\\s%{NUMBER:body_bytes_send}\\s%{DATA:referer}\\s%{NOTSPACE:cookie_info}\\s\&quot;%{DATA:user_agent}&#x27;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">   }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # Convert data in &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; format to</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    # format supported in Elasticsearch</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    date {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_field = &quot;timestamp&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_field = &quot;datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        source_time_format = &quot;dd/MMM/yyyy:HH:mm:ss Z&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        target_time_format = &quot;yyyy-MM-dd&#x27;T&#x27;HH:mm:00.SSS+08:00&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ## Aggregate data with SQL</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    sql {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        table_name = &quot;access_log&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        sql = &quot;select domain, hostname, status, datetime, count(*) from access_log group by domain, hostname, status, datetime&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain"> }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">output {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    elasticsearch {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        hosts = [&quot;localhost:9200&quot;]</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index = &quot;seatunnel-${now}&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        es.batch.size.entries = 100000</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        index_time_format = &quot;yyyy.MM.dd&quot;</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Execute the command, specify the configuration file, and run Seatunnel to write data to Elasticsearch. Here we take the local mode as an example.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">./bin/start-seatunnel.sh --config config/batch.conf -e client -m &#x27;local[2]&#x27;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Finally, the data written into Elasticsearch is as follows, and with Kibana, real-time monitoring of web services can be realized ^_^.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">&quot;_source&quot;: {</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;domain&quot;: &quot;elasticsearch.cn&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;hostname&quot;: &quot;localhost&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;status&quot;: &quot;200&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;datetime&quot;: &quot;2018-11-26T21:54:00.000+08:00&quot;,</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    &quot;count&quot;: 26</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2><p>In this post, we introduced how to write data from Kafka to Elasticsearch via Seatunnel. You can quickly run a Spark Application with only one configuration file, complete data processing and writing, and do not need to write any code, which is very simple.</p><p>When there are scenarios that Logstash cannot support or the performance of Logstah cannot meet expectations during data processing, you can try to use Seatunnel to solve the problem.</p><p>If you want to know more functions and cases of using Seatunnel in combination with Elasticsearch, Kafka and Hadoop, you can go directly to the official website <a href="https://seatunnel.apache.org/" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/</a></p><p><strong>We will publish another article &quot;How to Use Spark and Elasticsearch for Interactive Data Analysis&quot; in the near future, so stay tuned.</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="contract-us">Contract us<a href="#contract-us" class="hash-link" aria-label="Direct link to Contract us" title="Direct link to Contract us">‚Äã</a></h2><ul><li>Mailing list : <strong><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></strong>. Send anything to <code>dev-subscribe@seatunnel.apache.org</code> and subscribe to the mailing list according to the replies.</li><li>Slack: Send a <code>Request to join SeaTunnel slack</code> email to the mailing list (<code>dev@seatunnel.apache.org</code>), and we will invite you to join (please make sure you are registered with Slack before doing so).</li><li><a href="https://space.bilibili.com/1542095008" target="_blank" rel="noopener noreferrer">bilibili B station video</a></li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/spark">Spark</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/kafka">Kafka</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/elasticsearch">Elasticsearch</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog"><div class="pagination-nav__label">Newer Entries</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/page/3"><div class="pagination-nav__label">Older Entries</div></a></nav></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">SeaTunnel</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/faq">FAQ</a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/releases" target="_blank" rel="noopener noreferrer" class="footer__link-item">Releases<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">Issue Tracker<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/pulls" target="_blank" rel="noopener noreferrer" class="footer__link-item">Pull Requests<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Subscribe Mailing List</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/community/contribution_guide/subscribe">How to Subscribe</a></li><li class="footer__item"><a href="mailto:dev-subscribe@seatunnel.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe Mail<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://lists.apache.org/list.html?dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mail Archive<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">
            <div style="margin-top: 20px;">
                <img style="height:50px;margin: 30px 0 10px;" alt="Apache Software Foundation" src="/image/asf_logo_wide.svg">
                <div style="border-top: 1px solid #ccc;min-height: 60px;line-height: 20px;text-align: center;font-family: Avenir-Medium;font-size: 14px;color: #999;display: flex;align-items: center;"><span>Copyright ¬© 2021-2025 The Apache Software Foundation. Apache SeaTunnel, SeaTunnel, and its feather logo are trademarks of The Apache Software Foundation.</span></div>
                <div style="text-align: center;">
                    <a href="https://twitter.com/asfseatunnel?s=21" target="_blank" title="Twitter"><svg t="1644553365083" class="icon" viewBox="0 0 1260 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="7015" width="38" height="38"><path d="M1259.846921 121.148242c-46.524504 20.728739-96.273478 34.547899-148.325646 40.536201 53.434084-31.784067 94.430924-82.454319 113.777747-142.797982-50.209613 29.480874-105.486251 51.13089-164.447999 62.646857A257.584528 257.584528 0 0 0 872.449815 0.000276c-142.797982 0-258.418284 115.620302-258.418284 258.418284 0 20.268101 2.303193 40.075563 6.909579 58.961748C405.82286 306.32498 215.579097 203.602561 87.98219 47.446058c-22.110655 38.233008-35.008538 82.454319-35.008538 129.900099 0 89.824537 45.603227 168.593747 115.159663 215.118251-42.378756-1.381916-81.99368-12.897882-117.002217-32.244706v3.224471c0 125.293713 88.90326 229.398049 207.287393 253.351259-21.650017 5.988302-44.681949 9.212773-68.17452 9.212773-16.582991 0-32.705344-1.842555-48.827697-4.606387 32.705344 102.722419 128.518184 177.345881 241.374653 179.649074-88.442621 69.095798-199.917175 110.553277-321.06514 110.553277-20.728739 0-41.457479-1.381916-61.72558-3.685109 114.238386 73.241546 250.126788 116.08094 396.149241 116.08094 475.379089 0 735.179289-393.846048 735.179289-735.179289 0-11.055328-0.460639-22.571294-0.921277-33.626621 51.13089-36.851092 94.891562-82.454319 129.439461-134.045848z" fill="#909094" p-id="7016"></path></svg></a> 
                    <a href="https://s.apache.org/seatunnel-slack" target="_blank" title="Slack" style="margin-left: 20px;"><svg t="1644553076784" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3088" width="36" height="36"><path d="M215.125333 647.04a107.861333 107.861333 0 0 1-107.52 107.648A107.861333 107.861333 0 0 1 0 647.04a107.818667 107.818667 0 0 1 107.605333-107.52h107.52v107.52z m54.229334 0a107.818667 107.818667 0 0 1 107.562666-107.52 107.818667 107.818667 0 0 1 107.562667 107.52v269.354667A107.861333 107.861333 0 0 1 376.917333 1024a107.861333 107.861333 0 0 1-107.562666-107.605333v-269.354667zM376.917333 215.125333a107.861333 107.861333 0 0 1-107.562666-107.52A107.861333 107.861333 0 0 1 376.917333 0a107.861333 107.861333 0 0 1 107.562667 107.605333v107.52H376.917333z m0 54.229334a107.861333 107.861333 0 0 1 107.562667 107.562666 107.861333 107.861333 0 0 1-107.562667 107.562667H107.605333A107.861333 107.861333 0 0 1 0 376.917333a107.861333 107.861333 0 0 1 107.605333-107.562666h269.312z m431.872 107.562666a107.861333 107.861333 0 0 1 107.605334-107.562666A107.861333 107.861333 0 0 1 1024 376.917333a107.861333 107.861333 0 0 1-107.605333 107.562667h-107.605334V376.917333z m-54.101333 0a107.861333 107.861333 0 0 1-107.648 107.562667 107.818667 107.818667 0 0 1-107.52-107.562667V107.605333A107.818667 107.818667 0 0 1 647.04 0a107.861333 107.861333 0 0 1 107.648 107.605333v269.312z m-107.648 431.872a107.861333 107.861333 0 0 1 107.648 107.605334A107.861333 107.861333 0 0 1 647.04 1024a107.818667 107.818667 0 0 1-107.52-107.605333v-107.605334h107.52z m0-54.101333a107.818667 107.818667 0 0 1-107.52-107.648 107.776 107.776 0 0 1 107.52-107.52h269.354667A107.818667 107.818667 0 0 1 1024 647.04a107.861333 107.861333 0 0 1-107.605333 107.648h-269.354667z" p-id="3089" fill="#909094"></path></svg></a> 
                    <a href="https://lists.apache.org/list.html?dev@seatunnel.apache.org" target="_blank" title="Mailing list" style="margin-left: 20px;"><svg t="1644553175467" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5304" width="39" height="39"><path d="M853.333333 170.666667H170.666667c-46.933333 0-85.333333 38.4-85.333334 85.333333v512c0 46.933333 38.4 85.333333 85.333334 85.333333h682.666666c46.933333 0 85.333333-38.4 85.333334-85.333333V256c0-46.933333-38.4-85.333333-85.333334-85.333333z m0 170.666666l-341.333333 213.333334-341.333333-213.333334V256l341.333333 213.333333 341.333333-213.333333v85.333333z" p-id="5305" fill="#909094"></path></svg></a> 
                    <a href="https://github.com/apache/seatunnel" target="_blank" title="GitHub" style="margin-left: 20px;"><svg t="1644553223000" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6156" width="36" height="36"><path d="M512 12.64c-282.752 0-512 229.216-512 512 0 226.208 146.72 418.144 350.144 485.824 25.6 4.736 35.008-11.104 35.008-24.64 0-12.192-0.48-52.544-0.704-95.328-142.464 30.976-172.512-60.416-172.512-60.416-23.296-59.168-56.832-74.912-56.832-74.912-46.464-31.776 3.52-31.136 3.52-31.136 51.392 3.616 78.464 52.768 78.464 52.768 45.664 78.272 119.776 55.648 148.992 42.56 4.576-33.088 17.856-55.68 32.512-68.48-113.728-12.928-233.28-56.864-233.28-253.024 0-55.904 20-101.568 52.768-137.44-5.312-12.896-22.848-64.96 4.96-135.488 0 0 43.008-13.76 140.832 52.48a491.296 491.296 0 0 1 128.16-17.248c43.488 0.192 87.328 5.888 128.256 17.248 97.728-66.24 140.64-52.48 140.64-52.48 27.872 70.528 10.336 122.592 5.024 135.488 32.832 35.84 52.704 81.536 52.704 137.44 0 196.64-119.776 239.936-233.792 252.64 18.368 15.904 34.72 47.04 34.72 94.816 0 68.512-0.608 123.648-0.608 140.512 0 13.632 9.216 29.6 35.168 24.576C877.472 942.624 1024 750.784 1024 524.64c0-282.784-229.248-512-512-512z" p-id="6157" fill="#909094"></path></svg></a> 
                </div>
            <div></div></div></div></div></div></footer></div>
<script src="/assets/js/runtime~main.4be0285b.js"></script>
<script src="/assets/js/main.33ff0e4e.js"></script>
</body>
</html>