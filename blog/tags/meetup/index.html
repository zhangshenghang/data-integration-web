<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.3">
<title data-rh="true">9 posts tagged with &quot;Meetup&quot; | Apache SeaTunnel</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://seatunnel.apache.org/blog/tags/meetup"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="9 posts tagged with &quot;Meetup&quot; | Apache SeaTunnel"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/image/favicon.ico"><link data-rh="true" rel="canonical" href="https://seatunnel.apache.org/blog/tags/meetup"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/blog/tags/meetup" hreflang="en"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/zh-CN/blog/tags/meetup" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://seatunnel.apache.org/blog/tags/meetup" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://S2J1A7LWND-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache SeaTunnel RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache SeaTunnel Atom Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Apache SeaTunnel" href="/opensearch.xml">



<link rel="alternate" type="application/rss+xml" href="/user_cases/rss.xml" title="Apache SeaTunnel RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/user_cases/atom.xml" title="Apache SeaTunnel Atom Feed"><link rel="stylesheet" href="/assets/css/styles.72e1e0e2.css">
<link rel="preload" href="/assets/js/runtime~main.85c0a3ab.js" as="script">
<link rel="preload" href="/assets/js/main.3d77c3f1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();null!==e?t(e):window.matchMedia("(prefers-color-scheme: dark)").matches?t("dark"):(window.matchMedia("(prefers-color-scheme: light)").matches,t("light"))}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:rgb(70, 125, 175, 0.8)" role="banner"><div class="content_knG7 announcementBarContent_xLdY">ðŸ¤” Have queries regarding Apache SeaTunnel, Join Slack channel to discuss them join <a target="_blank" rel="noopener noreferrer" href="https://s.apache.org/seatunnel-slack">#SeaTunnel</a> channel! ðŸŒŸ</div></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/image/logo.png" alt="Apache SeaTunnel Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/image/logo.png" alt="Apache SeaTunnel Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Apache SeaTunnel</b></a></div><div class="navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/">Home</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Document</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/2.3.7/about">2.3.7</a></li><li><a class="dropdown__link" href="/docs/2.3.6/about">2.3.6</a></li><li><a class="dropdown__link" href="/docs/2.3.5/about">2.3.5</a></li><li><a class="dropdown__link" href="/docs/2.3.4/about">2.3.4</a></li><li><a class="dropdown__link" href="/docs/2.3.3/about">2.3.3</a></li><li><a class="dropdown__link" href="/docs/about">Next</a></li><li><a class="dropdown__link" href="/versions/">All versions</a></li></ul></div><a class="navbar__item navbar__link" href="/download">Download</a><a class="navbar__item navbar__link" href="/community/contribution_guide/contribute">Community</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/user_cases">UserCases</a><a class="navbar__item navbar__link" href="/team">Team</a><a class="navbar__item navbar__link" href="/user">Users</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">ASF</a><ul class="dropdown__menu"><li><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Foundation</a></li><li><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="dropdown__link">License</a></li><li><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="dropdown__link">Events</a></li><li><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sponsorship</a></li><li><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Thanks</a></li><li><a href="https://apache.org/foundation/policies/privacy.html" target="_blank" rel="noopener noreferrer" class="dropdown__link">Privacy</a></li></ul></div><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a class="navbar__item navbar__link" href="/security">Security</a><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>English</a><ul class="dropdown__menu"><li><a href="/blog/tags/meetup" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="en">English</a></li><li><a href="/zh-CN/blog/tags/meetup" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-CN">ç®€ä½“ä¸­æ–‡</a></li></ul></div><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/3/31/SeaTunnel_2_3_1_Released_Refactored_AI_Compatible_Feature_Allows_ChatGPT_Automatic_Get">SeaTunnel 2.3.1 is released! The refactored AI Compatible feature allows ChatGPT to automatically generate Connector code</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/3/29/Performance_Test_Report_SeaTunnel_Synchronizes_Data_in_Batches_420_Percent_Faster_than_GLUE.md">Performance Test Report: SeaTunnel Synchronizes data in batches 420% Faster than GLUE!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/2023/02/09/SeaTunnel_Now_Supports_CDC_Writing_by_ClickHouse_Connector.md">SeaTunnel now supports CDC (Capture Change Data) writing by ClickHouse Connector!</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!">In the recently released SeaTunnel 2.3.0 official version</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection">SeaTunnel supports IoTDB to implement IoT data synchronization</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>9 posts tagged with &quot;Meetup&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="In the recently released SeaTunnel 2.3.0 official version, the community self-developed engine SeaTunnel Zeta which has been under preparation for more than a yearâ€”â€”is officially released, and it will be used as the default engine of SeaTunnel in the future, providing users with high throughput, low latency, reliable consistent synchronization job operation guarantee."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Reveal the core design of the SeaTunnel Zeta synchronization engine!">In the recently released SeaTunnel 2.3.0 official version</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2023-01-10T00:00:00.000Z" itemprop="datePublished">January 10, 2023</time> Â· <!-- -->12 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/16733443077196-048cdae0ce9892c195e1c6ca1374cfc5.png" width="900" height="383" class="img_ev3q"></p><p>In the recently released SeaTunnel 2.3.0 official version, the community self-developed engine SeaTunnel Zeta which has been under preparation for more than a yearâ€”â€”is officially released, and it will be used as the default engine of SeaTunnel in the future, providing users with high throughput, low latency, reliable consistent synchronization job operation guarantee.</p><p>Why does SeaTunnel develop its synchronization engine? What is the positioning of the SeaTunnel Engine? How is it different from traditional computing engines? What is the design idea? What is unique about the architectural design? These questions will be answered in this article.</p><ul><li>Why develop our engine</li><li>SeaTunnel Engine Positioning</li><li>Design ideas</li><li>Architecture design</li><li>Unique advantages and features</li><li>Current basic functions and features</li><li>Future optimization plan</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-why-develop-our-engine">01 Why develop our engine<a href="#01-why-develop-our-engine" class="hash-link" aria-label="Direct link to 01 Why develop our engine" title="Direct link to 01 Why develop our engine">â€‹</a></h2><p>It was a year ago that the SeaTunnel community publicly stated for the first time that it would develop its engine. The reason why the team decided to develop a self-developed engine was that SeaTunnel&#x27;s connector can run only on Flink or Spark, and Flink and Spark, as computing engines, have many unsolvable problems when integrating and synchronizing data.</p><p>Refer to:
Why do we self-develop the big data synchronization engine SeaTunnel Zeta?
<a href="https://github.com/apache/incubator-seatunnel/issues/1954" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/1954</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-design-ideas">02 Design ideas<a href="#02-design-ideas" class="hash-link" aria-label="Direct link to 02 Design ideas" title="Direct link to 02 Design ideas">â€‹</a></h2><p>The general idea of engine design is as follows:</p><ol><li>Simple and easy to use, the new engine minimizes the dependence on third-party services, and can realize cluster management, snapshot storage, and cluster HA functions without relying on big data components such as Zookeeper and HDFS. This is very useful for users who do not have a big data platform or are unwilling to rely on a big data platform for data synchronization.</li><li>More resource-saving, at the CPU level, Zeta Engine internally uses Dynamic Thread Sharing (dynamic thread sharing) technology. In the real-time synchronization scenario, if the number of tables is large but the amount of data in each table is small, Zeta Engine will Synchronous tasks run in shared threads, which can reduce unnecessary thread creation and save system resources. On the read and data write side, the Zeta Engine is designed to minimize the number of JDBC connections. In the CDC scenario, Zeta Engine will try to reuse log reading and parsing resources as much as possible.</li><li>More stable. In this version, Zeta Engine uses Pipeline as the minimum granularity of Checkpoint and fault tolerance for data synchronization tasks. The failure of a task will only affect the tasks that have upstream and downstream relationships with it. Try to avoid task failures that cause the entire Job to fail. or rollback. At the same time, for scenarios where the source data has a storage time limit, Zeta Engine supports enabling data cache to automatically cache the data read from the source, and then the downstream tasks read the cached data and write it to the target. In this scenario, even if the target end fails and data cannot be written, it will not affect the normal reading of the source end, preventing the source end data from being deleted due to expiration.</li><li>Faster, Zeta Engineâ€™s execution plan optimizer will optimize the execution plan to reduce the possible network transmission of data, thereby reducing the loss of overall synchronization performance caused by data serialization and deserialization, and completing faster Data synchronization operations. Of course, it also supports speed limiting, so that sync jobs can be performed at a reasonable speed.</li><li>Data synchronization support for all scenarios. SeaTunnel aims to support full synchronization and incremental synchronization under offline batch synchronization, and support real-time synchronization and CDC.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-architecture-design">03 Architecture design<a href="#03-architecture-design" class="hash-link" aria-label="Direct link to 03 Architecture design" title="Direct link to 03 Architecture design">â€‹</a></h2><p>SeaTunnel Engine is mainly composed of a set of APIs for data synchronization processing and a core computing engine. Here we mainly introduce the architecture design of the SeaTunnel Engine core engine.
<img loading="lazy" src="/assets/images/16733443263288-1764bbf249c027947d7c6d39cbf41202.png" width="2342" height="1182" class="img_ev3q">
picture</p><p>SeaTunnel Engine consists of three main services: <strong>CoordinatorService, TaskExecutionService, and SlotService.</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="coordinator-service">Coordinator Service<a href="#coordinator-service" class="hash-link" aria-label="Direct link to Coordinator Service" title="Direct link to Coordinator Service">â€‹</a></h3><p>CoordinatorService is the Master service of the cluster, which provides the generation process of each job from LogicalDag to ExecutionDag, and then to PhysicalDag, and finally creates the JobMaster of the job for scheduling execution and status monitoring of the job. CoordinatorService is mainly composed of 4 large functional modules:</p><ol><li>JobMaster is responsible for the generation process from LogicalDag to ExecutionDag to PhysicalDag of a single job, and is scheduled to run by PipelineBaseScheduler.</li><li>CheckpointCoordinator, responsible for the Checkpoint process control of the job.</li><li>ResourceManager is responsible for the application and management of job resources. It currently supports Standalone mode and will support On Yarn and On K8s in the future.</li><li>Metrics Service, responsible for the statistics and summary of job monitoring information.</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="taskexecutionservice">TaskExecutionService<a href="#taskexecutionservice" class="hash-link" aria-label="Direct link to TaskExecutionService" title="Direct link to TaskExecutionService">â€‹</a></h3><p>TaskExecutionService is the Worker service of the cluster, which provides the real runtime environment of each Task in the job. TaskExecutionService uses Dynamic Thread Sharing technology to reduce CPU usage.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="slotservice">SlotService<a href="#slotservice" class="hash-link" aria-label="Direct link to SlotService" title="Direct link to SlotService">â€‹</a></h3><p>SlotService runs on each node of the cluster and is mainly responsible for the division, application, and recycling of resources on the node.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-unique-advantages-and-features">04 Unique advantages and features<a href="#04-unique-advantages-and-features" class="hash-link" aria-label="Direct link to 04 Unique advantages and features" title="Direct link to 04 Unique advantages and features">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="autonomous-cluster">Autonomous cluster<a href="#autonomous-cluster" class="hash-link" aria-label="Direct link to Autonomous cluster" title="Direct link to Autonomous cluster">â€‹</a></h3><p>SeaTunnel Engine has realized autonomous clustering (no centralization). To achieve cluster autonomy and job fault tolerance without relying on third-party service components (such as Zookeeper), SeaTunnel Engine uses Hazelcast as the underlying dependency. Hazelcast provides a distributed memory network, allowing users to operate a distributed collection like a normal Java collection locally. SeaTunnel saves the status information of the job in the memory grid of Hazelcast. When the Master node switches, it can Job state recovery based on data in the Hazelcast in-memory grid. At the same time, we have also implemented the persistence of Hazelcast memory grid data, and persisted the job status information to the storage (database of JDBC protocol, HDFS, cloud storage) in the form of WAL. In this way, even if the entire cluster hangs and restarts, the runtime information of the job can be repaired.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-cache">Data cache<a href="#data-cache" class="hash-link" aria-label="Direct link to Data cache" title="Direct link to Data cache">â€‹</a></h3><p>SeaTunnel Engine is different from the traditional Spark/Flink computing engine, it is an engine specially used for data synchronization. The SeaTunnel engine naturally supports data cache. When multiple synchronous jobs in the cluster share a data source, the SeaTunnel engine will automatically enable the data cache. The source of a job will read the data and write it into the cache, and all other jobs will no longer read data from the data source but are automatically optimized to read data from the Cache. The advantage of this is that it can reduce the reading pressure of the data source and reduce the impact of data synchronization on the data source.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="speed-control">Speed control<a href="#speed-control" class="hash-link" aria-label="Direct link to Speed control" title="Direct link to Speed control">â€‹</a></h3><p>SeaTunnel Engine supports the speed limit during data synchronization, which is very useful when reading data sources with high concurrency. A reasonable speed limit can not only ensure that the data is synchronized on time, but also minimize the pressure on the data source.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shared-connection-pool-to-reduce-database-pressure">Shared connection pool to reduce database pressure<a href="#shared-connection-pool-to-reduce-database-pressure" class="hash-link" aria-label="Direct link to Shared connection pool to reduce database pressure" title="Direct link to Shared connection pool to reduce database pressure">â€‹</a></h3><p>At present, the underlying operating tools and data synchronization tools provided by computing engines such as Spark/Flink cannot solve the problem that each table needs a JDBC connection when the entire database is synchronized. Database connections are resources for the database. Too many database connections will put great pressure on the database, resulting in a decrease in the stability of database read and write delays. This is a very serious accident for business databases. To solve this problem, SeaTunnel Engine uses a shared connection pool to ensure that multiple tables can share JDBC connections, thereby reducing the use of database connections.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="breakpoint-resume-incrementalfull-volume">Breakpoint resume (incremental/full volume)<a href="#breakpoint-resume-incrementalfull-volume" class="hash-link" aria-label="Direct link to Breakpoint resume (incremental/full volume)" title="Direct link to Breakpoint resume (incremental/full volume)">â€‹</a></h3><p>SeaTunnel Engine supports resumed uploads under offline synchronization. When the amount of data is large, a data synchronization job often needs to run for tens of minutes or several hours. If the middle job hangs up and reruns, it means wasting time. SeaTunnel Engine will continue to save the state (checkpoint) during the offline synchronization process. When the job hangs up and reruns, it will continue to run from the last checkpoint, which effectively solves the data that may be caused by hardware problems such as node downtime. Delay.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-schema-revolution-route">The Schema revolution route<a href="#the-schema-revolution-route" class="hash-link" aria-label="Direct link to The Schema revolution route" title="Direct link to The Schema revolution route">â€‹</a></h3><p>Schema evolution is a feature that allows users to easily change a table&#x27;s current schema to accommodate data that changes over time. Most commonly, it is used when performing an append or overwrite operation, to automatically adjust the schema to include one or more new columns.</p><p>This capability is required in real-time data warehouse scenarios. Currently, the Flink and Spark engines do not support this feature.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fine-grained-fault-tolerant-design">Fine-grained fault-tolerant design<a href="#fine-grained-fault-tolerant-design" class="hash-link" aria-label="Direct link to Fine-grained fault-tolerant design" title="Direct link to Fine-grained fault-tolerant design">â€‹</a></h3><p>Flink&#x27;s design is fault tolerance and rollback at the entire job level. If a task fails, the entire job will be rolled back and restarted. The design of SeaTunnel Engine takes into account that in the data synchronization scenario, in many q cases, the failure of a task should only need to focus on fault tolerance for tasks that have upstream and downstream relationships with it. Based on this design principle, SeaTunnel Engine will first generate a logical DAG according to the user-configured job configuration file, then optimize the logical DAG, and finally generate a pipeline (a connected subgraph in a job DAG) to call and execute jobs at the granularity. fault tolerance.</p><p>A typical usage scenario is:</p><p>Use the CDC connector to read data from MySQL&#x27;s binlog and write it to another MySQL. If you use Flink or Spark engine, once the target MySQL cannot write, it will cause the task of CDC to read the binlog to be terminated. If MySQL is set If the expiration time of the log is set, the problem of the target MySQL is solved, but the log of the source MySQL is cleared, which leads to data loss and other problems.</p><p>SeaTunnel Engine will automatically optimize this synchronization task, automatically add the source to the target Cache, and then further optimize this job into two Pipelines, pipeline#1 is responsible for reading data from the CDC and writing it to the SeaTunnel Cache, and pipeline#2 is responsible for reading data from the SeaTunnel Cache Cache reads data and writes to target MySQL. If there is a problem with the target MySQL and cannot be written, the pipeline#2 of this synchronization job will be terminated, and the pipeline#1 will still run normally. This design fundamentally solves the above problems and is more in line with the processing logic of the data synchronization engine.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="dynamically-share-threads-to-reduce-resource-usage">Dynamically share threads to reduce resource usage<a href="#dynamically-share-threads-to-reduce-resource-usage" class="hash-link" aria-label="Direct link to Dynamically share threads to reduce resource usage" title="Direct link to Dynamically share threads to reduce resource usage">â€‹</a></h3><p>SeaTunnel Engine&#x27;s Task design uses shared thread technology. Different from Flink/Spark, SeaTunnel Engine does not simply allow a Task to occupy a thread, but through a dynamic perception method - Dynamic Thread Sharing (Dynamic Thread Sharing) To judge whether a Task should share a thread with other Tasks or should monopolize a thread.</p><p>Compared with single-threaded serial computing, multi-threaded parallel computing has better performance advantages, but if each Task uses an independent thread to run, when there are many tables for data synchronization and the number of Tasks is large, it will be in the Worker node Start very many threads on it. When the number of CPU cores is fixed, the more threads, the better. When the number of threads is too large, the CPU needs to spend a lot of time on thread context switching, which will affect computing performance.</p><p>Flink/Spark usually limits the maximum number of tasks running on each node. In this way, it can avoid starting too many threads. To run more tasks on one node, SeaTunnel Engine can share thread technology. Let those tasks with a small amount of data share threads, and tasks with a large amount of data exclusively use threads. This method makes it possible for SeaTunnel Engine to run hundreds or thousands of table synchronization tasks on one node, with less resource occupation. Complete the synchronization of more tables.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-basic-functions-and-features">05 Basic functions and features<a href="#05-basic-functions-and-features" class="hash-link" aria-label="Direct link to 05 Basic functions and features" title="Direct link to 05 Basic functions and features">â€‹</a></h2><p>2.3.0 is the first official version of SeaTunnel Engine, which implements some basic functions. For the detailed design, please refer to: <a href="https://github.com/apache/incubator-seatunnel/issues/2272" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/2272</a></p><p><strong>[ Cluster Management ]</strong></p><ul><li>Support stand-alone operation</li><li>Support cluster operation</li><li>Autonomous cluster (no centralization), no need to specify a Master node for the SeaTunnel Engine cluster, SeaTunnel Engine elects the Master node by itself during operation and automatically selects a new Master node after the Master node hangs up.</li><li>Automatic discovery of cluster nodes, the nodes with the same cluster_name will automatically form a cluster.</li></ul><p><strong>[ Core function ]</strong></p><ul><li>Supports running jobs in Local mode. The cluster is automatically destroyed after the job runs.</li><li>It supports running jobs in Cluster mode (single machine or cluster) and submitting jobs to the SeaTunnel Engine service through SeaTunnel Client. After the job is completed, the service continues to run and waits for the next job submission.</li><li>Support offline batch synchronization.</li><li>Support real-time synchronization.</li><li>Batch and flow integration, all SeaTunnel V2 version connectors can run in SeaTunnel Engine.</li><li>Supports distributed snapshot algorithm cooperates with SeaTunnel V2 connector to support two-phase commit, and ensures data exactly-once.</li><li>Supports job invocation at the Pipeline level to ensure that it can be started even when resources are limited.</li><li>Supports job fault tolerance at the Pipeline level. The failure of a Task only affects the Pipeline it is in, and only the Task under the Pipeline needs to be rolled back.</li><li>Supports dynamic thread sharing to achieve real-time synchronization of a large number of small data sets.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="06-future-optimization-plan">06 Future optimization plan<a href="#06-future-optimization-plan" class="hash-link" aria-label="Direct link to 06 Future optimization plan" title="Direct link to 06 Future optimization plan">â€‹</a></h2><ul><li>Support Cache mode, and first support Kafka as Cache</li><li>Support JobHistory, support the persistence of JobHistory.</li><li>Support indicator (Reader Rows, QPS, Reader Bytes) monitoring and indicator query</li><li>Support dynamic modification of the execution plan.</li><li>Support CDC.</li><li>Support whole database synchronization</li><li>Support multi-table synchronization</li><li>Support for Schema Revolution</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Apache IoTDB (Internet of Things Database) is a software system that integrates the collection, storage, management, and analysis of time series data of the Internet of Things, which can meet the needs of massive data storage, high-speed data reading, and complex data analysis in the field of Industrial Internet of Things. Currently, SeaTunnel already supports IoTDB Connector, realizing the connection of data synchronization scenarios in the IoT field."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Apache IoTDB (Internet of Things Database) is a software system that integrates the collection">SeaTunnel supports IoTDB to implement IoT data synchronization</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-12-10T00:00:00.000Z" itemprop="datePublished">December 10, 2022</time> Â· <!-- -->12 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/16714316482580-41ece0a1c0d976185e2c75f3329aa12d.jpg" width="720" height="320" class="img_ev3q"></p><blockquote><p>Apache IoTDB (Internet of Things Database) is a software system that integrates the collection, storage, management, and analysis of time series data of the Internet of Things, which can meet the needs of massive data storage, high-speed data reading, and complex data analysis in the field of Industrial Internet of Things. Currently, SeaTunnel already supports IoTDB Connector, realizing the connection of data synchronization scenarios in the IoT field.</p></blockquote><blockquote><p>At the SeaTunnel community online meeting in October this year, SeaTunnel Committer Wang Hailin introduced the implementation process of SeaTunnelâ€™s access to IoTDB, allowing users to have a deeper understanding of the operation method and principle of IoTDB data synchronization.</p></blockquote><p>The topic Iâ€™m sharing today is using SeaTunnel to play around with data synchronization in IoTDB.</p><p>This session is divided into 6 subsections. Firstly, we will have an understanding of the basic concept of SeaTunnel, and on this basis, we will focus on the functional features of IoTDB Connector, then we will analyze the data read and write functions of IoTDB Connector and the parsing of the implementation, and finally, we will show some typical usage scenarios and cases to let you understand how to use Finally, we will show some typical usage scenarios and cases to understand how to use the IoTDB Connector to implement into production environments. The last point is the communityâ€™s next steps for the IoTDB Connector and guidance on how to get involved in contributing.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-to-seatunnel-basic-concepts">Introduction to SeaTunnel basic concepts<a href="#introduction-to-seatunnel-basic-concepts" class="hash-link" aria-label="Direct link to Introduction to SeaTunnel basic concepts" title="Direct link to Introduction to SeaTunnel basic concepts">â€‹</a></h2><p>This is the basic architecture of SeaTunnel, an engine built for data synchronization, with a set of abstract APIs for reading data from and writing to a variety of data sources.</p><p><img loading="lazy" src="/assets/images/16714316839299-a92e73bfbc3881b099146d6d161c412f.jpg" width="720" height="389" class="img_ev3q">
The left-hand side briefly lists the Source scenarios, for example, we abstract the Sourceâ€™s API, Type, and State, to read the data source, unifying the data types of the various data sources to the abstract type defined in it, and some state recovery and retention of the read location during the reading process.</p><p>This is an abstraction for Source, and we have done a similar abstraction for Sink, i.e. how data is written, and how the data type matches the real data source type, and how the state is restored and retained.</p><p>Based on these APIs, we will have a translation layer to translate these APIs to the corresponding execution engine. SeaTunnel currently supports three execution engines, Spark, Flink, and our own execution engine, SeaTunnel Engine, which will be released soon.</p><p>This is roughly what SeaTunnel does, SeaTunnel relies on Source and Sink to read and write data for data synchronization, we call them Connectors. The Connector consists of a Source and a Sink.</p><p><img loading="lazy" src="/assets/images/16714316928812-337c3d4770094c09ae55a5312670d87f.jpg" width="720" height="758" class="img_ev3q">
From the diagram above we see the different data sources, Source is responsible for reading data from the various data sources and transforming it into SeaTunnelRow abstraction layer and Type to form the abstraction layer, Sink is responsible for pulling data from the abstraction layer and writing it to the concrete data store to transform it into the store concrete format.</p><p>The combination of Source + Abstraction Layer + Sink enables the synchronization of data between multiple heterogeneous data sources.</p><p>Iâ€™ll use a simple example below to illustrate how SeaTunnelâ€™s Source and Sink work.</p><p><img loading="lazy" src="/assets/images/16714317022389-f17930bc8877d173d40eab8fdfe47631.jpg" width="720" height="822" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/16714317067444-f828b5a72003d3caec8958edccdc8dca.jpg" width="720" height="340" class="img_ev3q">
We can specify the number of Sources, Sink configuration file combinations through the configuration file The commands in the toolkit provided by SeaTunnel take the configuration file with them and when executed enable data handling.</p><p><img loading="lazy" src="/assets/images/16714317166018-4c6a20778066aa8bfb5c319c4706b17b.jpg" width="720" height="1202" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714317203806-c4d2fe2e0a1d9a235d59e878c42b9308.jpg" width="651" height="1280" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714317262218-a608b27e7ed69d6bab301f6bdf929966.jpg" width="720" height="933" class="img_ev3q">
This is the Connector ecosystem that is currently supported by SeaTunnel, such as the data sources supported by JBDC, HDFS, Hive, Pulsar, message queues, etc. are currently supported.</p><p>The list in the picture is not exhaustive of the Connectors supported by SeaTunnel. Under the GitHub SeaTunnel project, you can see the Plugins directory, where supported Connector plugins are constantly being added and where you can see the latest access in real-time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="iotdb-connector-features">IoTDB Connector Features<a href="#iotdb-connector-features" class="hash-link" aria-label="Direct link to IoTDB Connector Features" title="Direct link to IoTDB Connector Features">â€‹</a></h2><p>Below is information about access to the IoTDB Connector.</p><p>Firstly, we would like to introduce the functional features of IoTDB, the IoTDB Connector integrated with SeaTunnel, and what exactly it supports for your reference.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="source-features">Source Features<a href="#source-features" class="hash-link" aria-label="Direct link to Source Features" title="Direct link to Source Features">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714317512435-d9aa02e6d36efefd2a2c20cf77ce8f6b.jpg" width="720" height="384" class="img_ev3q">
Firstly, there are the typical usage scenarios supported by Source, such as bulk reading of devices, field projection, data type mapping, parallel reading, etc.</p><p>As you can see above, IoTDB supports all features except once, exactly once and stream mode, such as batch reads, IoTDB has a SQL syntax similar to group by device, which allows you to read data from multiple devices in a single batch. For basic data type projection, the SQL in IoTDB will take time by default when looking up any metric, or group by the device will take the device column, and we also support projection onto SeaTunnel columns by default.</p><p>The only data type not supported is Victor, all others are supported.</p><p>For the parallel read piece, the IoTDB data is actually timestamped and we use timestamped ranges to achieve parallel reads.</p><p>The recovery of the state, since we have divided the time range read into different splits, can be done based on the Split location information.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sink-functional-features">Sink functional features<a href="#sink-functional-features" class="hash-link" aria-label="Direct link to Sink functional features" title="Direct link to Sink functional features">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714317679569-a1923e70d93e6204c42aa7531a96a024.jpg" width="720" height="372" class="img_ev3q"></p><p>The diagram above shows the features already supported by SeaTunnel. Regarding metadata extraction, we support the extraction of metadata such as measurement, device, etc. from SeaTunnelRow and the extraction or use of current processing time from SeaTunnelRow. Batch commits and exception retries are also supported.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="iotdb-data-reading-analysis">IoTDB data reading analysis<a href="#iotdb-data-reading-analysis" class="hash-link" aria-label="Direct link to IoTDB data reading analysis" title="Direct link to IoTDB data reading analysis">â€‹</a></h2><p>Next, we analyze the implementation and support for data reading.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-type-mapping">Data type mapping<a href="#data-type-mapping" class="hash-link" aria-label="Direct link to Data type mapping" title="Direct link to Data type mapping">â€‹</a></h2><p>The first is the data type mapping, which actually reads the IoTDB data type to SeaTunnel, so it has to be converted to the SeaTunnel data type.
<img loading="lazy" src="/assets/images/16714317930593-3931931989e2fc8224bb071304463c53.jpg" width="720" height="460" class="img_ev3q">
The BOOLEAN, INT32, INT64, etc. listed here all have corresponding SeaTunnel data types. INT32 can be mapped according to the read type on the SeaTunnel, or to TINYINT, SMALLINT, or INT when the range of values is small.</p><p>The Vector type is not currently supported.</p><p><img loading="lazy" src="/assets/images/16714318216373-6472b0b9745b67620ca357054a0715fb.jpg" width="720" height="758" class="img_ev3q">
This is the corresponding example code showing how the mapping is done where the type conversion is done.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="field-projection">Field projection<a href="#field-projection" class="hash-link" aria-label="Direct link to Field projection" title="Direct link to Field projection">â€‹</a></h2><p>The other is the field projection when reading, we can automatically map Time fields when reading IoTDB data, or we can choose to map some of the data to SeaTunnel, such as TIMESTAMP, or BIGINT.</p><p><img loading="lazy" src="/assets/images/16714318381313-29a760765ae69e80e04015976521aabe.jpg" width="720" height="320" class="img_ev3q">
The SQL extraction of column codes allows you to extract only some of the columns you need, and when used on SeaTunnel, you can specify the name, type, etc. of the column after it is mapped to SeaTunnel via fields. The final result of the data read on SeaTunnel is shown in the figure above.</p><p><img loading="lazy" src="/assets/images/16714318550071-25f4bde1d47b33463a1f2d6e13651435.jpg" width="720" height="368" class="img_ev3q"></p><p>We have just seen that we do not have the time column in the SQL, but the actual result is that there is this column, so we support the projection of the time column field, the time column can actually be projected into different data types, the user can convert according to their needs. The diagram above shows the implementation logic.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="batch-read-device">Batch read Device<a href="#batch-read-device" class="hash-link" aria-label="Direct link to Batch read Device" title="Direct link to Batch read Device">â€‹</a></h2><p>This is a common requirement, as we are likely to synchronize data in large batches with the same data structure.</p><p><img loading="lazy" src="/assets/images/16714318796196-bee32b3eae066fdc0ce82182fc6eeb09.jpg" width="720" height="333" class="img_ev3q"></p><p>SeaTunnel supports the align-by-device syntax so that device columns can also be projected onto the SeaTunnelRow</p><p><img loading="lazy" src="/assets/images/16714318873362-54b8d7281a93bc335a39e995db5e1a4f.jpg" width="720" height="185" class="img_ev3q">
Assuming there is a table in IoTDB, we project the device column onto SeaTunnel by making it data as well through syntax. After configuring the device name column and specifying the data type, we end up reading the data on SeaTunnel in the format shown above, containing the Time, device column, and the actual data value. This makes it possible to read data from the same device in bulk.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="parallel-reading">Parallel reading<a href="#parallel-reading" class="hash-link" aria-label="Direct link to Parallel reading" title="Direct link to Parallel reading">â€‹</a></h2><p>The other is a parallel read.</p><ul><li><p>Split
We have scoped the table by the Time column and if we are reading in parallel we may want to scope the table to allow parallel threads/processes to read a specific range of data. By configuring the three parameters, the end result will be a query SQL, where the original SQL is divided into different splits with query conditions to achieve the actual read SQL.</p></li><li><p>Allocate Split to the reader
Once the split is done, there is an allocation logic to follow in order to distribute it to each parallel reader.</p></li></ul><p><img loading="lazy" src="/assets/images/16714319281280-dfc833cfbd8315f969c518271cd38e95.jpg" width="720" height="162" class="img_ev3q"></p><p>This logic is based on the ID of the split to the reader, which may be more random, or more uniform if the ID of the split is more hashed, depending on the Connector.</p><p><img loading="lazy" src="/assets/images/16714319372730-361662ffb0d74adea0f64073bf2cf44f.jpg" width="720" height="528" class="img_ev3q"></p><p>The result achieved is shown in the picture.</p><h1>Status recovery</h1><p>There is also state recovery involved when reading because if the task is large, the reading will take longer, and if there is an error or exception in the middle, you have to consider how to recover the state from the point where the error occurred, and then read it again afterward.</p><p><img loading="lazy" src="/assets/images/16714319281280-dfc833cfbd8315f969c518271cd38e95.jpg" width="720" height="162" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714319569097-2f328036d7363eb6b0d3678984d78af0.jpg" width="720" height="268" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714319599521-13dc977f614b93a4db1eed15ee1e4f30.jpg" width="720" height="256" class="img_ev3q"></p><p>SeaTunnelâ€™s state recovery is mainly through the reader storing the unread Split information into the state, and then the engine will periodically take a snapshot of the state when reading so that we can restore the last snapshot when we recover and continue reading afterward.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="iotdb-connector-data-write-analysis">IoTDB Connector Data Write Analysis<a href="#iotdb-connector-data-write-analysis" class="hash-link" aria-label="Direct link to IoTDB Connector Data Write Analysis" title="Direct link to IoTDB Connector Data Write Analysis">â€‹</a></h2><p>The next step is the parsing of the data writes.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-type-mapping-1">Data type mapping<a href="#data-type-mapping-1" class="hash-link" aria-label="Direct link to Data type mapping" title="Direct link to Data type mapping">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714319862080-244a507b57e8568444d4ead4b29349aa.jpg" width="720" height="457" class="img_ev3q"></p><p>Data writing also involves data type mapping, but here, in contrast to data reading, it maps the SeaTunnel data types to the IoTDB data types. As IoTDB only has INT32, the writing process involves lifting the data types TINYINT and SMALLINT. All other data types can be converted one-to-one; ARRAY and VECTOR data types are not yet supported.</p><p><img loading="lazy" src="/assets/images/16714319949478-1f6dc7ae103ae0ab3852b1350c74f659.jpg" width="720" height="574" class="img_ev3q"></p><p>The above diagram shows the corresponding code, the implementation logic will need to be seen in our specific mapping.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="dynamic-injection-of-metadata">Dynamic injection of metadata<a href="#dynamic-injection-of-metadata" class="hash-link" aria-label="Direct link to Dynamic injection of metadata" title="Direct link to Dynamic injection of metadata">â€‹</a></h2><p>SeaTunnel supports the dynamic injection of metadata.</p><p>When heterogeneous data sources are written to the IoTDB, device, measurement, and time are extracted from each row of data, either by serializing the SeaTunnelRow with a fixed column value as configured. Alternatively, the system time can be used as the time, or the current system time can be populated if no time column is specified, and the storage group can be configured to be automatically appended to the device prefix.</p><p><img loading="lazy" src="/assets/images/16714320117277-aca2fc93f15601665ef8fd53529953fa.jpg" width="720" height="180" class="img_ev3q"></p><p>For example, suppose that the structure of a row in SeaTunnel reading the data format shown above can be configured to synchronize to the IoTDB and the result obtained is as follows.</p><p><img loading="lazy" src="/assets/images/16714320201848-d017fa8600fc03c9e77914a53c961557.jpg" width="720" height="197" class="img_ev3q"></p><p>The temperature and humidity columns we need were extracted, and ts and device names were extracted as the original data for the IoTDB.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="batch-commits-and-exception-retries">Batch commits and exception retries<a href="#batch-commits-and-exception-retries" class="hash-link" aria-label="Direct link to Batch commits and exception retries" title="Direct link to Batch commits and exception retries">â€‹</a></h2><p>In addition, Sink needs to handle batch and retry when writing. For batches, we can configure the appropriate batch configuration, including support for configuring the number and interval of batch commits; if the data is cached to memory, you can enable a separate thread for timed commits.</p><p>For retries, SeaTunnel supports the configuration of the number of retries, the waiting interval and the maximum number of retries, as well as the possibility to end a retry if it encounters a non-recoverable error when it has finished.</p><p><img loading="lazy" src="/assets/images/16714320394193-27ad106e26762595d5b919019bd72550.jpg" width="720" height="466" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="iotdb-connector-usage-examples">IoTDB Connector Usage Examples<a href="#iotdb-connector-usage-examples" class="hash-link" aria-label="Direct link to IoTDB Connector Usage Examples" title="Direct link to IoTDB Connector Usage Examples">â€‹</a></h2><p>After the previous analysis of reading and writing data, letâ€™s look at three typical examples of usage scenarios.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="exporting-data-from-iotdb">Exporting data from IoTDB<a href="#exporting-data-from-iotdb" class="hash-link" aria-label="Direct link to Exporting data from IoTDB" title="Direct link to Exporting data from IoTDB">â€‹</a></h2><p>The first scenario is exporting data from the IoTDB, the example I have given here is reading data from the IoTDB to the Console.</p><ul><li>Read in parallel, output to Console</li></ul><p>Parallelism: 2</p><p>Number of batches: 24</p><p>Time frame: 2022â€“09â€“25 ~ 2022â€“09â€“26
<img loading="lazy" src="/assets/images/16714320856052-31c7463162a38c89034e9a14813d4686.jpg" width="720" height="247" class="img_ev3q"></p><p>Letâ€™s assume that we have a data table in IoTDB and we want to export the data to the Console. The whole configuration is shown above and needs to map the columns of data we want to export and the time range to check.</p><p>This is the simplest example, but in practice, the Sink side may be more complex, so you will need to refer to the documentation of the corresponding data source for the appropriate configuration.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="importing-data-to-iotdb">Importing data to IoTDB<a href="#importing-data-to-iotdb" class="hash-link" aria-label="Direct link to Importing data to IoTDB" title="Direct link to Importing data to IoTDB">â€‹</a></h2><ul><li>Read database, batch write to IoTDB<ul><li>Batch writing: one commit every 1024 entries or every 1000 ms</li><li>-Extracting metadata device, timestamp, measurement</li><li>-Specify the storage group: root.test_group</li></ul></li></ul><p><img loading="lazy" src="/assets/images/16714321290339-3c0479957e9a28da554374f5c8d4f426.jpg" width="720" height="284" class="img_ev3q">
Another typical usage scenario is to import data from other data sources into IoTDB. suppose I have an external database table with columns like ts, temperature, humidity, etc. and we import it into IoTDB, requiring the columns of temperature and humidity, but the rest can be left out. The whole configuration is shown in the diagram above, you can refer to it.</p><p>On the Sink side, you mainly have to specify the Key of the device column, such as from which data the device is extracted, from which class the time is extracted, which columns to write to the IoTDB, etc.</p><p>As you can see, we can configure the storage group, which is the storage group of the IoTDB, which can be specified by the storage group.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="synchronizing-data-between-iotdb">Synchronizing data between IoTDB<a href="#synchronizing-data-between-iotdb" class="hash-link" aria-label="Direct link to Synchronizing data between IoTDB" title="Direct link to Synchronizing data between IoTDB">â€‹</a></h2><p>The third scenario is to synchronize data between IoTDB and IoTDB and write to IoTDB in bulk, suppose there is a table in IoTDB that needs to be synchronized to another IoTDB, after synchronization the storage group has changed and the name of the indicator of the data column has also changed, then you can use projection to rewrite the indicator name and use SQL to rewrite the storage group.</p><p><img loading="lazy" src="/assets/images/16714321480992-670f922b9dbe990571115d671159bb22.jpg" width="720" height="309" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-involved-in-contribution">How to get involved in contribution<a href="#how-to-get-involved-in-contribution" class="hash-link" aria-label="Direct link to How to get involved in contribution" title="Direct link to How to get involved in contribution">â€‹</a></h2><p>Finally, a few words about the next steps for the IoTDB Connector and how you can get involved in improving the Connector and contributing new features that are needed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="next-steps-for-the-iotdb-connector">Next steps for the IoTDB Connector<a href="#next-steps-for-the-iotdb-connector" class="hash-link" aria-label="Direct link to Next steps for the IoTDB Connector" title="Direct link to Next steps for the IoTDB Connector">â€‹</a></h2><ul><li>Support for reading and writing vector data types</li><li>Support for tsfile reads and writes</li><li>Support for writing tsfile and reloading to IoTDB</li></ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Apache SeaTunnel Committer | Zongwen Li"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Apache SeaTunnel Committer | Zongwen Li">SeaTunnel engine, designed for tens-of-billions data integration</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-12-09T00:00:00.000Z" itemprop="datePublished">December 9, 2022</time> Â· <!-- -->10 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/16714309876928-859e41ec15f205a23a1b25d8a2b80046.jpg" width="720" height="306" class="img_ev3q">
Apache SeaTunnel Committer | Zongwen Li</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-to-apache-seatunnel">Introduction to Apache SeaTunnel<a href="#introduction-to-apache-seatunnel" class="hash-link" aria-label="Direct link to Introduction to Apache SeaTunnel" title="Direct link to Introduction to Apache SeaTunnel">â€‹</a></h2><p>Apache SeaTunnel is a very easy-to-use ultra-high-performance distributed data integration platform that supports real-time synchronization of massive data.</p><p>Apache SeaTunnel will try its best to solve the problems that may be encountered in the process of mass data synchronization, such as data loss and duplication, task accumulation and delay, low throughput, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="milestones-of-seatunnel">Milestones of SeaTunnel<a href="#milestones-of-seatunnel" class="hash-link" aria-label="Direct link to Milestones of SeaTunnel" title="Direct link to Milestones of SeaTunnel">â€‹</a></h2><p>SeaTunnel, formerly known as Waterdrop, was open-sourced on GitHub in 2017.</p><p>In October 2021, the Waterdrop community joined the Apache incubator and changed its name to SeaTunnel.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-growth">SeaTunnel Growth<a href="#seatunnel-growth" class="hash-link" aria-label="Direct link to SeaTunnel Growth" title="Direct link to SeaTunnel Growth">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714310892722-67ccd10c8545d757fc1dab589404be3d.jpg" width="720" height="469" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714310916195-8701b0e12c30eae7063696ec13464375.jpg" width="720" height="378" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714310939883-8df5afa031f552b198757dc4dbe0bf70.jpg" width="720" height="392" class="img_ev3q">
When SeaTunnel entered the Apache incubator, the SeaTunnel community ushered in rapid growth.</p><p>As of now, the SeaTunnel community has a total of 151 contributors, 4314 Stars, and 804 forks.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pain-points-of-existing-engines">Pain points of Existing engines<a href="#pain-points-of-existing-engines" class="hash-link" aria-label="Direct link to Pain points of Existing engines" title="Direct link to Pain points of Existing engines">â€‹</a></h2><p>There are many pain points faced by the existing computing engines in the field of data integration, and we will talk about this first. The pain points usually lie in three directions:</p><ul><li>The fault tolerance ability of the engine;</li><li>Difficulty in configuration, operation, and maintenance of engine jobs;</li><li>The resource usage of the engine.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="fault-tolerance">fault tolerance<a href="#fault-tolerance" class="hash-link" aria-label="Direct link to fault tolerance" title="Direct link to fault tolerance">â€‹</a></h2><p>Global Failover
<img loading="lazy" alt="Global-failover" src="/assets/images/16714311670656-14b472dd8abdeff2639c1701ed9997f9.jpg" width="720" height="176" class="img_ev3q">
For distributed streaming processing systems, high throughput and low latency are often the most important requirements. At the same time, fault tolerance is also very important in distributed systems. For scenarios that require high correctness, the implementation of exactly once is often very important.</p><p>In a distributed streaming processing system, since the computing power, network, load, etc. of each node are different, the state of each node cannot be directly merged to obtain a true global state. To obtain consistent results, the distributed processing system needs to be resilient to node failure, that is, it can recover to consistent results when it fails.</p><p>Although it is claimed in their official blog that Sparkâ€™s Structured Streaming uses the Chandy-Lamport algorithm for Failover processing, it does not disclose more details.</p><p>Flink implemented Checkpoint as a fault-tolerant mechanism based on the above algorithm and published related papers: Lightweight Asynchronous Snapshots for Distributed Dataflows</p><p>In the current industrial implementation, when a job fails, all nodes of the job DAG need to failover, and the whole process will last for a long time, which will cause a lot of upstream data to accumulate.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="loss-of-data">Loss of Data<a href="#loss-of-data" class="hash-link" aria-label="Direct link to Loss of Data" title="Direct link to Loss of Data">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714312426416-f778eaa67ecf527449353404cf0bf421.jpg" width="720" height="175" class="img_ev3q">
The previous problem will cause a long-time recovery, and the business service may accept a certain degree of data delay.</p><p>In a worse case, a single sink node cannot be recovered for a long time, and the source data has a limited storage time, such as MySQL and Oracle log data, which will lead to data loss.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="configuration-is-cumbersome">Configuration is cumbersome<a href="#configuration-is-cumbersome" class="hash-link" aria-label="Direct link to Configuration is cumbersome" title="Direct link to Configuration is cumbersome">â€‹</a></h2><p>Single table Configuration</p><p><img loading="lazy" src="/assets/images/16714312637015-abff06af8bf814cbb39e2fddfd5b0271.jpg" width="720" height="360" class="img_ev3q">
The previous examples are cases regarding a small number of tables, but in real business service development, we usually need to synchronize thousands of tables, which may be divided into databases and tables at the same time;</p><p>The status quo is that we need to configure each table, a large number of table synchronization takes a lot of time for users, and it is prone to problems such as field mapping errors, which are difficult to maintain.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="not-supporting-schema-evolution">Not supporting Schema Evolution<a href="#not-supporting-schema-evolution" class="hash-link" aria-label="Direct link to Not supporting Schema Evolution" title="Direct link to Not supporting Schema Evolution">â€‹</a></h2><p><img loading="lazy" alt="Not-supports-DDL" src="/assets/images/16714312769761-80425f8f27f3efc3ab62487814dad59e.jpg" width="720" height="360" class="img_ev3q">
Besides, according to the research report of Fivetran, 60% of the companyâ€™s schema will change every month, and 30% will change every week.</p><p>However, none of the existing engines supports Schema Evolution. After changing the Schema each time, the user needs to reconfigure the entire link, which makes the maintenance of the job very cumbersome.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-high-volume-of-resource-usage">The high volume of resource usage<a href="#the-high-volume-of-resource-usage" class="hash-link" aria-label="Direct link to The high volume of resource usage" title="Direct link to The high volume of resource usage">â€‹</a></h2><p>The database link takes up too much</p><p><img loading="lazy" src="/assets/images/16714313100541-98837b408ddcd3e5d7114421e893de80.jpg" width="720" height="480" class="img_ev3q">
If our Source or Sink is of JDBC type, since the existing engine only supports one or more links per table, when there are many tables to be synchronized, more link resources will be occupied, which will bring a great burden to the database server.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="operator-pressure-is-uncontrollable">Operator pressure is uncontrollable<a href="#operator-pressure-is-uncontrollable" class="hash-link" aria-label="Direct link to Operator pressure is uncontrollable" title="Direct link to Operator pressure is uncontrollable">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714313301435-5d797838b956772428477a7a26a2591c.jpg" width="720" height="333" class="img_ev3q">
In the existing engine, a buffer and other control operators are used to control the pressure, that is, the back pressure mechanism; since the back pressure is transmitted level by level, there will be pressure delay, and at the same time, the processing of data will not be smooth enough, increasing the GC time, fault-tolerant completion time, etc.</p><p>Another case is that neither the source nor the sink has reached the maximum pressure, but the user still needs to control the synchronization rate to prevent too much impact on the source database or the target database, which cannot be controlled through the back pressure mechanism.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-goals-of-apache-seatunnel-engine">Architecture goals of Apache SeaTunnel Engine<a href="#architecture-goals-of-apache-seatunnel-engine" class="hash-link" aria-label="Direct link to Architecture goals of Apache SeaTunnel Engine" title="Direct link to Architecture goals of Apache SeaTunnel Engine">â€‹</a></h2><p>To solve these severe issues faced by computing engines, we self-developed our engine expertise in big data integration.</p><p>Firstly, letâ€™s get through what goals this engine wants to achieve.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="pipeline-failover">Pipeline Failover<a href="#pipeline-failover" class="hash-link" aria-label="Direct link to Pipeline Failover" title="Direct link to Pipeline Failover">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714313559400-d6029daeb8672d49a5bfb8718c518e3f.jpg" width="720" height="176" class="img_ev3q">
In the data integration case, there is a possibility that a job can synchronize hundreds of sheets, and the failure of one node or one table will lead to the failure of all tables, which is too costly.</p><p>We expect that unrelated Job Tasks will not affect each other during fault tolerance, so we call a vertex collection with upstream and downstream relationships a Pipeline, and a Job can consist of one or more pipelines.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="regional-failover">Regional Failover<a href="#regional-failover" class="hash-link" aria-label="Direct link to Regional Failover" title="Direct link to Regional Failover">â€‹</a></h2><p>Now if there is an exception in the pipeline, we still need to failover all the vertex in the pipeline; but can we restore only part of the vertex?
<img loading="lazy" src="/assets/images/16714313919617-100427ea7efcebd349ce998e13a8c0a0.jpg" width="720" height="176" class="img_ev3q">
For example, if the Source fails, the Sink does not need to restart. In the case of a single Source and multiple Sinks, if a single Sink fails, only the Sink and Source that failed will be restored; that is, only the node that failed and its upstream nodes will be restored.</p><p>Obviously, the stateless vertex does not need to be restarted, and since SeaTunnel is a data integration framework, we do not have aggregation state vertexes such as Agg and Count, so we only need to consider Sink;</p><ul><li>Sink does not support idempotence &amp; 2PC; no restart and restart will result in the same data duplication, which can only be solved by Sink without restarting;</li><li>Sink supports idempotence, but does not support 2PC: because it is idempotent writing, it does not matter whether the source reads data inconsistently every time, and it does not need to be restarted;</li><li>Sink supports 2PC:</li><li>If the Source supports data consistency, if an abort is not executed, the processed old data will be automatically ignored through the channel data ID, and at the same time, it will face the problem that the transaction session time may time out;</li><li>If the Source does not support data consistency, perform abort on the Sink to discard the last data, which has the same effect as restarting but does not require initialization operations such as re-establishing links;</li><li>That is, the simplest implementation is to execute abort.
We use the pipeline as the minimum granularity for fault-tolerant management, and use the Chandy-Lamport algorithm to realize fault-tolerant distributed jobs.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-cache">Data Cache<a href="#data-cache" class="hash-link" aria-label="Direct link to Data Cache" title="Direct link to Data Cache">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714314318184-95b8b08ab40b5d0da4ca1bd2e659f965.jpg" width="720" height="175" class="img_ev3q">
For sink failure, when data cannot be written, a possible solution is to work two jobs at the same time.</p><p>One job reads the database logs using the CDC source connector and then writes the data to Kafka using the Kafka Sink connector. Another job reads data from Kafka using the Kafka source connector and writes data to the destination using the destination sink connector.</p><p>This solution requires users to have a deep understanding of the underlying technology, and both tasks will increase the difficulty of operation and maintenance. Because every job needs JobMaster, it requires more resources.</p><p>Ideally, the user only knows that they will be reading data from the source and writing data to the sink, and at the same time, during this process, the data can be cached in case the sink fails. The sync engine needs to automatically add caching operations to the execution plan and ensure that the source still works in the event of a sink failure. In this process, the engine needs to ensure that the data written to the cache and read from the cache are transactional, to ensure data consistency.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="sharding--multi-table-sync">Sharding &amp; Multi-table Sync<a href="#sharding--multi-table-sync" class="hash-link" aria-label="Direct link to Sharding &amp; Multi-table Sync" title="Direct link to Sharding &amp; Multi-table Sync">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714314489916-cd0b185b6626a8c1598aa3ac569ecdbd.jpg" width="720" height="448" class="img_ev3q"></p><p>For a large number of table synchronization, we expect that a single Source can support reading multiple structural tables, and then use the side stream output to keep consistent with a single table stream.</p><p>The advantage of this is that it can reduce the link occupation of the data source and improve the utilization rate of thread resources.</p><p>At the same time, in SeaTunnel Engine, these multiple tables will be regarded as a pipeline, which will increase the granularity of fault tolerance; there are trade-offs, and the user can choose how many tables a pipeline can pass through.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="schema-evolution">Schema Evolution<a href="#schema-evolution" class="hash-link" aria-label="Direct link to Schema Evolution" title="Direct link to Schema Evolution">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714314658701-b97c66b4a4eacfe54acf2e5ea6cc001b.jpg" width="720" height="199" class="img_ev3q">
Schema Evolution is a feature that allows users to easily change the current schema of a table to accommodate changing data over time. Most commonly, it is used when performing an append or overwrite operation, to automatically adjust the schema to include one or more new columns.</p><p>This feature is required for real-time data warehouse scenarios. Currently, the Flink and Spark engines do not support this feature.</p><p>In SeaTunnel Engine, we will use the Chandy-Lamport algorithm to send DDL events, make them flow in the DAG graph and change the structure of each operator, and then synchronize them to the Sink.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="shared-resource">Shared Resource<a href="#shared-resource" class="hash-link" aria-label="Direct link to Shared Resource" title="Direct link to Shared Resource">â€‹</a></h2><p><img loading="lazy" alt="Shared-resource" src="/assets/images/16714314806989-3f0543f4a2b9412e0d6c48eb1b7aa5c1.jpg" width="720" height="455" class="img_ev3q">
The Multi-table feature can reduce the use of some Source and Sink link resources. At the same time, we have implemented Dynamic Thread Resource Sharing in SeaTunnel Engine, reducing the resource usage of the engine on the server.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="speed-control">Speed Control<a href="#speed-control" class="hash-link" aria-label="Direct link to Speed Control" title="Direct link to Speed Control">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714315001348-d6e97c70a71c7f4b9220a68751cb1401.jpg" width="720" height="323" class="img_ev3q">
As for the problems that cannot be solved by the back pressure mechanism, we will optimize the Buffer and Checkpoint mechanism:</p><ul><li>Firstly, We try to allow Buffer to control the amount of data in a period;</li><li>Secondly, by the Checkpoint mechanism, the engine can lock the buffer after the Checkpoint reaches the maximum number of parallelism and executes an interval time, prohibiting the writing of Source data, achieving the result of taking the pressure proactively, avoiding issues like back pressure delay or failure to be delivered to Source.
The above is the design goal of SeaTunnel Engine, hoping to help you better solve the problems that bother you in data integration. In the future, we will continue to optimize the experience of using SeaTunnel so that more people are willing to use it.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-future-of-apache-seatunnel">The future of Apache SeaTunnel<a href="#the-future-of-apache-seatunnel" class="hash-link" aria-label="Direct link to The future of Apache SeaTunnel" title="Direct link to The future of Apache SeaTunnel">â€‹</a></h2><p>As an Apache incubator project, the Apache SeaTunnel community is developing rapidly. In the following community planning, we will focus on four directions:</p><p>Support more data integration scenarios (Apache SeaTunnel Engine)
It is used to solve the pain points that existing engines cannot solve, such as the synchronization of the entire database, the synchronization of table structure changes, and the large granularity of task failure;</p><blockquote><p>Guys who are interested in the engine can pay attention to this Umbrella: <a href="https://github.com/apache/incubator-seatunnel/issues/2272" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/2272</a></p></blockquote><p>Expand and improve Connector &amp; Catalog ecology
Support more Connector &amp; Catalog, such as TiDB, Doris, Stripe, etc., and improve existing connectors, improve their usability and performance, etc.;
Support CDC connector for real-time incremental synchronization scenarios.</p><blockquote><p>Guys who are interested in connectors can pay attention to this Umbrella: <a href="https://github.com/apache/incubator-seatunnel/issues/1946" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues/1946</a></p></blockquote><p>Support for more versions of the engines
Such as Spark 3.x, Flink 1.14.x, etc.</p><blockquote><p>Guys who are interested in supporting Spark 3.3 can pay attention to this PR: <a href="https://github.com/apache/incubator-seatunnel/pull/2574" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pull/2574</a></p></blockquote><p>Easier to use (Apache SeaTunnel Web)
Provides a web interface to make operations more efficient in the form of DAG/SQL Simple and more intuitive display of Catalog, Connector, Job, etc.;
Access to the scheduling platform to make task management easier</p><blockquote><p>Guys who are interested in Web can pay attention to our Web sub-project: <a href="https://github.com/apache/incubator-seatunnel-web" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel-web</a></p></blockquote></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Bo Bi, data engineer at Mafengwo"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/During the joint Apache SeaTunnel &amp; IoTDB Meetup on October 15,">Mafengwo finally chose Apache SeaTunnel after analyzing these 9 points of how it works!</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-11-17T00:00:00.000Z" itemprop="datePublished">November 17, 2022</time> Â· <!-- -->19 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/16714322908857-c6ce9a962f477d13b5480206b583b6d2.jpg" width="720" height="480" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/16714322944041-351d2ac6aa565d636c97a1ad6b0c136e.jpg" width="360" height="309" class="img_ev3q">
Bo Bi, data engineer at Mafengwo</p><blockquote><p>During the joint Apache SeaTunnel &amp; IoTDB Meetup on October 15, Bo Bi, the data engineer at a leading Chinese travel-social e-commerce platform Mafengwo, introduced the basic principles of SeaTunnel and related enterprise practice thinking, the pain points and optimization thinking in typical scenarios of Mafengwoâ€™s big data development and scheduling platform, and shared his experience of participating in community contributions. We hope to help you understand SeaTunnel and the paths and skills of community building at the same time.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduction-to-the-technical-principle-of-seatunnel">Introduction to the technical principle of SeaTunnel<a href="#introduction-to-the-technical-principle-of-seatunnel" class="hash-link" aria-label="Direct link to Introduction to the technical principle of SeaTunnel" title="Direct link to Introduction to the technical principle of SeaTunnel">â€‹</a></h2><p>SeaTunnel is a distributed, high-performance data integration platform for the synchronization and transformation of large volumes of data (offline and real-time)</p><p>The diagram above shows the workflow of SeaTunnel, which in simple terms consists of 3 parts: input, transformation, and output; more complex data processing is just a combination of several actions.</p><p>In a synchronization scenario, such as importing Kafka to Elasticsearch, Kafka is the Source of the process and Elasticsearch is the Sink of the process.</p><p>If, during the import process, the field columns do not match the external data columns to be written and some column or type conversion is required, or if you need to join multiple data sources and then do some data widening, field expansion, etc., then you need to add some Transform in the process, corresponding to the middle part of the picture.</p><p><img loading="lazy" src="/assets/images/16714323322988-74b7a47d1a3299efd23c6375d3acaa5e.jpg" width="660" height="781" class="img_ev3q">
This shows that the core of SeaTunnel is the Source, Transform and Sink process definitions.</p><p>In Source we can define the data sources we need to read, in Sink, we can define the data pipeline and eventually write the external storage, and we can transform the data in between, either using SQL or custom functions.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-api-version-v1-architecture-breakdown">SeaTunnel Connector API Version V1 Architecture Breakdown<a href="#seatunnel-connector-api-version-v1-architecture-breakdown" class="hash-link" aria-label="Direct link to SeaTunnel Connector API Version V1 Architecture Breakdown" title="Direct link to SeaTunnel Connector API Version V1 Architecture Breakdown">â€‹</a></h2><p>For a mature component framework, there must be something unique about the design pattern of the API design implementation that makes the framework scalable.</p><p>The SeaTunnel architecture consists of three main parts.</p><p>1ã€SeaTunnel Basic API.</p><ol><li><p>the implementation of the SeaTunnel base API.</p></li><li><p>SeaTunnelâ€™s plug-in system.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-basic-api">SeaTunnel Basic API<a href="#seatunnel-basic-api" class="hash-link" aria-label="Direct link to SeaTunnel Basic API" title="Direct link to SeaTunnel Basic API">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714323668557-9c9260e0c54017b5282b1294ccc9c692.jpg" width="720" height="194" class="img_ev3q">
The above diagram shows the definition of the interface, the Plugin interface in SeaTunnel abstracts the various actions of data processing into a Plugin.</p><p>The five parts of the diagram below, Basesource, Basetransfform, Basesink, Runtimeenv, and Execution, all inherit from the Plugin interface.
<img loading="lazy" src="/assets/images/16714323741126-a61ed7b20a44b14b78d39c7ffe42ec41.jpg" width="720" height="229" class="img_ev3q"></p><p>As a process definition plug-in, Source is responsible for reading data, Transform is responsible for transforming, Sink is responsible for writing and Runtimeenv is setting the base environment variables.</p><p>The overall SeaTunnel base API is shown below</p><p><img loading="lazy" src="/assets/images/16714323846302-eabb8409469fa34d9b0ebd2402fca23d.jpg" width="720" height="347" class="img_ev3q">
Execution, the data flow builder used to build the entire data flow based on the first three, is also part of the base API</p><p><img loading="lazy" src="/assets/images/16714323920717-b5bb28d92939847f1f30c61b6895191a.jpg" width="720" height="192" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-base-api-implementation">SeaTunnel Base API Implementation<a href="#seatunnel-base-api-implementation" class="hash-link" aria-label="Direct link to SeaTunnel Base API Implementation" title="Direct link to SeaTunnel Base API Implementation">â€‹</a></h2><p>Based on the previous basic APIs, SeaTunnel has been implemented in separate packages for different computing engines, currently the Spark API abstraction and the Flink API abstraction, which logically completes the process of building the data pipeline.</p><p><img loading="lazy" src="/assets/images/16714323741126-a61ed7b20a44b14b78d39c7ffe42ec41.jpg" width="720" height="229" class="img_ev3q"></p><p>Due to space constraints, we will focus on Spark batch processing. Based on the wrapped implementation of the previous base Api, the first is that Base spark source implements Base source, base Spark transform implements Base transform and Base Spark sink implements Base sink.</p><p>The method definition uses Sparkâ€™s Dataset as the carrier of the data, and all data processing is based on the Dataset, including reading, processing and exporting.</p><p>The SparkEnvironment, which internally encapsulates Sparkâ€™s Sparksession in an Env, makes it easy for individual plugins to use.</p><p><img loading="lazy" src="/assets/images/16714324136843-44e65c36b7ef55c34b50a9eb0e43c3cc.jpg" width="720" height="350" class="img_ev3q"></p><p>The Spark batch process ends with SparkBatchExecution (the data stream builder), which is the core code snippet used to functionally build our data stream Pipeline, the most basic data stream on the left in the diagram below.</p><p>The user-based definition of each process component is also the configuration of Source Sink, Transform. More complex data flow logic can be implemented, such as multi-source Join, multi-pipeline processing, etc., all of which can be built through Execution.</p><p><img loading="lazy" src="/assets/images/16714324237449-e5a12e608045d5a153853c93eb844852.jpg" width="720" height="405" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v1-api-architecture-summary">SeaTunnel Connector V1 API Architecture Summary<a href="#seatunnel-connector-v1-api-architecture-summary" class="hash-link" aria-label="Direct link to SeaTunnel Connector V1 API Architecture Summary" title="Direct link to SeaTunnel Connector V1 API Architecture Summary">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714324336701-6e5cadce0d1a1858d7bffbf96c5cae82.jpg" width="720" height="405" class="img_ev3q">
SeaTunnelâ€™s API consists of three main parts.</p><p>The first part is the SeaTunnel base API, which provides the basic abstract interfaces such as Source, Sink, Transform, and Plugin.</p><p>The second part is based on a set of interfaces Transform, Sink, Source, Runtime, and Execution provided by the SeaTunnel base API, which is wrapped and implemented on the Flink and Spark engines respectively, i.e. Spark engine API layer abstraction and Flink engine API layer abstraction.</p><p>Both Flink and Spark engines support stream and batch processing, so there are different ways to use streams/batches under the Flink API abstraction and Spark abstraction APIs, such as Flinkstream and Flinkbatch under the Flink abstraction API, and Sparkbatch and Sparkstreaming under the Spark abstraction API.</p><p>The third part is the plug-in system, based on Spark abstraction and Flink API abstraction, SeaTunnel engine implements rich connectors and processing plug-ins, while developers can also be based on different engine API abstractions, and extensions to achieve their own Plugin.</p><p>SeaTunnel Implementation Principle
Currently, SeaTunnel offers a variety of ways to use Flink, Spark, and FlinkSQL. Due to space limitations, we will introduce the execution principles of the Spark method.</p><p>First, the entry starts the command Start-seatunnel-spark.sh via the shell, which internally calls Sparkstarterâ€™s Class, which parses the parameters passed by the shell script, and also parses the Config file to determine which Connectors are defined in the Config file, such as Fake, Console, etc.
<img loading="lazy" src="/assets/images/16714324454477-200fd76badcfc17bdd291f364c70a191.jpg" width="720" height="405" class="img_ev3q">
Then find the Connector path from the Connector plugin directory and stitch it into the Spark-submit launch command with â€” jar, so that the found Plugin jar package can be passed to the Spark cluster as a dependency.</p><p>For Connector plugins, all Spark Connectors are packaged in the plugin directory of the distribution (this directory is managed centrally).</p><p>After Spark-submit is executed, the task is submitted to the Spark cluster, and the Main class of the Spark jobâ€™s Driver builds the data flow Pipeline through the data flow builder Execution, combined with Souce, Sink, and Transform so that the whole chain is connected.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v2-api-architecture">SeaTunnel Connector V2 API Architecture<a href="#seatunnel-connector-v2-api-architecture" class="hash-link" aria-label="Direct link to SeaTunnel Connector V2 API Architecture" title="Direct link to SeaTunnel Connector V2 API Architecture">â€‹</a></h2><p>In the latest community release of SeaTunnel 2.2.0-beta, the refactoring of the Connectorapi, now known as the SeaTurnelV2 API, has been completed!</p><p>Why do we need to reconfigure?</p><p>As the Container is currently a strongly coupled engine, i.e. Flink and Spark API, if the Flink or Spark engine is upgraded, the Connector will also have to be adjusted, possibly with changes to parameters or interfaces.</p><p>This can lead to multiple implementations for different engines and inconsistent parameters to develop a new Connector. Therefore, the community has designed and implemented the V2 version of the API based on these pain points.</p><p><img loading="lazy" src="/assets/images/16714324726276-11d9d5c6d4e848796fa71329819caa72.jpg" width="720" height="405" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-v2-api-architecture">SeaTunnel V2 API Architecture<a href="#seatunnel-v2-api-architecture" class="hash-link" aria-label="Direct link to SeaTunnel V2 API Architecture" title="Direct link to SeaTunnel V2 API Architecture">â€‹</a></h2><p>SeaTunnel V2 API Architecture</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1table-api">1.Table API<a href="#1table-api" class="hash-link" aria-label="Direct link to 1.Table API" title="Direct link to 1.Table API">â€‹</a></h3><p>Â·DataType: defines SeaTunnelâ€™s data structure SeaTunnelRow, which is used to isolate the engine</p><p>Â·Catalog: used to obtain Table Scheme, Options, etc..</p><p>Â·Catalog Storage: used to store user-defined Table Schemes etc. for unstructured engines such as Kafka.</p><p>Â·Table SPI: mainly used to expose the Source and Sink interfaces as an SPI</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-source--sink-api">2. Source &amp; Sink API<a href="#2-source--sink-api" class="hash-link" aria-label="Direct link to 2. Source &amp; Sink API" title="Direct link to 2. Source &amp; Sink API">â€‹</a></h3><p>Define the Connectorâ€™s core programming interface for implementing the Connector</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3engine-api">3.Engine API<a href="#3engine-api" class="hash-link" aria-label="Direct link to 3.Engine API" title="Direct link to 3.Engine API">â€‹</a></h3><p>Â·Translation: The translation layer, which translates the Source and Sink APIs implemented by the Connector into a runnable API inside the engine.</p><p>Â·Execution: Execution logic, used to define the execution logic of Source, Transform, Sink and other operations within the engine.</p><p>The Source &amp; Sink API is the basis for the implementation of the connector and is very important for developers.</p><p>The design of the v2 Source &amp; Sink API is highlighted below</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v2-source-api">SeaTunnel Connector V2 Source API<a href="#seatunnel-connector-v2-source-api" class="hash-link" aria-label="Direct link to SeaTunnel Connector V2 Source API" title="Direct link to SeaTunnel Connector V2 Source API">â€‹</a></h2><p>The current version of SeaTunnelâ€™s API design draws on some of Flinkâ€™s design concepts, and the more core classes of the Source API are shown below.</p><p><img loading="lazy" src="/assets/images/16714325444078-fbed659c615c445655896b93a093177f.jpg" width="720" height="405" class="img_ev3q">
<img loading="lazy" src="/assets/images/16714325474972-48cbee3672ad2f32317d48263f204978.jpg" width="720" height="405" class="img_ev3q">
The core Source API interaction flow is shown above. In the case of concurrent reads, the enumerator SourceSplitEnumerator is required to split the task and send the SourceSplit down to the SourceReader, which receives the split and uses it to read the external data source.</p><p>In order to support breakpoints and Eos semantics, it is necessary to preserve and restore the state, for example by preserving the current Readerâ€™s Split consumption state and restoring it after a failure in each Reader through the Checkpoint state and Checkpoint mechanism, so that the data can be read from the place where it failed.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-connector-v2-sink-api">SeaTunnel Connector V2 Sink API<a href="#seatunnel-connector-v2-sink-api" class="hash-link" aria-label="Direct link to SeaTunnel Connector V2 Sink API" title="Direct link to SeaTunnel Connector V2 Sink API">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714325600316-f066630b909ec30a7727b7877a4838b0.jpg" width="720" height="405" class="img_ev3q">
The overall Sink API interaction flow is shown in the diagram below. The SeaTunnel sink is currently designed to support distributed transactions, based on a two-stage transaction commit.</p><p>First SinkWriter continuously writes data to an external data source, then when the engine does a checkpoint, it triggers a first-stage commit.</p><p>SinkWriter needs to do a Prepare commit, which is the first stage of the commit.</p><p>The engine will determine if all the Writer&#x27;s first stage succeeds, and if they all succeed, the engine will combine the Subtaskâ€™s Commit info with the Commit method of the Committer to do the actual commit of the transaction and operate the database for the Commit, i.e. the second stage of the commit. This is the second stage of commit.</p><p><img loading="lazy" src="/assets/images/16714325681738-973c53bd86223df4ee98cbb2ecb30eaf.jpg" width="570" height="814" class="img_ev3q">
For the Kafka sink connector implementation, the first stage is to do a pre-commit by calling KafkaProducerSender.prepareCommit().</p><p>The second commit is performed via Producer.commitTransaction();.</p><p>flush(); flushes the data from the Brokerâ€™s system cache to disk.</p><p>Finally, it is worth noting!</p><p>Both SinkCommitter and SinkAggregatedCommitter can perform a second stage commit to replace the Committer in the diagram. The difference is that SinkCommitter can only do a partial commit of a single Subtaskâ€™s CommitInfo, which may be partially successful and partially unsuccessful, and cannot be handled globally. The difference is that the SinkCommitter can only do partial commits of a single Subtaskâ€™s CommitInfo, which may be partially successful and partially unsuccessful.</p><p>SinkAggregatedCommitter is a single parallel, aggregating the CommitInfo of all Subtask, and can do the second stage commit as a whole, either all succeed or all fail, avoiding the problem of inconsistent status due to partial failure of the second stage.</p><p>It is therefore recommended that the SinkAggregatedCommitter be used in preference.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-of-seatunnel-v1-and-v2-api-processing-flows">Comparison of SeaTunnel V1 and V2 API processing flows<a href="#comparison-of-seatunnel-v1-and-v2-api-processing-flows" class="hash-link" aria-label="Direct link to Comparison of SeaTunnel V1 and V2 API processing flows" title="Direct link to Comparison of SeaTunnel V1 and V2 API processing flows">â€‹</a></h2><p>We can look at the changes before and after the V1 V2 upgrade from a data processing perspective, which is more intuitive, Spark batch processing as an example: SeaTunnel V1: The entire data processing process is based on the Spark dataset API, and the Connector and the compute engine are strongly coupled.</p><p><img loading="lazy" src="/assets/images/16714325887598-d27009789ff28e56c8bfcca29bcedfe1.jpg" width="720" height="405" class="img_ev3q">
SeaTunnel V2: Thanks to the work of the engine translator, the Connector API, and the SeaTunnelRow, the data source of the SeaTunnel internal data structures accessed through the Connector, are translated by the translation layer into a runnable Spark API and spark dataset that is recognized inside the engine during data transformation.</p><p>As data is written out, the Spark API and Spark dataset are translated through the translation layer into an executable connector API inside the SeaTunnel connector and a data source of internal SeaTunnel structures that can be used.</p><blockquote><p>Overall, the addition of a translation layer at the API and compute engine layers decouples the Connector API from the engine, and the Connector implementation no longer depends on the compute engine, making the extension and implementation more flexible.</p></blockquote><blockquote><p>In terms of community planning, the V2 API will be the main focus of development, and more features will be supported in V2, while V1 will be stabilized and no longer maintained.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="practice-and-reflections-on-our-off-line-development-scheduling-platform">Practice and reflections on our off-line development scheduling platform<a href="#practice-and-reflections-on-our-off-line-development-scheduling-platform" class="hash-link" aria-label="Direct link to Practice and reflections on our off-line development scheduling platform" title="Direct link to Practice and reflections on our off-line development scheduling platform">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="practice-and-reflections-on-our-off-line-development-scheduling-platform-1">Practice and reflections on our off-line development scheduling platform<a href="#practice-and-reflections-on-our-off-line-development-scheduling-platform-1" class="hash-link" aria-label="Direct link to Practice and reflections on our off-line development scheduling platform" title="Direct link to Practice and reflections on our off-line development scheduling platform">â€‹</a></h3><p><img loading="lazy" src="/assets/images/16714326227360-bcd55d2c5b7b23ec91a5d1e27c04fb0e.jpg" width="720" height="405" class="img_ev3q">
Hornetâ€™s Nest Big Data Development Platform, which focuses on providing one-stop big data development and scheduling services, helps businesses solve complex problems such as data development management, task scheduling and task monitoring in offline scenarios.</p><p>The offline development and scheduling platform plays the role of the top and the bottom. The top is to provide open interface API and UI to connect with various data application platforms and businesses, and the bottom is to drive various computations and storage, and then run in an orderly manner according to the task dependency and scheduling time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="platform-capabilities">Platform Capabilities<a href="#platform-capabilities" class="hash-link" aria-label="Direct link to Platform Capabilities" title="Direct link to Platform Capabilities">â€‹</a></h2><p><strong>Data development</strong></p><p>Task configuration, quality testing, release live</p><p><strong>Â·Data synchronisation</strong></p><p>Data access, data processing, data distribution</p><p><strong>Â·Scheduling capabilities</strong></p><p>Supports timed scheduling, triggered scheduling</p><p><strong>Â·Operations and Maintenance Centre
</strong>
Job Diagnosis, Task O&amp;M, Instance O&amp;M</p><p><strong>Â·Management</strong></p><p>Library table management, permission management, API management, script management</p><p>In summary, the core capabilities of the offline development scheduling platform are openness, versatility, and one-stop shopping. Through standardized processes, the entire task development cycle is managed and a one-stop service experience is provided.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-architecture-of-the-platform">The architecture of the platform<a href="#the-architecture-of-the-platform" class="hash-link" aria-label="Direct link to The architecture of the platform" title="Direct link to The architecture of the platform">â€‹</a></h2><p><img loading="lazy" src="/assets/images/16714326749427-2957d8414a3175ed8cc46bde32a08565.jpg" width="720" height="405" class="img_ev3q">
The Hornetâ€™s Nest Big Data Development and Scheduling Platform consists of four main modules: the task component layer, the scheduling layer, the service layer, and the monitoring layer.</p><p>The service layer is mainly responsible for job lifecycle management (e.g. job creation, testing, release, offline); Airflow dagphthon file building and generating, task bloodline dependency management, permission management, API (providing data readiness, querying of task execution status).</p><p>The scheduling layer is based on Airflow and is responsible for the scheduling of all offline tasks.</p><p>A task component layer that enables users to develop data through supported components that include tools such as SparkSQL/, HiveSQ, LMR), StarRocks import, etc., directly interfacing with underlying HDFS, MySQL, and other storage systems.</p><p>The monitoring layer is responsible for all aspects of monitoring and alerting on scheduling resources, computing resources, task execution, etc.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="open-data-sync-capability-scenarios">Open Data Sync Capability Scenarios<a href="#open-data-sync-capability-scenarios" class="hash-link" aria-label="Direct link to Open Data Sync Capability Scenarios" title="Direct link to Open Data Sync Capability Scenarios">â€‹</a></h2><p>Challenges with open capabilities: Need to support multiple business scenarios and meet flexible data pipeline requirements (i.e. extend to support more task components such as hive2clickhourse, clickhourse2mysql, etc.)</p><p>Extending task components based on Airflow: higher maintenance costs for extensions, need to reduce costs and increase efficiency (based on the limited provider&#x27;s Airflow offers, less applicable in terms of usage requirements, Airflow is a Python technology stack, while our team is mainly based on the Java technology stack, so the technology stack difference brings higher iteration costs)</p><p>Self-developed task components: the high cost of platform integration, long development cycle, high cost of the configuration of task components. (Research or implement task components by yourself, different ways of adapting the parameters of the components in the service layer, no uniform way of parameter configuration)</p><p>We wanted to investigate a data integration tool that, firstly, supported a rich set of components, provided out-of-the-box capabilities, was easy to extend, and offered a uniform configuration of parameters and a uniform way of using them to facilitate platform integration and maintenance.</p><ul><li>Selection of data integration tools
<img loading="lazy" src="/assets/images/16714327002726-6bfd742beb9534e7fdbd917db5f53d51.jpg" width="720" height="405" class="img_ev3q">
To address the pain points mentioned above, we actively explored solutions and conducted a selection analysis of several mainstream data integration products in the industry. As you can see from the comparison above, Datax and SeaTunnel both offer good scalability, and high stability, support rich connector plugins, provide scripted, uniformly configurable usage, and have active communities.</li></ul><p>However, Datax is limited by being distributed and is not well suited to massive data scenarios.</p><p>In contrast, SeaTunnel offers the ability to provide distributed execution, distributed transactions, scalable levels of data handling, and the ability to provide a unified technical solution in data synchronization scenarios.</p><p>In addition to the advantages and features described above and the applicable scenarios, more importantly, the current offline computing resources for big data are unified and managed by yarn, and for the subsequently extended tasks we also wish to execute on Yarn, we finally prefer SeaTunnel for our usage scenarios.</p><p>Further performance testing of SeaTunnel and the development of an open data scheduling platform to integrate SeaTunnel may be carried out at a later stage, and its use will be rolled out gradually.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="outbound-scenario-hive-data-sync-to-starrocks">Outbound scenario: Hive data sync to StarRocks<a href="#outbound-scenario-hive-data-sync-to-starrocks" class="hash-link" aria-label="Direct link to Outbound scenario: Hive data sync to StarRocks" title="Direct link to Outbound scenario: Hive data sync to StarRocks">â€‹</a></h2><p>To briefly introduce the background, the Big Data platform has now completed the unification of the OLAP engine layer, using the StarRocks engine to replace the previous Kylin engine as the main query engine in OLAP scenarios.</p><p>In the data processing process, after the data is modelled in the data warehouse, the upper model needs to be imported into the OLAP engine for query acceleration, so there are a lot of tasks to push data from Hive to StarRocks every day. task (based on a wrapper for the StarRocks Broker Load import method) to a StarRocks-based table.</p><p>The current pain points are twofold.</p><p>Â·Long data synchronization links: Hive2StarRocks processing links, which require at least two tasks, are relatively redundant.</p><p>Â·Outbound efficiency: From the perspective of outbound efficiency, many Hive models themselves are processed by Spark SQL, and based on the processing the Spark Dataset in memory can be pushed directly to StarRocks without dropping the disk, improving the modelâ€™s regional time.</p><p><img loading="lazy" src="/assets/images/16714327218590-2644dc4ad1179eab81d40fc774d970e9.jpg" width="720" height="405" class="img_ev3q">
StarRocks currently also supports Spark Load, based on the Spark bulk data import method, but our ETL is more complex, needs to support data conversion multi-table Join, data aggregation operations, etc., so temporarily can not meet.</p><p>We know from the SeaTunnel community that there are plans to support the StarRocks Sink Connector, and we are working on that part as well, so we will continue to communicate with the community to build it together later.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-involved-in-community-building">How to get involved in community building<a href="#how-to-get-involved-in-community-building" class="hash-link" aria-label="Direct link to How to get involved in community building" title="Direct link to How to get involved in community building">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-community-contribution">SeaTunnel Community Contribution<a href="#seatunnel-community-contribution" class="hash-link" aria-label="Direct link to SeaTunnel Community Contribution" title="Direct link to SeaTunnel Community Contribution">â€‹</a></h3><p>As mentioned earlier, the community has completed the refactoring of the V1 to V2 API and needs to implement more connector plug-ins based on the V2 version of the connector API, which I was lucky enough to contribute to.</p><p>I am currently responsible for big data infrastructure work, which many mainstream big data components big data also use, so when the community proposed a connector issue, I was also very interested in it.</p><p>As the platform is also investigating SeaTunnel, learning and being able to contribute pr to the community is a great way to learn about SeaTunnel.</p><p>I remember at first I proposed a less difficult pr to implement the WeChat sink connector, but in the process of contributing I encountered many problems, bad coding style, code style did not take into account the rich output format supported by the extension, etc. Although the process was not so smooth, I was really excited and accomplished when the pr was merged. Although the process was not so smooth, it was very exciting and rewarding when the pr was merged.</p><p>As I became more familiar with the process, I became much more efficient at submitting pr and was confident enough to attempt difficult issues.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-involved-in-community-contributions-quickly">How to get involved in community contributions quickly<a href="#how-to-get-involved-in-community-contributions-quickly" class="hash-link" aria-label="Direct link to How to get involved in community contributions quickly" title="Direct link to How to get involved in community contributions quickly">â€‹</a></h3><ul><li>Good first issue
Good first issue #3018 #2828</li></ul><p>If you are a first-time community contributor, it is advisable to focus on the Good first issue first, as it is basically a relatively simple and newcomer-friendly issue.</p><p>Through Good first issue, you can get familiar with the whole process of participating in the GitHub open source community contribution, for example, first fork the project, then submit the changes, and finally submit the pull request, waiting for the community to review, the community will target to you to put forward some suggestions for improvement, directly will leave a comment below, until when your pr is merged in, this will have completed a comp</p><ul><li>Subscribe to community mailings
Once youâ€™re familiar with the pr contribution process, you can subscribe to community emails to keep up to date with whatâ€™s happening in the community, such as what features are currently being worked on and whatâ€™s planned for future iterations. If youâ€™re interested in a feature, you can contribute to it in your own situation!</li><li>Familiarity with git use
The main git commands used in development are git clone, git pull, git rebase and git merge. git rebase is recommended in the community development specification and does not generate additional commits compared to git merge.</li><li>Familiarity with GitHub project collaboration process
Open source projects are developed collaboratively by multiple people, and the collaboration method on GitHub is at its core outlined in fork For example, the apache st project, which is under the apache space, is first forked to our own space on GitHub</li></ul><p>Then modify the implementation, mention a pull request, and submit the pull request to be associated with the issue, in the commit, if we change a long time, in the upward commit, then the target branch has a lot of new commits exhausted this time we need to do a pull&amp; merge or rebase.</p><ul><li>Source code compilation project
It is important to be familiar with source compilation, as local source compilation can prove that the code added to a project can be compiled, and can be used as a preliminary check before committing to pr. Source compilation is generally slow and can be speeded up by using mvn -T for multi-threaded parallel compilation.</li><li>Compilation checks
Pre-compilation checks, including Licence header, Code checkstyle, and Document checkstyle, will be checked during Maven compilation, and if they fail, the CI will not be passed. So it is recommended to use some plug-in tools in the idea to improve the efficiency, such as Code checkstyle has a plug-in to automatically check the code specification, Licence header can add code templates in the idea, these have been shared by the community before how to do!</li><li>Add full E2E</li></ul><p>Add full E2E testing and ensure that the E2E is passed before the Pull request.</p><p>Finally, I hope more students will join the SeaTunnel community, where you can not only feel the open-source spirit and culture of Apache but also understand the management process of Apache projects and learn good code design ideas.</p><p>We hope that by working together and growing together, we can build SeaTunnel into a top-notch data integration platform.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Upcoming API Connector Development Analysis">Upcoming API Connector Development Analysis</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-06-23T00:00:00.000Z" itemprop="datePublished">June 23, 2022</time> Â· <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><p>After days of community development, the preliminary development of the new Connector API of SeaTunnel is completed. The next step is to adapt this new connector. In order to aid the developers to use this connector, this article provides guide to develop a new API.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="priliminary-setup">Priliminary Setup<a href="#priliminary-setup" class="hash-link" aria-label="Direct link to Priliminary Setup" title="Direct link to Priliminary Setup">â€‹</a></h2><ul><li><p>Environment configuration: JDK8 and Scala2.11 are recommended.</p></li><li><p>As before, we need to download the latest code locally through git and import it into the IDE, project address: <a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel</a> . At the same time, switch the branch to api-draft, and currently use this branch to develop the new version of the API and the corresponding Connector. The project structure is as follows:</p><p><img loading="lazy" alt="Project Structure" src="/assets/images/0-82b23e9c80c8b70ce10feaccfc96a2a6.png" width="583" height="1014" class="img_ev3q"></p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="prerequisites">Prerequisites<a href="#prerequisites" class="hash-link" aria-label="Direct link to Prerequisites" title="Direct link to Prerequisites">â€‹</a></h2><ul><li><p>At present, in order to distinguish different Connectors, we put the connectors that support</p><ul><li>Flink/Spark under the <code>seatunnel-connectors/seatunnel-connectors-flink(spark)</code> module.</li><li>New version of the Connector is placed under the <code>seatunnel-connectors/seatunnel-connectors-seatunnel</code> module.</li></ul><p>As we can see from the above figure, we have implemented Fake, Console, Kafka Connector, and Clickhouse Connector is also being implemented.</p></li><li><p>At present, the data type we support is SeaTunnelRow, so no matter the type of data generated by the Source or the type of data consumed by the Sink, it should be SeaTunnelRow.</p></li></ul><h1>Development of Connector</h1><p>Taking Fake Connector as an example, let&#x27;s introduce how to implement a new Connector:</p><ul><li><p>Create a corresponding module with a path under <code>seatunnel-connectors-seatunnel</code>, which is at the same level as other new connectors.</p></li><li><p>Modify the <code>seatunnel-connectors-seatunnel/pom.xml</code> file, add a new module to modules, modify <code>seatunnel-connectors-seatunnel/seatunnel-connector-seatunnel-fake/pom.xml</code>, add seatunnel-api dependencies, and correct parent Quote. The resulting style is as follows:</p><p><img loading="lazy" alt="Style" src="/assets/images/1-27a269d360e9ee05b1dd696eeb0aa8e4.png" width="949" height="568" class="img_ev3q"></p></li><li><p>The next step is to create the corresponding package and related classes, create FakeSource, and need to inherit SeaTunnel Source.</p><ul><li>Note : The Source of SeaTunnel adopts the design of stream and batch integration. The Source of SeaTunnel determines whether current Source is a stream or batch through attribute getBoundedness.</li></ul><p>So you can specify a Source as a stream or batch by dynamic configuration (refer to the default method). The configuration defined by the user in the configuration file can be obtained through the prepare method to realize the customized configuration.</p><p>Then create FakeSourceReader, FakeSource SplitEnumerator, and FakeSourceSplit to inherit the corresponding abstract classes (which can be found in the corresponding classes). As long as we implement the corresponding methods of these classes, then our SeaTunnel Source Connector is basically completed.</p></li><li><p>Next, just follow the existing example to write the corresponding code. The most important one is the FakeSource Reader, which defines how we obtain data from the outside, which is the most critical part of the Source Connector. Every time a piece of data is generated, we need to place it in the collector as shown:</p><p><img loading="lazy" alt="Source" src="/assets/images/2-6e56482ef5f497868040295fe7edff23.png" width="935" height="424" class="img_ev3q"></p></li><li><p>After the code development is complete, we need to configure the configuration file <code>plugin-mapping.properties</code> located under <code>seatunnel-connectors/modules</code>. Adding a seatunnel
<code>.source.FakeSource = seatunnel-connector-fake</code>
means that SeaTunnel can find the jar package corresponding to the project by looking for a Source named FakeSource. This allows the Connector to be used in the normal configuration file.</p></li><li><p>For a detailed description of writing Source and Sink and SeaTunnel API, please refer to the introduction at <code>seatunnel-connectors/seatunnel-connectors-seatunnel/ README.zh.md</code>.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="connector-testing">Connector Testing<a href="#connector-testing" class="hash-link" aria-label="Direct link to Connector Testing" title="Direct link to Connector Testing">â€‹</a></h2><ul><li>For testing, we can find the <code>seatunnel-flink(spark)-new-connector-example</code> module in seatunnel-examples, and test it against different engines to ensure that the performance of the Connector is as consistent as possible. If you find any discrepancies, you can mark them in the document, modify the configuration file under resource, add our Connector to the configuration, and introduce <code>seatunnel-flink(spark)-new-connector-example/pom.xml</code> dependency, you can execute <code>SeaTunnelApiExample</code> to test.</li><li>The default is stream processing mode, and the execution mode is switched to batch mode by modifying <code>job.mode=BATCH</code> in the environment of the configuration file.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="submit-pr">Submit PR<a href="#submit-pr" class="hash-link" aria-label="Direct link to Submit PR" title="Direct link to Submit PR">â€‹</a></h2><p>When our Connector is ready, we can submit PR to github. After reviewing by other partners, our contributed Connector will become part of SeaTunnel!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Translator | Critina"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/How dose Apache SeaTunnel refactor the API to decouple with the computing engine">How dose Apache SeaTunnel refactor the API to decouple with the computing engine</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-31T00:00:00.000Z" itemprop="datePublished">May 31, 2022</time> Â· <!-- -->12 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-1b0b7ccac22c107239f10a37321c7719.jpg" width="1920" height="1316" class="img_ev3q"></p><p>Translator | Critina</p><p>In the May joint Meetup between Apache SeaTunnel and Apache Inlong, Li Zongwen, a senior engineer at WhaleOps, shared his experiences about finding and refactoring of the the four major problems with Apache SeaTunnel (Incubating).i.e. the connectors of SeaTunnel have to be implemented many times,the inconsistent parameters, SeaTunnel is not supportive of multiple versions of the engine, and itâ€™s difficult to upgrade the engine. In order to solve these problems, Li Zongwen aimed to decouple Apache SeaTunnel (Incubating) from thw computing engines, and re-factor the Source and Sink apis to improve the development experience.</p><p>This speech mainly consists of five parts.The first part is about Apache SeaTunnel (Incubator) refactoring background and motivation. The second part introduces Apache SeaTunnel (Incubating) Target for refactoring.The third part discusses Apache SeaTunnel (Incubating) overall design for refactoring. The last two parts is about Apache SeaTunnel (Incubating) Source API design and Apache SeaTunnel (Incubating) Sink API design.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-background-and-motivation-for-refactoring">01 Background and motivation for refactoring<a href="#01-background-and-motivation-for-refactoring" class="hash-link" aria-label="Direct link to 01 Background and motivation for refactoring" title="Direct link to 01 Background and motivation for refactoring">â€‹</a></h2><p>Those of you who have used Apache SeaTunnel (Incubator) or developers should know that Apache SeaTunnel (Incubator) is now fully coupled with the engine, which is entirely based on Spark or Flink, and so are the configuration file parameters. From the perspective of contributors and users, we can find they face some problems.</p><p>In the view of the contributors, repeated implementing connector is meaningless and it is unable for potential contributors to contribute to the community due to inconsistent engine versions.</p><p>At present, many companies use Lambda architecture, Spark is used for offline operations and Flink is used for real-time operations. In the view of the users, it can be found that Spark may have the Connector of SeaTunnel, but Flink does not, and the parameters of the two engines for the Connector of the same storage engine are not unified, thus resulting a high cost of and deviating from its original intention of being easy to use. And some users question that Flink version 1.14 is not supported nowadays. While with the current SeaTunnel architecture, we must discard the previous version in order to support Flink version 1.14, which will bring great trouble for early version users.</p><p>As a result, it was difficult for us to either upgrade engine or support more versions.</p><p>In addition, Spark and Flink both adopt the Checkpoint fault-tolerant mechanism implemented by Chandy-Lamport algorithm and internally unify DataSet and DataStream. On this premise, we believe decoupling is feasible.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-apache-seatunnel-incubating-decouples-with-computing-engine">02 Apache SeaTunnel (Incubating) decouples with computing engine<a href="#02-apache-seatunnel-incubating-decouples-with-computing-engine" class="hash-link" aria-label="Direct link to 02 Apache SeaTunnel (Incubating) decouples with computing engine" title="Direct link to 02 Apache SeaTunnel (Incubating) decouples with computing engine">â€‹</a></h2><p>Therefore, in order to solve the problems raised above, we set the following goals.</p><ol><li><p>Connector is only implemented once. To solve the problems that parameters are not unified and Connector is implemented for too many times, we hope to achieve a unified Source and Sink API;</p></li><li><p>Multiple versions of Spark and Flink engines are supported. A translation layer above the Source and Sink API is added to support multiple versions of Spark and Flink engines.</p></li><li><p>The logic for parallel shard of Source and the Sink submission should be clarified. We must provide a good API to support Connector development.</p></li><li><p>The full database synchronization in real-time scenarios should be supported. This is a derivative requirement that many users have mentioned for CDC support. I once participated the Flink CDC community before and many users pointed out that in the CDC scenario, if you wanted to use the Flink CDC directly, each table would have a link and there would be thousands of links for thousands of tables when you need to synchronize the whole library, which was unacceptable for both the database and the DBA. To solve this problem, the simplest way was to introduce Canalã€Debezium or other components, which were used to pull incremental data to Kafka or other MQ for intermediate storage, and then we could use Flink SQL for synchronization. This actually contradicted the original idea of the Flink CDC to reduce links. However, the Flink CDC aimed only a Connector and was unable to deal with the whole link, so the proposal was not seen in the SeaTunnel community. By the chance of the reconstruction, we submitted the proposal to the SeaTunnel community.</p></li><li><p>Automatic discovery and storage of meta information are realized. The users should have awful experience due to the storage engines such as Kafka lacking of record of the data structure, when we need to read structured data, the user must define the topic of structured data types before read one topic at a time . We hope once the configuration is completed, there is no need to do any redundant work again.</p></li></ol><p>Some people may wonder why we donâ€™t use Apache Beam directly. That is because Beam sources are divided into BOUNDED and UNBOUNDED sources, which means it needs to be implemented twice. Moreover, some features of Source and Sink are not supported, which will be mentioned later.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-apache-seatunnelincubating-overall-design-for-refactoring">03 Apache SeaTunnel(Incubating) overall design for refactoring<a href="#03-apache-seatunnelincubating-overall-design-for-refactoring" class="hash-link" aria-label="Direct link to 03 Apache SeaTunnel(Incubating) overall design for refactoring" title="Direct link to 03 Apache SeaTunnel(Incubating) overall design for refactoring">â€‹</a></h2><p><img loading="lazy" src="/assets/images/1-cbc169e3bc121f1008b807103abe9fd8.jpg" width="1440" height="810" class="img_ev3q"></p><p>The Apache SeaTunnel(Incubating) API architecture is described in the picture above.</p><p>The Source &amp; Sink API is one of the core APIS of data integration, which defines the logic for parallel shard of Source and the commitment of Sink to realize the Connector.</p><p>The Engine API includes the translation and the execution layers. The translation is used to translate Souce and Sink API of SeaTunnel into connectors that can be run inside the engine.</p><p>The execution defines the execution logic of Source, Transform, Sink and other operations in the engine.</p><p>The Table SPI is mainly used to expose the interface of Source and Sink in SPI mode, and to specify mandatory and optional parameters of Connector etc.</p><p>The DataType includes SeaTunnel data structure used to isolate engines and declare Table schema.</p><p>The Catalog is Used to obtain Table schemes and Options, etc. The Catalog Storage is used to store Table Schemes defined by unstructured engines such as Kafka.</p><p><img loading="lazy" src="/assets/images/2-507471aef3b2e7dc6ee4bc03188bc784.jpg" width="1440" height="810" class="img_ev3q"></p><p>The execution flow we assumed nowadays can be see in the above picture.</p><ol><li><p>Obtain task parameters from configuration files or UI.</p></li><li><p>Obtain the Table Schema, Option and other information by analyzing the parameters from Catalog.</p></li><li><p>Pull up the Connector of SeaTunnel in SPI mode and inject Table information.</p></li><li><p>Translate the Connector from SeaTunnel into the Connector within the engine.</p></li><li><p>Execute the operation logic of the engine. The multi-table distribution in the picture only exists in the synchronization of the whole database of CDC, while other connectors are single tables and do not need the distribution logic.</p></li></ol><p>It can be seen that the hardest part of the plan is to translate Source and Sink into an internal Source and Sink in the engine.</p><p>Many users today use Apache SeaTunnel (Incubating) not only as a data integration tool but also as a data storage tool, and use a lot of Spark and Flink SQLs. We want to preserve that SQL capability for users to upgrade seamlessly.</p><p><img loading="lazy" src="/assets/images/3-68113e215aa8e91a5d2469ccb37a3c22.jpg" width="1440" height="810" class="img_ev3q"></p><p>According to our research, the feature above shows the ideal execution logic of Source and Sink. Since SeaTunnel is incubated as WaterDrop, the terms in the figure are tended towards Spark.</p><p>Ideally, the Source and Sink coordinators can be run on the Driver, and the Source Reader and Sink Writer will run on the Worker. In terms of the Source Coordinator, we expect it to support several features.</p><p>The first capability is that the slicing logic of data can be dynamically added to the Reader.</p><p>The second is that the coordination of Reader can be supported. Source Reader is used to read data, and then send the data to the engine, and finally to the Source Writer for data writing. Meanwhile, Writer can support the two-phase transaction submission, and the coordinator of Sink supports the aggregation submission requirements of Connector such as Iceberg.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-source-api">04 Source API<a href="#04-source-api" class="hash-link" aria-label="Direct link to 04 Source API" title="Direct link to 04 Source API">â€‹</a></h2><p>After research, we found the following features that are required by Source.</p><ol><li><p>Unified offline and real-time API , which supports that source is implemented only once and supports both offline and real-time API;</p></li><li><p>Supportive of parallel reading. For example that Kafka generates a reader for each partition and execute in parallel.</p></li><li><p>Supporting dynamic slice-adding. For example, Kafka defines a regular topic, when a new topic needs to be added due to the volume of business, the Source API allows to dynamically add the slice to the job.</p></li><li><p>Supporting the work of coordinating reader, which is currently only needed in the CDC Connector. CDC is currently supported by NetFilxâ€™s DBlog parallel algorithms, which requires reader coordination between full synchronization and incremental synchronization.</p></li><li><p>Supporting a single reader to process multiple tables, i.e. to allows the whole database synchronization in the real-time scenario as mentioned above.</p></li></ol><p><img loading="lazy" src="/assets/images/4-ac907e3f8e305b9f6585d2171013a973.jpg" width="1440" height="810" class="img_ev3q"></p><p>Based on the above requirements, we have created the basic API as shown in the figure above. And the code has been submitted to the API-Draft branch in the Apache SeaTunnel(Incubator) community. If youâ€™re interested, you can view the code in detail.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-adapt-to-spark-and-flink-engines">How to adapt to Spark and Flink engines<a href="#how-to-adapt-to-spark-and-flink-engines" class="hash-link" aria-label="Direct link to How to adapt to Spark and Flink engines" title="Direct link to How to adapt to Spark and Flink engines">â€‹</a></h3><p>Flink and Spark unify the API of DataSet and DataStream, and they can support the first two features. Then, for the remaining three features, how do we</p><ul><li>Support dynamic slice-addingï¼Ÿ</li><li>Support the work of coordinating readerï¼Ÿ</li><li>Support a single reader to process multiple tablesï¼Ÿ</li></ul><p>Let&#x27;s review the design with questions.</p><p><img loading="lazy" src="/assets/images/5-f743cf882b36bb3c383c66dec4bad95f.jpg" width="1440" height="810" class="img_ev3q"></p><p>We found that other connectors do not need coordinators, except for CDC. For those connectors that do not need coordinators, we have a Source that supports parallel execution and engine translation.</p><p>As shown in the figure above, there is a slice enumerator on the left, which can list which slices the source needs and show what there are. After enumerating slices in real time, each slice would be distributed to SourceReader, the real data reading module. Boundedness marker is used to differentiate offline and real-time operations. Connector can mark whether there is a stop Offset in a slice. For example, Kafka can support real-time and offline operations. The degree of parallelism can be set for the ParallelSource in the engine to support parallel reading.</p><p>As shown in the figure above, in a scenario where a coordinator is required, Event transmission is done between the Reader and Enumerator. Enumerator coordinates events by the Event sent by the Reader. The Coordinated Source needs to ensure single parallelism at the engine level to ensure data consistency. Of course, this does not make good use of the engineâ€™s memory management mechanism, but trade-offs are necessary.</p><p><img loading="lazy" src="/assets/images/6-ef5d53449db28f4466c14827335d07a6.jpg" width="1440" height="810" class="img_ev3q"></p><p>For the last question, how can we support a single reader to process multiple tables? This involves the Table API layer. Once all the required tables have been read from the Catalog, some of the tables may belong to a single job and can be read by a link, and some may need to be separated, depending on how Source is implemented. Since this is a special requirement, we want to make it easier for the developers. In the Table API layer, we will provide a SupportMultipleTable interface to declare that Source supports multiple Table reads. The Source is implemented based on the corresponding deserializer of multiple tables. As for how to separate derived multi-table data, Flink will adopt Side Output mechanism, while Spark is going to use Filter or Partition mechanism.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-sink-api">5 Sink API<a href="#5-sink-api" class="hash-link" aria-label="Direct link to 5 Sink API" title="Direct link to 5 Sink API">â€‹</a></h2><p>At present, there are not many features required by Sink, but three mojor requirements are considerable according to our research.</p><p>The first is about idempotent writing, which requires no code and depends on whether the storage engine can support it.</p><p>The second is about distributed transactions. The mainstream method is two-phase commitments, such as Kafka etc.</p><p>The third is about the submission of aggregation. For Storage engines like Iceberg and Hoodie, we hope there is no issues triggered by small files, so we expect to aggregate these files into a single file and commit it as a whole.</p><p>Based on these three requirements, we built three APIS: SinkWriter, SinkCommitter, and SinkAggregated Committer. SinkWriter plays a role of writing, which may or may not be idempotent. SinkCommitter supports for two-phase commitments. SinkAggregatedCommitter supports for aggregated commitments.</p><p><img loading="lazy" src="/assets/images/7-e9830d63f81f7139a7cd1d4d9b9f5e43.jpg" width="1440" height="810" class="img_ev3q"></p><p>Ideally, AggregatedCommitter runs in Driver in single or parallel, and Writer and Committer run in Worker with multiple parallels, with each parallel carrying its own pre-commit work and then send Aggregated messages to Aggregated committers.</p><p>Current advanced versions of Spark and Flink all support AggregatedCommitter running on the Driver(Job Manager) and Writer and Committer running on the worker(Task Manager).</p><p><img loading="lazy" src="/assets/images/8-aa0537c76d15543b46623445ff00e490.jpg" width="1440" height="810" class="img_ev3q"></p><p>However, for the lower versions of Flink, AggregatedCommitter cannot be supported to run in JM, so we are also carrying translation adaptation. Writer and Committer will act as pre-operators, packaged by Flinkâ€™s ProcessFunction, supports concurrent pre-delivery and write, and implement two-phase commitment based on Flinkâ€™s Checkpoint mechanism. This is also the current 2PC implementation of many of Flink connectors. The ProcessFunction can send messages about pre-commits to downstream Aggregated committers, which can be wrapped around operators such as SinkFunction or ProcessFunction. Of course, We need to ensure that only one single parallel will be started by the AggregatedCommitter in case of the broken of the logic of the aggregated commitment.</p><p>Thank you for watching. If youâ€™re interested in the specific implementations mentioned in my speech, you can refer to the Apache SeaTunnel (Incubating) community and check out the API-Draft branch code. Thank you again.</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="Author | Fan Jia, Apache SeaTunnel(Incubating) Contributor"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/How to synchronize tens of billions of data based on SeaTunnel&#x27;s ClickHouse">How to synchronize tens of billions of data based on SeaTunnel&#x27;s ClickHouse</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-10T00:00:00.000Z" itemprop="datePublished">May 10, 2022</time> Â· <!-- -->8 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-c3f068094d4f0308d7100502a6162925.jpg" width="1920" height="1275" class="img_ev3q"></p><p>Author | Fan Jia, Apache SeaTunnel(Incubating) Contributor
Editor | Test Engineer Feng Xiulan</p><p>For importing billions of batches of data, the traditional JDBC approach does not perform as well as it should in some massive data synchronization scenarios. To write data faster, Apache SeaTunnel (Incubating) has just released version 2.1.1 to provide support for ClickhouseFile-Connector to implement Bulk load data writing.</p><p>Bulk load means synchronizing large amounts of data to the target DB. SeaTunnel currently supports data synchronization to ClickHouse.</p><p>At the Apache SeaTunnel (Incubating) April Meetup, Apache SeaTunnel (Incubating) contributor Fan Jia shared the topic of &quot;ClickHouse bulk load implementation based on SeaTunnel&quot;, explaining in detail the implementation principle and process of ClickHouseFile for efficient processing of large amounts of data.</p><p>Thanks to the test engineer Feng Xiulan for the article arrangement!</p><p>This presentation contains seven parts.</p><ul><li>State of ClickHouse Sink</li><li>Scenarios that ClickHouse Sink isn&#x27;t good at </li><li>Introduction to the ClickHouseFile plugin</li><li>ClickHouseFile core technologies</li><li>Analysis of ClickHouseFile plugin implementation</li><li>Comparison of plug-in capabilities</li><li>Post-optimization directions</li></ul><p><img loading="lazy" src="/assets/images/0-1-56defefcc273a6e21b09dd483bf95914.png" width="1171" height="1171" class="img_ev3q"></p><p>Fan Jia,  Apache SeaTunnel (Incubating) contributor, Senior Enginee of WhaleOps.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-status-of-clickhouse-sink">01 Status of ClickHouse Sink<a href="#01-status-of-clickhouse-sink" class="hash-link" aria-label="Direct link to 01 Status of ClickHouse Sink" title="Direct link to 01 Status of ClickHouse Sink">â€‹</a></h2><p>At present, the process of synchronizing data from SeaTunnel to ClickHouse is as follows: as long as the data source is supported by SeaTunnel, the data can be extracted, converted (or not), and written directly to the ClickHouse sink connector, and then written to the ClickHouse server via JDBC. </p><p><img loading="lazy" src="/assets/images/1-76284c6612152506e0111e0f0d25d0f5.png" width="1139" height="585" class="img_ev3q"></p><p>However, there are some problems with writing to the ClickHouse server via traditional JDBC.</p><p>Firstly, the tool used now is the driver provided by ClickHouse and implemented via HTTP, however, HTTP is not very efficient to implement in certain scenarios. The second is the huge amount of data, if there is duplicate data or a large amount of data written at once, it needs to generate the corresponding insert statement and send it via HTTP to the ClickHouse server-side by the traditional method, where it is parsed and executed item by item or in batches, which does not allow data compression.</p><p>Finally, there is the problem we often encounter, i.e. too much data may lead to an OOM on the SeaTunnel side or a server-side hang due to too much data being written to the server-side too often.</p><p>So we thought, is there a faster way to send than HTTP? If data pre-processing or data compression could be done on the SeaTunnel side, then the network bandwidth pressure would be reduced and the transmission rate would be increased.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-scenarios-that-clickhouse-sink-isnt-good-at">02 Scenarios that ClickHouse Sink isn&#x27;t good at<a href="#02-scenarios-that-clickhouse-sink-isnt-good-at" class="hash-link" aria-label="Direct link to 02 Scenarios that ClickHouse Sink isn&#x27;t good at" title="Direct link to 02 Scenarios that ClickHouse Sink isn&#x27;t good at">â€‹</a></h2><ol><li>If the HTTP transfer protocol is used, HTTP may not be able to handle it when the volume of data is too large and the batch is sending requests in micro-batches.</li><li>Too many INSERT requests may put too much pressure on the server. The bandwidth can handle a large number of requests, but the server-side is not always able to carry them. The online server not only needs data inserts but more importantly, the query data can be used by other business teams. If the server cluster goes down due to too much-inserted data, it is more than worth the cost.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-clickhouse-file-core-technologies">03 ClickHouse File core technologies<a href="#03-clickhouse-file-core-technologies" class="hash-link" aria-label="Direct link to 03 ClickHouse File core technologies" title="Direct link to 03 ClickHouse File core technologies">â€‹</a></h2><p>In response to these scenarios that ClickHouse is not good at, we wondered is there a way to do data compression right on the Spark side, without increasing the resource load on the Server when writing data, and with the ability to write large amounts of data quickly? So we developed the ClickHouseFile plugin to solve the problem.</p><p>The key technology of the ClickHouseFile plugin is ClickHouse -local. ClickHouse-local mode allows users to perform fast processing of local files without having to deploy and configure a ClickHouse Server. C lickHouse-local uses the same core as ClickHouse Server, so it supports most features as well as the same format and table engine.</p><p>These two features mean that users can work directly with local files without having to do the processing on the ClickHouse Server side. Because it is the same format, the data generated by the operations we perform on the remote or SeaTunnel side is seamlessly compatible with the server-side and can be written to using ClickHouse local. ClickHouse local is the core technology for the implementation of ClickHouseFile, which allows for implementing the ClickHouse file connector.</p><p>ClickHouse local core is used in the following ways.</p><p><img loading="lazy" src="/assets/images/2-2367f70ae655c30a94a2ec65e67a6b26.png" width="1112" height="262" class="img_ev3q"></p><p>First line: pass the data to the test_table table of the ClickHouse-local program via the Linux pipeline.</p><p>Lines two to five: create a result_table for receiving data.</p><p>The sixth line: pass data from test_table to the result_table.</p><p>Line 7: Define the disk path for data processing.</p><p>By calling the Clickhouse-local component, the Apache SeaTunnel (Incubating) is used to generate the data files and compress the data. By communicating with the Server, the generated data is sent directly to the different nodes of Clickhouse and the data files are then made available to the nodes for the query.</p><p>Comparison of the original and current implementations.</p><p><img loading="lazy" src="/assets/images/3-6204c709b48243f88914bfd492dc67f2.png" width="1272" height="576" class="img_ev3q"></p><p>Originally, the data, including the insert statements was sent by Spark to the server, and the server did the SQL parsing, generated and compressed the table data files, generated the corresponding files, and created the corresponding indexes. If we use ClickHouse local technology, the data file generation, file compression and index creation are done by SeaTunnel, and the final output is a file or folder for the server-side, which is synchronized to the server and the server can queries the data without additional operations.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-core-technical-points">04 Core technical points<a href="#04-core-technical-points" class="hash-link" aria-label="Direct link to 04 Core technical points" title="Direct link to 04 Core technical points">â€‹</a></h2><p><img loading="lazy" src="/assets/images/4-d47e1da865afa7ea4de50b2d6e4b6ac1.png" width="1164" height="435" class="img_ev3q"></p><p>The above process makes data synchronization more efficient, thanks to three optimizations we have made to it.</p><p>Firstly, the data is transferred from the pipeline to the ClickHouseFile by the division, which imposes limitations in terms of length and memory. For this reason, we write the data received by the ClickHouse connector, i.e. the sink side, to a temporary file via MMAP technology, and then the ClickHouse local reads the data from the temporary file to generate our target local file, in order to achieve the effect of incremental data reading and solve the OM problem.</p><p><img loading="lazy" src="/assets/images/5-9f00635b1727843f705cd5a28632e2e4.png" width="1206" height="565" class="img_ev3q"></p><p>Secondly, it supports sharding. If only one file or folder is generated in a cluster, the file is distributed to only one node, which will greatly reduce the performance of the query. Therefore, we carry out slicing support. Users can set the key for slicing in the configuration folder, and the algorithm will divide the data into multiple log files and write them to different cluster nodes, significantly improving the read performance.</p><p><img loading="lazy" src="/assets/images/6-35b30550d6a18fbea49856083aa85094.png" width="1043" height="558" class="img_ev3q"></p><p>The third key optimization is file transfer. Currently, SeaTunnel supports two file transfer methods, one is SCP, which is characterized by security, versatility, and no additional configuration; the other is RSYNC, which is somewhat fast and efficient and supports breakpoint resume, but requires additional configuration, users can choose between the way suits their needs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-plugin-implementation-analysis">05 Plugin implementation analysis<a href="#05-plugin-implementation-analysis" class="hash-link" aria-label="Direct link to 05 Plugin implementation analysis" title="Direct link to 05 Plugin implementation analysis">â€‹</a></h2><p>In summary, the general implementation process of ClickHouseFile is as follows.</p><p><img loading="lazy" src="/assets/images/7-1be978da30a55fe0289c683f2ae61aac.png" width="533" height="635" class="img_ev3q"></p><p>1.caching data to the ClickHouse sink side.
2.calling ClickHouse-local to generate the file.
3.sending the data to the ClickHouse server.
4.Execution of the ATTACH command.</p><p>With the above four steps, the generated data reaches a queryable state.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="06-comparison-of-plug-in-capabilities">06 Comparison of plug-in capabilities<a href="#06-comparison-of-plug-in-capabilities" class="hash-link" aria-label="Direct link to 06 Comparison of plug-in capabilities" title="Direct link to 06 Comparison of plug-in capabilities">â€‹</a></h2><p><img loading="lazy" src="/assets/images/8-261e7ba686f3fadf5d7c1445e9be5b66.png" width="1071" height="485" class="img_ev3q"></p><p>(a) In terms of data transfer, ClickHouseFile is more suitable for massive amounts of data, with the advantage that no additional configuration is required and it is highly versatile, while ClickHouseFile is more complex to configure and currently supports fewer engines.</p><p>In terms of environmental complexity, ClickHouse is more suitable for complex environments and can be run directly without additional configuration.</p><p>In terms of versatility, ClickHouse, due to being an officially supported JDBC diver by SeaTunnel, basically supports all engines for data writing, while ClickHouseFile supports relatively few engines.</p><p>In terms of server pressure, ClickHouseFile&#x27;s advantage shows when it comes to massive data transfers that don&#x27;t put too much pressure on the server.</p><p>However, the two are not in competition and the choice needs to be based on the usage scenario.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="07-follow-up-plans">07 Follow-up plans<a href="#07-follow-up-plans" class="hash-link" aria-label="Direct link to 07 Follow-up plans" title="Direct link to 07 Follow-up plans">â€‹</a></h2><p>Although SeaTunnel currently supports the ClickHouseFile plugin, there are still many defects that need to be optimized, mainly including</p><ul><li>Rsync support.</li><li>Exactly-Once support.</li><li>Zero Copy support for transferring data files.</li><li>More Engine support.</li></ul><p>Anyone interested in the above issues is welcome to contribute to the follow-up plans, or tell me your ideas!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="At the Apache SeaTunnel (Incubating) Meetup in April, Yuan Hongjun, a big data expert and OLAP platform architect at Kidswant, shared a topic of SeaTunnel Application and Refactoring at Kidswant."><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/SeaTunnel Application and Refactoring at Kidswant">SeaTunnel Application and Refactoring at Kidswant</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-05-01T00:00:00.000Z" itemprop="datePublished">May 1, 2022</time> Â· <!-- -->10 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="/assets/images/0-38f7968af0b7239e9d427a85adee4452.png" width="1920" height="1080" class="img_ev3q"></p><p>At the Apache SeaTunnel (Incubating) Meetup in April, Yuan Hongjun, a big data expert and OLAP platform architect at Kidswant, shared a topic of SeaTunnel Application and Refactoring at Kidswant.</p><p>The presentation contains five parts.</p><ul><li>Background of the introduction of Apache SeaTunnel (Incubating) by Kidswant</li><li>A comparison of mainstream tools for big data processing</li><li>The implementation of Apache SeaTunnel (Incubating)</li><li>Common problems in Apache SeaTunnel (Incubating) refactoring</li><li>Predictions on the future development of Kidswant</li></ul><p><img loading="lazy" src="/assets/images/0-1-4c853aa726b29acc5954ba53240dc2b8.png" width="2578" height="2567" class="img_ev3q"></p><p>Yuan Hongjun, Big data expert, OLAP platform architect of Kidswant. He has many years of experience in big data platform development and management, and has rich research experience in data assets, data lineage mapping, data governance, OLAP, and other fields.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-background">01 Background<a href="#01-background" class="hash-link" aria-label="Direct link to 01 Background" title="Direct link to 01 Background">â€‹</a></h2><p><img loading="lazy" src="/assets/images/1-198d9a9b685f80a9814d5620d1194355.png" width="1166" height="720" class="img_ev3q"></p><p>At present, Kidswantâ€™s OLAP platform consists of seven parts: metadata layer, task layer, storage layer, SQL layer, scheduling layer, service layer, and monitoring layer. This sharing focuses on offline tasks in the task layer.</p><p>In fact, Kidswant had a complete internal collection and push system, but due to some historical legacy issues, the companyâ€™s existing platform could not quickly support the OLAP platform getting online, so at that time the company had to abandon its own platform and start developing a new system instead.
There were three options in front of OLAP at the time.</p><p>1, Re-develop the collection and push system.</p><p>2ã€Self-R&amp;D.</p><p>3, Participate in open source projects.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-big-data-processing-mainstream-tools-comparison">02 Big data processing mainstream tools comparison<a href="#02-big-data-processing-mainstream-tools-comparison" class="hash-link" aria-label="Direct link to 02 Big data processing mainstream tools comparison" title="Direct link to 02 Big data processing mainstream tools comparison">â€‹</a></h2><p>These three options have their own pros and cons. Carrying re-research and development based on the collection and push system is convenient for us to take advantage of the experience of previous results and avoid repeatedly stepping into the pit. But the disadvantage is that it requires a large amount of code, time, a longer research period, and with less abstract code and lots of customized functions bound to the business, itâ€™s difficult to do the re-development.</p><p>If completely self-developed, though the development process is autonomous and controllable, some engines such as Spark can be done to fit our own architecture, while the disadvantage is that we may encounter some unknown problems.</p><p>For the last choice, if we use open-source frameworks, the advantage is that there is more abstract code, and the framework can be guaranteed in terms of performance and stability after verification by other major companies. Therefore Kidswant mainly studied three open-source data synchronization tools, DATAX, Sqoop, and SeaTunnel in the early stages of OLAP data synchronization refactoring.</p><p><img loading="lazy" src="/assets/images/2-fa4598ecce2b564564a8dfb2f7ccae87.png" width="919" height="720" class="img_ev3q"></p><p>From the diagram we can see that Sqoopâ€™s main function is data synchronization for RDB, and its implementation is based on MAP/REDUCE. Sqoop has rich parameters and command lines to perform various operations. The advantage of Sqoop is that it fits Hadoop ecology, and already supports most of the conversion from RDB to HIVE arbitrary source, with a complete set of commands and APIs.</p><p>The disadvantages are that Sqoop only supports RDB data synchronization and has some limitations on data files, and there is no concept of data cleansing yet.</p><p><img loading="lazy" src="/assets/images/3-53716e33ac191ff35ff66591ba4e1711.png" width="1174" height="720" class="img_ev3q"></p><p>DataX mainly aims at synchronizing data from any source by configurable files + multi-threading, which runs three main processes: Reader, Framework, and Writer, where Framework mainly plays the role of communication and leaving empty space.</p><p>The advantage of DataX is that it uses plug-in development, has its own flow control and data control, and is active in the community, with DataXâ€™s official website offering data pushes from many different sources. The disadvantage of DataX, however, is that it is memory-based and there may be limitations on the amount of data available.</p><p><img loading="lazy" src="/assets/images/4-1897837b0d7e4b52bc086c94b5aa4aea.png" width="1025" height="720" class="img_ev3q"></p><p>Apache SeaTunnel (Incubating) also does data synchronization from any source and implements the process in three steps: source, transform and sink based on configuration files, Spark or Flink. </p><p>The advantage is that the current 2.1.0 version has a very large number of plug-ins and source pushes, based on the idea of plug-ins also makes it very easy to extend and embrace Spark and Flink while with a distributed architecture. The only downside to Apache SeaTunnel (Incubating) is probably the lack of IP calls at the moment and the need to manage the UI interface by yourself.</p><p>In summary, although Sqoop is distributed, it only supports data synchronization between RDB and HIVE, Hbase and has poor scalability, which is not convenient for re-development. DataX is scalable and stable overall, but because it is a standalone version, it cannot be deployed in a distributed cluster, and there is a strong dependency between data extraction capability and machine performance. SeaTunnel, on the other hand, is similar to DataX and makes up for the flaw of non-distributed DataX. It also supports real-time streaming, and the community is highly active as a new product. We chose SeaTunnel based on a number of factors such as whether it supported distributed or not, and whether it needed to be deployed on a separate machine.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-implementation">03 Implementation<a href="#03-implementation" class="hash-link" aria-label="Direct link to 03 Implementation" title="Direct link to 03 Implementation">â€‹</a></h2><p>On the Apache SeaTunnel (Incubating) website, we can see that the basic process of Apache SeaTunnel (Incubating) consists of three parts: source, transform and sink. According to the guidelines on the website, Apache SeaTunnel (Incubating) requires a configuration script to start, but after some research, we found that the final execution of Apache SeaTunnel (Incubating) is bansed on an application submitted by spark-submit that relies on the config file.</p><p>This initialization, although simple, has the problem of having to rely on the config file, which is generated and then cleared after each run, and although it can be dynamically generated in the scheduling script, it raises two questions: 1) whether frequent disk operations make sense; and 2) whether there is a more efficient way to support Apache SeaTunnel (Incubating).</p><p><img loading="lazy" src="/assets/images/5-c2ec6a581e98f15b50a0925325be6acf.png" width="1196" height="720" class="img_ev3q"></p><p>With these considerations in mind, we added a Unified Configuration Template Platform module to the final design solution. Scheduling is done by initiating a commit command, and Apache SeaTunnel (Incubating) itself pulls the configuration information from the unified configuration template platform, then loads and initializes the parameters.</p><p><img loading="lazy" src="/assets/images/6-5d4653fed2f38a65b3e8c04428e1d53e.png" width="1048" height="720" class="img_ev3q"></p><p>The diagram above shows the business process for Kidswantâ€™s OLAP, which is divided into three sections. The overall flow of data from Parquet, i.e. Hive, through the Parquet tables to KYLIN and CK source.</p><p><img loading="lazy" src="/assets/images/7-2eed2f9ff97c6bfcc712e992568dd102.png" width="1080" height="687" class="img_ev3q"></p><p>This is the page where we construct the model, which is generated mainly through drag and drop, with some transactional operations between each table, and micro-processing for Apache SeaTunnel (Incubating) on the right.</p><p><img loading="lazy" src="/assets/images/8-8d444012b45ea1a51088db223d29efa6.png" width="1247" height="596" class="img_ev3q"></p><p>So we end up submitting the commands as above, where the first one marked in red is <!-- -->[-conf customconfig/jars]<!-- -->, referring to the fact that the user can then unify the configuration template platform for processing, or specify it separately when modeling. The last one marked in red is <!-- -->[421 $start_time $end_time $taskType]<!-- --> Unicode, which is a unique encoding.</p><p>Below, on the left, are the 38 commands submitted by our final dispatch script. Below, on the right, is a modification made for Apache SeaTunnel (Incubating), and you can see a more specific tool class called WaterdropContext. It can first determine if Unicode exists and then use Unicode_code to get the configuration information for the different templates, avoiding the need to manipulate the config file.</p><p>In the end, the reportMeta is used to report some information after the task is completed, which is also done in Apache SeaTunnel (Incubating).</p><p><img loading="lazy" src="/assets/images/9-54db46eb9209112af1884f1ef6454536.png" width="720" height="752" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/10-0472de8100ba32800e40ea9e86f3b35f.png" width="1280" height="585" class="img_ev3q"></p><p><img loading="lazy" src="/assets/images/11-910cd332e1ebf6b9d7d3ea9186a5c8fa.png" width="1280" height="512" class="img_ev3q"></p><p>In the finalized config file as above, it is worth noting that in terms of transforms, Kidswant has made some changes. The first is to do desensitization for mobile phones or ID numbers etc. If the user specifies a field, they do it by field, if not they will scan all fields and then desensitize and encrypt them according to pattern matching.</p><p>Second, transform also supports custom processing, as mentioned above when talking about OLAP modeling. With the addition of HideStr, the first ten fields of a string of characters can be retained and all characters at the back encrypted, providing some security in the data.</p><p>Then, on the sink side, we added pre_sql in order to support the idempotency of the task, which is mainly done for tasks such as data deletion, or partition deletion, as the task cannot be run only once during production, and this design needed to account for the data deviation and correctness once operations such as reruns or complement occur.</p><p>On the right side of the diagram, on the Sink side of a Clickhouse, we have added an is_senseless_mode, which forms a read/write senseless mode, where the user does not perceive the whole area when querying and complementing but uses the CK partition conversion, i.e. the command called MOVE PARTITION TO TABLE to operate.</p><p>A special note here is the Sink side of KYLIN. KYLIN is a very special source with its own set of data entry logic and its monitoring page, so the transformation we have done on KYLIN is simply a call to its API operation and a simple API call and constant polling of the state when using KYLIN, so the resources for KYLIN are limited in the Unified Template Configuration platform.</p><p><img loading="lazy" src="/assets/images/12-859f200352aab300d0e421ba01e86da6.png" width="1249" height="586" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-common-problems-about-the-apache-seatunnel-incubating-transformation">04 Common problems about the Apache SeaTunnel (Incubating) transformation<a href="#04-common-problems-about-the-apache-seatunnel-incubating-transformation" class="hash-link" aria-label="Direct link to 04 Common problems about the Apache SeaTunnel (Incubating) transformation" title="Direct link to 04 Common problems about the Apache SeaTunnel (Incubating) transformation">â€‹</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="01-oom--too-many-parts">01 OOM &amp; Too Many Parts<a href="#01-oom--too-many-parts" class="hash-link" aria-label="Direct link to 01 OOM &amp; Too Many Parts" title="Direct link to 01 OOM &amp; Too Many Parts">â€‹</a></h4><p>The problem usually arises during the Hive to Hive process, even if we go through automatic resource allocation, but there are cases where the data amount suddenly gets bigger, for example after holding several events. Such problems can only be avoided by manually and dynamically tuning the reference and adjusting the data synchronization batch time. In the future, we may try to control the data volume to achieve fine control.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="02-field-and-type-inconsistency-issues">02 Field and type inconsistency issues<a href="#02-field-and-type-inconsistency-issues" class="hash-link" aria-label="Direct link to 02 Field and type inconsistency issues" title="Direct link to 02 Field and type inconsistency issues">â€‹</a></h4><p>When the model runs, the user will make some changes to the upstream tables or fields that the task depends on, and these changes may lead to task failure if they are not perceived. The current solution is to rely on data lineage+ snapshots for advance awareness to avoid errors.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="03-custom-data-sources--custom-separators">03 Custom data sources &amp; custom separators<a href="#03-custom-data-sources--custom-separators" class="hash-link" aria-label="Direct link to 03 Custom data sources &amp; custom separators" title="Direct link to 03 Custom data sources &amp; custom separators">â€‹</a></h4><p>If the finance department requires a customized separator or jar information, the user can now specify the loading of additional jar information as well as the separator information themselves in the unified configuration template platform.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="04-data-skewing-issues">04 Data skewing issues<a href="#04-data-skewing-issues" class="hash-link" aria-label="Direct link to 04 Data skewing issues" title="Direct link to 04 Data skewing issues">â€‹</a></h4><p>This may be due to users setting their parallelism but not being able to do so perfectly. We havenâ€™t finished dealing with this issue yet, but we may add post-processing to the Source module to break up the data and complete the skew.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="05-kylin-global-dictionary-lock-problem">05 KYLIN global dictionary lock problem<a href="#05-kylin-global-dictionary-lock-problem" class="hash-link" aria-label="Direct link to 05 KYLIN global dictionary lock problem" title="Direct link to 05 KYLIN global dictionary lock problem">â€‹</a></h4><p>As the business grows, one cube will not be able to meet the needs of the users, so it will be necessary to create more than one cube. If the same fields are used between multiple cubes, the problem of KYLIN global dictionary lock will be encountered. The current solution is to separate the scheduling time between two or more tasks, or if this is not possible, we can make a distributed lock control, where the sink side of KYLIN has to get the lock to run.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="05-an-outlook-on-the-future-of-kidswant">05 An outlook on the future of Kidswant<a href="#05-an-outlook-on-the-future-of-kidswant" class="hash-link" aria-label="Direct link to 05 An outlook on the future of Kidswant" title="Direct link to 05 An outlook on the future of Kidswant">â€‹</a></h2><ol><li>Multi-source data synchronization, maybe processing for RDB sources</li><li>Real-time Flink-based implementation</li><li>Take over the existing collection and scheduling platform (mainly to solve the problem of splitting library and tables)</li><li>Data quality verification, like some null values, the vacancy rate of the whole data, main time judgment, etc.</li></ol><p>This is all I have to share, I hope we can communicate more with the community in the future and make progress together, thanks!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><meta itemprop="description" content="1"><header><h2 class="title_f1Hy" itemprop="headline"><a itemprop="url" href="/blog/Innovation of Data Integration Technology in the Intelligent Era">Innovation of Data Integration Technology in the Intelligent Era</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2022-04-08T00:00:00.000Z" itemprop="datePublished">April 8, 2022</time> Â· <!-- -->4 min read</div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="1" src="/assets/images/1-2af82fe19d2d0ad453f547d8c27647c4.png" width="1600" height="900" class="img_ev3q"></p><p>As we know, only manageable, callable, computable, and magnetizable data resources can be deposited as assets. The interconnection of information systems has created a huge demand for multi-source and multidimensional data integration, which imposes strict requirements on data processing and integration tools.</p><p>In the era of intelligence, under the trends of â€œsmart cityâ€, â€œsmart governanceâ€, and â€œintelligent productsâ€, enterprises are mostly faced with the challenge of how to achieve efficient data push, improve platform quality, and ensure data security. Only by choosing the right data integration tools and platforms can data play a key role.</p><p>As a next-generation high-performance, distributed, and massive data integration framework, Apache SeaTunnel is committed to making data synchronization simpler and more efficient and accelerating the implementation of distributed data processing capabilities in the production environment.</p><p>At the Apache SeaTunnel Meetup (April 16, 2022), the community will invite experienced Apache SeaTunnel users to share the best practices of the project in intelligent production environments. In addition, there will be contributors to analyze the source code of Apache SeaTunnel, guiding you to have a comprehensive and in-depth understanding of this powerful data integration tool.</p><p>Whether you are a beginner who is interested in Apache SeaTunnel or users who encounter complex and difficult deployment problems in daily practice, you can come here to communicate with our instructors and get the answers you want.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-sign-up">01 Sign up<a href="#01-sign-up" class="hash-link" aria-label="Direct link to 01 Sign up" title="Direct link to 01 Sign up">â€‹</a></h2><p>Apache SeaTunnel Meetup | April online live registration has been started, hurry up and register!</p><p>Time: 2022â€“4â€“16 14:00â€“17:00</p><p>Format: live online</p><p>Click the link to register (free):Â <a href="https://www.slidestalk.com/m/780" target="_blank" rel="noopener noreferrer">https://www.slidestalk.com/m/780</a></p><p>Join Slack:</p><p><a href="https://join.slack.com/t/apacheseatunnel/shared_invite/zt-10u1eujlc-g4E~ppbinD0oKpGeoo_dAw" target="_blank" rel="noopener noreferrer">https://join.slack.com/t/apacheseatunnel/shared_invite/zt-10u1eujlc-g4E~ppbinD0oKpGeoo_dAw</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-highlights">02 Highlights<a href="#02-highlights" class="hash-link" aria-label="Direct link to 02 Highlights" title="Direct link to 02 Highlights">â€‹</a></h2><ul><li>Detailed case study</li><li>Feature Analysis</li><li>Tips to avoid stepping into the pit from enterprises</li><li>Open-source community growth strategy</li><li>Face-to-face Q&amp;A with industry technical experts</li><li>Surprise gifts</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-event-agenda">03 Event Agenda<a href="#03-event-agenda" class="hash-link" aria-label="Direct link to 03 Event Agenda" title="Direct link to 03 Event Agenda">â€‹</a></h2><p>On the day of the event, big data engineers from Kidswant and oppo will share the front-line practical experience, and senior engineers from WhaleOps will give a â€œhard-coreâ€ explanation of the important function updates of Apache SeaTunnel.</p><p><img loading="lazy" alt="2" src="/assets/images/2-a5ffa068e39b9488fa6ebdc44358b9a3.png" width="1080" height="1075" class="img_ev3q"></p><p>Yuan Hongjun, Kidswant Big Data Expert, OLAP Platform Architect</p><p>Years of experience in R&amp;D and management of big data platforms, rich research experience in data assets, data linkage, data governance, OLAP, and other fields</p><p>Time: 14:00â€“14:40</p><p>Topic: Application Practice of Apache SeaTunnel in Kidswant</p><p>Speech outline: How to push data efficiently? How to improve the quality of the platform? How to ensure data security? What changes did Kidswant make to Apache SeaTunnel?</p><p><img loading="lazy" alt="3" src="/assets/images/3-22d7703611173db9493fd49adc5687ae.png" width="1080" height="1080" class="img_ev3q"></p><p>Fan Jia, WhaleOps Senior Engineer</p><p>Time: 14:40â€“15:20</p><p>Topic: Clickhouse Bulk Load Implementation Based on Apache SeaTunnel</p><p>Speech outline: How to implement the bulk load data synchronization function of Clickhouse by extending the Connector of Apache SeaTunnel?</p><p><img loading="lazy" alt="4" src="/assets/images/4-acd48a1d76d384e7e930ab44862aded8.png" width="1080" height="1078" class="img_ev3q"></p><p>Wang Zichao, Oppo Senior Backend Engineer</p><p>Time: 15:50â€“16:30</p><p>Topic: The technological innovation of oppo intelligent recommendation sample center based on Apache SeaTunnel</p><p>Speech outline: Introduce the evolution of oppoâ€™s intelligent recommendation machine learning sample dealing process and the role of Apache SeaTunnel in it.</p><p>In addition to the wonderful speeches, a number of lucky draw sessions were also set up on the meetup. Anyone participating in the lucky draw will have the opportunity to win exquisite customized gifts from Apache SeaTunnel, so stay tuned~</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-seatunnel"><strong>About SeaTunnel</strong><a href="#about-seatunnel" class="hash-link" aria-label="Direct link to about-seatunnel" title="Direct link to about-seatunnel">â€‹</a></h2><p>SeaTunnel (formerly Waterdrop) is an easy-to-use, ultra-high-performance distributed data integration platform that supports real-time synchronization of massive amounts of data and can synchronize hundreds of billions of data per day in a stable and efficient manner.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-do-we-need-seatunnel"><strong>Why do we need SeaTunnel?</strong><a href="#why-do-we-need-seatunnel" class="hash-link" aria-label="Direct link to why-do-we-need-seatunnel" title="Direct link to why-do-we-need-seatunnel">â€‹</a></h2><p>SeaTunnel does everything it can to solve the problems you may encounter in synchronizing massive amounts of data.</p><ul><li>Data loss and duplication</li><li>Task buildup and latency</li><li>Low throughput</li><li>Long application-to-production cycle time</li><li>Lack of application status monitoring</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="seatunnel-usage-scenarios"><strong>SeaTunnel Usage Scenarios</strong><a href="#seatunnel-usage-scenarios" class="hash-link" aria-label="Direct link to seatunnel-usage-scenarios" title="Direct link to seatunnel-usage-scenarios">â€‹</a></h2><ul><li>Massive data synchronization</li><li>Massive data integration</li><li>ETL of large volumes of data</li><li>Massive data aggregation</li><li>Multi-source data processing</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="features-of-seatunnel"><strong>Features of SeaTunnel</strong><a href="#features-of-seatunnel" class="hash-link" aria-label="Direct link to features-of-seatunnel" title="Direct link to features-of-seatunnel">â€‹</a></h2><ul><li>Rich components</li><li>High scalability</li><li>Easy to use</li><li>Mature and stable</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-get-started-with-seatunnel-quickly"><strong>How to get started with SeaTunnel quickly?</strong><a href="#how-to-get-started-with-seatunnel-quickly" class="hash-link" aria-label="Direct link to how-to-get-started-with-seatunnel-quickly" title="Direct link to how-to-get-started-with-seatunnel-quickly">â€‹</a></h2><p>Want to experience SeaTunnel quickly? SeaTunnel 2.1.0 takes 10 seconds to get you up and running.</p><p><a href="https://seatunnel.apache.org/docs/2.1.0/developement/setup" target="_blank" rel="noopener noreferrer">https://seatunnel.apache.org/docs/2.1.0/developement/setup</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="how-can-i-contribute"><strong>How can I contribute?</strong><a href="#how-can-i-contribute" class="hash-link" aria-label="Direct link to how-can-i-contribute" title="Direct link to how-can-i-contribute">â€‹</a></h2><p>We invite all partners who are interested in making local open-source global to join the SeaTunnel contributors family and foster open-source together!</p><p>Submit an issue:</p><p><a href="https://github.com/apache/incubator-seatunnel/issues" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/issues</a></p><p>Contribute code to:</p><p><a href="https://github.com/apache/incubator-seatunnel/pulls" target="_blank" rel="noopener noreferrer">https://github.com/apache/incubator-seatunnel/pulls</a></p><p>Subscribe to the community development mailing list :</p><p><a href="mailto:dev-subscribe@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev-subscribe@seatunnel.apache.org</a></p><p>Development Mailing List :</p><p><a href="mailto:dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer">dev@seatunnel.apache.org</a></p><p>Join Slack:</p><p><a href="https://the-asf.slack.com/archives/C053HND1D6X" target="_blank" rel="noopener noreferrer">https://the-asf.slack.com/archives/C053HND1D6X</a></p><p>Follow Twitter:</p><p><a href="https://twitter.com/ASFSeaTunnel" target="_blank" rel="noopener noreferrer">https://twitter.com/ASFSeaTunnel</a></p><p>Come and join us!</p></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/blog/tags/meetup">Meetup</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">SeaTunnel</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/faq">FAQ</a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/releases" target="_blank" rel="noopener noreferrer" class="footer__link-item">Releases<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/issues" target="_blank" rel="noopener noreferrer" class="footer__link-item">Issue Tracker<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://github.com/apache/incubator-seatunnel/pulls" target="_blank" rel="noopener noreferrer" class="footer__link-item">Pull Requests<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Subscribe Mailing List</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/community/contribution_guide/subscribe">How to Subscribe</a></li><li class="footer__item"><a href="mailto:dev-subscribe@seatunnel.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Subscribe Mail<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://lists.apache.org/list.html?dev@seatunnel.apache.org" target="_blank" rel="noopener noreferrer" class="footer__link-item">Mail Archive<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">
            <div style="margin-top: 20px;">
                <img style="height:50px;margin: 30px 0 10px;" alt="Apache Software Foundation" src="/image/asf_logo_wide.svg">
                <div style="border-top: 1px solid #ccc;min-height: 60px;line-height: 20px;text-align: center;font-family: Avenir-Medium;font-size: 14px;color: #999;display: flex;align-items: center;"><span>Copyright Â© 2021-2024 The Apache Software Foundation. Apache SeaTunnel, SeaTunnel, and its feather logo are trademarks of The Apache Software Foundation.</span></div>
                <div style="text-align: center;">
                    <a href="https://twitter.com/asfseatunnel?s=21" target="_blank" title="Twitter"><svg t="1644553365083" class="icon" viewBox="0 0 1260 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="7015" width="38" height="38"><path d="M1259.846921 121.148242c-46.524504 20.728739-96.273478 34.547899-148.325646 40.536201 53.434084-31.784067 94.430924-82.454319 113.777747-142.797982-50.209613 29.480874-105.486251 51.13089-164.447999 62.646857A257.584528 257.584528 0 0 0 872.449815 0.000276c-142.797982 0-258.418284 115.620302-258.418284 258.418284 0 20.268101 2.303193 40.075563 6.909579 58.961748C405.82286 306.32498 215.579097 203.602561 87.98219 47.446058c-22.110655 38.233008-35.008538 82.454319-35.008538 129.900099 0 89.824537 45.603227 168.593747 115.159663 215.118251-42.378756-1.381916-81.99368-12.897882-117.002217-32.244706v3.224471c0 125.293713 88.90326 229.398049 207.287393 253.351259-21.650017 5.988302-44.681949 9.212773-68.17452 9.212773-16.582991 0-32.705344-1.842555-48.827697-4.606387 32.705344 102.722419 128.518184 177.345881 241.374653 179.649074-88.442621 69.095798-199.917175 110.553277-321.06514 110.553277-20.728739 0-41.457479-1.381916-61.72558-3.685109 114.238386 73.241546 250.126788 116.08094 396.149241 116.08094 475.379089 0 735.179289-393.846048 735.179289-735.179289 0-11.055328-0.460639-22.571294-0.921277-33.626621 51.13089-36.851092 94.891562-82.454319 129.439461-134.045848z" fill="#909094" p-id="7016"></path></svg></a> 
                    <a href="https://s.apache.org/seatunnel-slack" target="_blank" title="Slack" style="margin-left: 20px;"><svg t="1644553076784" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3088" width="36" height="36"><path d="M215.125333 647.04a107.861333 107.861333 0 0 1-107.52 107.648A107.861333 107.861333 0 0 1 0 647.04a107.818667 107.818667 0 0 1 107.605333-107.52h107.52v107.52z m54.229334 0a107.818667 107.818667 0 0 1 107.562666-107.52 107.818667 107.818667 0 0 1 107.562667 107.52v269.354667A107.861333 107.861333 0 0 1 376.917333 1024a107.861333 107.861333 0 0 1-107.562666-107.605333v-269.354667zM376.917333 215.125333a107.861333 107.861333 0 0 1-107.562666-107.52A107.861333 107.861333 0 0 1 376.917333 0a107.861333 107.861333 0 0 1 107.562667 107.605333v107.52H376.917333z m0 54.229334a107.861333 107.861333 0 0 1 107.562667 107.562666 107.861333 107.861333 0 0 1-107.562667 107.562667H107.605333A107.861333 107.861333 0 0 1 0 376.917333a107.861333 107.861333 0 0 1 107.605333-107.562666h269.312z m431.872 107.562666a107.861333 107.861333 0 0 1 107.605334-107.562666A107.861333 107.861333 0 0 1 1024 376.917333a107.861333 107.861333 0 0 1-107.605333 107.562667h-107.605334V376.917333z m-54.101333 0a107.861333 107.861333 0 0 1-107.648 107.562667 107.818667 107.818667 0 0 1-107.52-107.562667V107.605333A107.818667 107.818667 0 0 1 647.04 0a107.861333 107.861333 0 0 1 107.648 107.605333v269.312z m-107.648 431.872a107.861333 107.861333 0 0 1 107.648 107.605334A107.861333 107.861333 0 0 1 647.04 1024a107.818667 107.818667 0 0 1-107.52-107.605333v-107.605334h107.52z m0-54.101333a107.818667 107.818667 0 0 1-107.52-107.648 107.776 107.776 0 0 1 107.52-107.52h269.354667A107.818667 107.818667 0 0 1 1024 647.04a107.861333 107.861333 0 0 1-107.605333 107.648h-269.354667z" p-id="3089" fill="#909094"></path></svg></a> 
                    <a href="https://lists.apache.org/list.html?dev@seatunnel.apache.org" target="_blank" title="Mailing list" style="margin-left: 20px;"><svg t="1644553175467" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5304" width="39" height="39"><path d="M853.333333 170.666667H170.666667c-46.933333 0-85.333333 38.4-85.333334 85.333333v512c0 46.933333 38.4 85.333333 85.333334 85.333333h682.666666c46.933333 0 85.333333-38.4 85.333334-85.333333V256c0-46.933333-38.4-85.333333-85.333334-85.333333z m0 170.666666l-341.333333 213.333334-341.333333-213.333334V256l341.333333 213.333333 341.333333-213.333333v85.333333z" p-id="5305" fill="#909094"></path></svg></a> 
                    <a href="https://github.com/apache/seatunnel" target="_blank" title="GitHub" style="margin-left: 20px;"><svg t="1644553223000" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6156" width="36" height="36"><path d="M512 12.64c-282.752 0-512 229.216-512 512 0 226.208 146.72 418.144 350.144 485.824 25.6 4.736 35.008-11.104 35.008-24.64 0-12.192-0.48-52.544-0.704-95.328-142.464 30.976-172.512-60.416-172.512-60.416-23.296-59.168-56.832-74.912-56.832-74.912-46.464-31.776 3.52-31.136 3.52-31.136 51.392 3.616 78.464 52.768 78.464 52.768 45.664 78.272 119.776 55.648 148.992 42.56 4.576-33.088 17.856-55.68 32.512-68.48-113.728-12.928-233.28-56.864-233.28-253.024 0-55.904 20-101.568 52.768-137.44-5.312-12.896-22.848-64.96 4.96-135.488 0 0 43.008-13.76 140.832 52.48a491.296 491.296 0 0 1 128.16-17.248c43.488 0.192 87.328 5.888 128.256 17.248 97.728-66.24 140.64-52.48 140.64-52.48 27.872 70.528 10.336 122.592 5.024 135.488 32.832 35.84 52.704 81.536 52.704 137.44 0 196.64-119.776 239.936-233.792 252.64 18.368 15.904 34.72 47.04 34.72 94.816 0 68.512-0.608 123.648-0.608 140.512 0 13.632 9.216 29.6 35.168 24.576C877.472 942.624 1024 750.784 1024 524.64c0-282.784-229.248-512-512-512z" p-id="6157" fill="#909094"></path></svg></a> 
                </div>
            <div></div></div></div></div></div></footer></div>
<script src="/assets/js/runtime~main.85c0a3ab.js"></script>
<script src="/assets/js/main.3d77c3f1.js"></script>
</body>
</html>